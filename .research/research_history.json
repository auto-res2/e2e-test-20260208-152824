{
  "research_topic": "Proposing an improved version of the Chain-of-Thought based on human thinking methods",
  "queries": [
    "cognitive chain-of-thought",
    "human reasoning prompting",
    "process supervision CoT"
  ],
  "research_study_list": [
    {
      "title": "Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought",
      "full_text": "arXiv:2402.05432v1  [econ.EM]  8 Feb 2024 Diﬀerence-in-Diﬀerences Estimators with Continuous T reatments and no Stayers Clément de Chaisemartin, Xavier D’Haultfœuille and Gonzal o V azquez-Bare ∗ Many treatments or policy interventions are continuous in n ature. Examples include prices, taxes or temperatures. Empirical researchers have usually relied on two-way ﬁxed eﬀect regressions to estimate treatment eﬀects in such cases, see e.g. Deschênes and Greenstone (2012). However, such estimators are not robust to heteroge neous treatment eﬀects in gen- eral (De Chaisemartin and D’Haultfœuille, 2020); they also rely on the linearity of treatment eﬀects. W e propose estimators for continuous treatments th at do not impose those restric- tions, and that can be used when there are no stayers: the trea tment of all units changes from one period to the next. This is for instance the case when the treatment is precip- itations or temperatures: for instance, temperatures of al l US counties change, if ever so slightly , between two consecutive years. W e start by extend ing the nonparametric results of de Chaisemartin et al. (2023) to cases without stayers. W e also present a parametric estimator, and use it to revisit Deschênes and Greenstone (2 012). 1 Set-up, assumptions and parameter of interest A representative unit is drawn from an inﬁnite super populat ion, and observed at two time periods. All expectations below are taken with respect to th e distribution of variables in the super population. W e are interested in the eﬀect of a cont inuous and scalar treatment variable on that unit’s outcome. Let Dt denote the unit’s treatment at period t∈ { 1, 2} and let Dt denote its support; let also D denote the support of ( D1 ,D 2). F or any ( d1,d 2) ∈ D , let Yt(d1 ,d 2) denote the unit’s potential outcome at t with treatment d, and let Yt denote ∗ Chaisemartin: Sciences Po Paris, clement.dechaisemartin @sciencespo.fr. D’Haultfœuille: CREST- ENSAE, xavier.dhaultfoeuille@ensae.fr. V azquez-Bare: U niversity of California, Santa Barbara, gvazquez@econ.ucsb.edu. 1their observed outcomes: Yt = Yt(D1,D 2). Finally , for any random variables ( Xt)t=1, 2, let ∆ X = X2 − X1 . W e impose the following assumptions: Assumption 1 (Static model) For al l t ∈ { 1, 2} and (d1,d 2) ∈ D , Yt(d1,d 2) only depends on dt; we denote it by Yt(dt). Assumption 2 (Paral lel trends) ∀d∈ D 1, E(∆ Y(d)|D1 = d,D 2) = E(∆ Y(d)|D1 = d). Assumption 3 (Bounded treatment, bounded-lipschitz potential outcomes) 1. D1 and D2 are bounded subsets of R. 2. ∃Y ≥ 0: sup(d1 ,d 2 )∈D E[Y|D1 = d1,D 2 = d2] < ∞, and ∀(t,d,d ′) ∈ { 1, 2} × D 2 t , |Yt(d) − Yt(d′)| ≤ Y|d− d′|. Assumptions 2-3 are also imposed by de Chaisemartin et al. (2 023), and are discussed therein. Assumption 4 (No stayers but quasi-stayers) P(∆ D= 0) = 0 , P(|∆ D| ≤ η) >0 ∀η >0. First, Assumption 4 states that there are no “stayers”, name ly units for which D1 = D2 . This is in contrast with de Chaisemartin et al. (2023), who as sume throughout that there are stayers. Second, Assumption 4 states that there are “qua si-stayers”, namely units whose treatment change may be inﬁnitesimally small. This assumpt ion is realistic when the treat- ment is, say , temperatures: some counties may have very simi lar temperatures from one year to the next, though no county has exactly the same temperatur es. Hereafter, we focus on the following eﬀect: θ0 =E ( |∆ D| E(|∆ D| × Y2(D2 ) − Y2(D1 ) D2 − D1 ) (1) = E(sgn(∆ D)(Y2(D2) − Y2(D1))) E(|∆ D|) . θ0 is a weighted average of the slopes of units’ potential-outc ome functions, from their period- one to their period-two treatment, the so-called W AOSS in de Chaisemartin et al. (2023). It follows from the mean-value theorem that it may be seen as a weighted average marginal eﬀect. 22 Nonparametric identiﬁcation and estimation Theorem 1 If Assumptions 1-4 hold, θ0 = [ E(S∆ Y) − ζ0]/E [|∆ D|], with S := sgn(∆ D) and ζ0 := E [ Slim η ↓0 E(∆ Y|D1, |D2 − D1 | ≤ η) ] . Theorem 1 shows that without stayers, θ0 is identiﬁed by the limit (as η ↓ 0) of a diﬀerence- in-diﬀerence comparing the ∆ Y of all units and of quasi-stayers. W e now discuss estimation of θ0 . Only the estimation of ζ0 raises diﬃculties. W e show in the proof of Theorem 1 that under our assumptions, g(d1,δ ) := E[∆ Y|D1 = d1 , ∆ D= δ] is well- deﬁned and continuous at ( d1, 0), for any d1 ∈ D 1. Hence, ζ0 satisﬁes ζ0 = E[Sg(D1, 0)]. This formulation links our problem to the estimation of nonparam etric additive models. T o see this, suppose that the variables ( W,X ) ∈ R × Rk satisfy h(x) := E[W|X = x] = ∑ k j=1 hj (xj ) for some unknown functions ( hj )j=1,...,k . Then, under the normalization E[hj (Xj )] = 0 for j <k, we can identify and estimate hk by remarking that hk (xk ) = E[h(X1,...,X k−1,x k )]. (2) W e can then estimate hk (xk ) by ﬁrst estimating h with any usual nonparametric esti- mator, and second plugging it in the sample counterpart of th e expectation in (2). As Linton and Nielsen (1995) and Kong, Linton and Xia (2010) sho w, the corresponding esti- mator is, under regularity conditions, asymptotically nor mal and converges at the standard univariate nonparametric rate (namely , n2/ 5 , with n the sample size). This rate is also the optimal convergence rate for this problem (Stone, 1985). Up to minor changes (in ζ0, gplays the role of h in (2) and ζ0 also includes S), our parameter ζ0 can be obtained in the same way as hk (xk ), so we can also obtain an asymptotically normal estimator c onverging at the n2/ 5 rate. This contrasts with the standard ( n1/ 2 ) rate obtained for the estimators of the W AOSS in the presence of stayers, as shown by de Chaisemartin et al. (2023). T o understand the diﬀerence, note that with stayers, the proportion of units u sed as controls to reconstruct switchers’ counterfactual outcome evolution remains posi tive as n→ ∞ . On the other hand, it tends to zero here, since we need to consider quasi-stayer s, with η → 0 as n→ ∞ to avoid any bias. This results in a lower rate of convergence. 3Finally , in applications with no stayers, it is more diﬃcult to propose placebo estimators of the parallel trends assumption. When a third period of dat a, period zero, is available, a placebo mimics the actual estimator, replacing ∆ Y by units’ period-zero-to-one outcome evolution. However, as units’ treatments may have changed f rom period zero to one, one would need to restrict the sample to period-zero-to-one qua si-stayers, to avoid that the placebo diﬀers from zero due to the treatment’s eﬀect. Thus, the placebo would compare the period-zero-to-one outcome evolution of period-one-t o-two switchers and quasi-stayers, restricting the sample to period-zero-to-one quasi-staye rs. Then, we conjecture that the number of units used as controls by the placebo may tend to zer o faster than the number of units used as controls by the actual estimator, for instance if being a period-zero-to-one and a period-one-to-two quasi-stayer are independent events. Then, the placebo may converge at an even slower rate than the actual estimator. 3 A parametric approach W e now consider a parametric root- n consistent estimator, that avoids issues related to nonparametric estimation and inference, while still allow ing for heterogeneous and nonlinear eﬀects. Speciﬁcally , we impose that g(d1,δ ) = gλ 0 (d1,δ ), where the family ( gλ )λ ∈Rp is known (but λ0 is not). By deﬁnition of g and Assumption 2, g(d1,δ ) = E[Y2(d1) − Y1(d1)|D1 = d1] + δE [ Y2(d1 + δ) − Y2(d1 ) δ ⏐ ⏐ ⏐D1 = d1, ∆ D= δ ] . Thus, the parametric assumption amounts to imposing restri ctions on both d1 ↦→E[Y2(d1) − Y1(d1)|D1 = d1] and the average slope ( d1,δ ) ↦→E[(Y2(d1 + δ) − Y2 (d1))/δ |D1 = d1, ∆ D= δ]. F or instance, if gλ (d1,δ ) is linear, we assume that the former function is linear, and the latter is constant. Similarly , g is a polynomial if both functions are polynomial. Note that w e can test that E[∆ Y|D1 = d1, ∆ D = δ] = gλ 0 (d1,δ ) for some λ0 by a parametric speciﬁcation test, see e.g. Bierens (1982) or Hong and White (1995). W e consider a simple two-step estimator based on this parame tric restriction and an i.i.d. sample ( D1i, ∆ Di , ∆ Yi)i=1,...,n . In the ﬁrst step, we estimate λ0 by (linear or nonlinear) least squares or, more generally , a GMM estimator ˆλ. In the second step, we estimate θ0 by ˆθ = ∑ n i=1 Si(∆ Yi − gˆλ (D1i, 0)) ∑ n i=1 |∆ Di| . Since ˆθ may be seen as a two-step GMM estimator, we obtain, under Assu mptions 1-4 and 4standard regularity conditions on λ ↦→gλ (d1,δ ), √n ( ˆθ − θ0 ) d− → N (0,V (ψ )) , where the inﬂuence function ψ satisﬁes ψ = 1 E[|∆ D|] [ S(∆ Y − gλ 0 (D1 , 0)) − E [ S∂g ∂λ (D1 , 0)|λ =λ 0 ] × ξ − θ0 |∆ D| ] , with ξ the inﬂuence function of ˆλ. W e can thus simply estimate V(ψ ) by a plug-in estimator, using an initial estimator of ξ. 4 Application W e use the data from Deschênes and Greenstone (2012) to compu te our parametric esti- mator. The authors use a balanced panel of 2,342 US counties i n years 1987, 1992, 1997, and 2002, and consider TWFE regressions, weighted by counti es’ farmland acres, of annual agricultural proﬁts in county cand year ton four treatment variables: growing season degree days, growing season degree days squared, precipitations, and precipitations squared. T o ﬁt in the two-periods-one-treatment case we consider, we rest rict the data to years 1997 and 2002, and we focus on the growing season degree days treatmen t. The coeﬃcient of that treatment in a TWFE regression estimated on years 1997 and 20 02 and weighted by counties’ farmland acres is equal to -0.024 (s.e. clustered at the coun ty level: 0.007), which is close to the corresponding TWFE coeﬃcient keeping the four years and all treatments (-0.015, s.e. clustered at the county level: 0.005). Assuming that E[Y2(d1) − Y1 (d1)|D1 = d1] = λ0, 1 + λ0, 2d1 and E [ Y2(d1 + δ) − Y2(d1) δ ⏐ ⏐ ⏐ ⏐ ⏐D1 = d1, ∆ D= δ ] = λ0, 3 + λ0, 4d1 + λ0, 5 δ, we ﬁnd that ˆθ, weighted by counties’ farmland acres as well, is equal to −0.018 (s.e.: 0.011) Thus, the conclusion from the TWFE regression seems robust t o allowing for some eﬀect heterogeneity , even though the estimated eﬀect is less sign iﬁcant. While arguably restrictive, our model for the conditional expectation function of slope s allows for some non-linearity and heterogeneity in the eﬀects of temperatures on agricult ural output. 5Appendix: proof of theorem 1 It suﬃces to show that a.s., lim η ↓0 E(∆ Y|D1, |∆ D| ≤ η) = E(Y2(D1 ) − Y1(D1)|D1,D 2) . (3) Fix η > 0. By Assumption 4, P(|∆ D| ≤ η|D1) > 0. Thus, E(∆ Y|D1 , |∆ D| ≤ η) is well- deﬁned. Moreover, E(∆ Y|D1, |∆ D| ≤ η) = E(Y2(D2) − Y2(D1 )|D1, |∆ D| ≤ η) + E(Y2(D1 ) − Y1(D1 )|D1, |∆ D| ≤ η) . (4) Now, by Jensen’s inequality and Point 2 of Assumption 3, ⏐ ⏐ ⏐E[Y2(D2 ) − Y2(D1 )|D1, |∆ D| ≤ η] ⏐ ⏐ ⏐≤E(|Y2(D2) − Y2(D1 )| | D1 , |∆ D| ≤ η) ≤E ( Y|D2 − D1 | | D1, |∆ D| ≤ η ) ≤ηE [ sup (d1 ,d 2 )∈D E ( Y|D1 = d1,D 2 = d2 ) |D1, |∆ D| ≤ η ] ≤Kη (5) for some K <∞. Next, by Assumption 2, E(Y2(D1) − Y1(D1)|D1, |∆ D| ≤ η) = E(Y2(D1) − Y1(D1)|D1) =E(Y2(D1) − Y1(D1)|D1,D 2) . Combined with (4)-(5), this yields (3) □ References Bierens, Herman J. 1982. “Consistent model speciﬁcation tests. ” Journal of Econometrics, 20(1): 105–134. De Chaisemartin, Clément, and Xavier D’Haultfœuille. 2020. “T wo-way ﬁxed eﬀects estimators with heterogeneous treatment eﬀects. ” American Economic Review , 110(9): 2964–2996. de Chaisemartin, Clément, Xavier D’Haultfœuille, Félix Pa squier, and Gonzalo V azquez-Bare. 2023. “Diﬀerence-in-diﬀerences estimators for treatment s continuously distributed at every period. ” arXiv preprint arXiv:2201.0 6898. 6Deschênes, Olivier, and Michael Greenstone. 2012. “The economic impacts of climate change: evidence from agricultural output and random ﬂuctu ations in weather: reply . ” American Economic Review, 102(7): 3761–3773. Hong, Y ongmiao, and Halbert White. 1995. “Consistent speciﬁcation testing via non- parametric series regression. ” Econometrica: Journal of the Econometric Society, 63: 1133– 1159. Kong, Efang, Oliver Linton, and Yingcun Xia. 2010. “Uniform Bahadur representation for local polynomial estimates of M-regression and its appl ication to the additive model. ” Econometric Theory, 26(5): 1529–1564. Linton, Oliver, and Jens Perch Nielsen. 1995. “A kernel method of estimating struc- tured nonparametric regression based on marginal integrat ion. ” Biometrika, 82(1): 93–100. Stone, Charles J. 1985. “Additive regression and other nonparametric models . ” The Annals of Statistics, 13(2): 689–705. 7",
      "references": [
        "Consistent model speciﬁcation tests.",
        "Two-way ﬁxed eﬀects estimators with heterogeneous treatment eﬀects.",
        "Diﬀerence-in-diﬀerences estimators for treatments continuously distributed at every period.",
        "The economic impacts of climate change: evidence from agricultural output and random ﬂuctuations in weather: reply.",
        "Consistent speciﬁcation testing via non-parametric series regression.",
        "Uniform Bahadur representation for local polynomial estimates of M-regression and its application to the additive model.",
        "A kernel method of estimating structured nonparametric regression based on marginal integration.",
        "Additive regression and other nonparametric models."
      ],
      "meta_data": {
        "arxiv_id": "2402.05432v1",
        "authors": [
          "Clément de Chaisemartin",
          "Xavier D'Haultfœuille",
          "Gonzalo Vazquez-Bare"
        ],
        "published_date": "2024-02-08T06:03:57Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This paper addresses the challenge of estimating treatment effects with continuous treatments, especially when there are no 'stayers' (units whose treatment does not change between periods). Traditional two-way fixed effect regressions are often inadequate due to their reliance on linearity and lack of robustness to heterogeneous treatment effects. The main contributions include extending nonparametric difference-in-differences identification results to settings without stayers but with 'quasi-stayers' (units with infinitesimally small treatment changes). Furthermore, the paper proposes both nonparametric and parametric estimators for this setting, allowing for heterogeneous and nonlinear treatment effects. The parametric estimator is shown to be root-n consistent and asymptotically normal. The study revisits a previous economic impact study to demonstrate the application and robustness of their proposed methods.",
        "methodology": "The paper introduces two main estimation approaches. First, a nonparametric identification strategy extends prior work by de Chaisemartin et al. (2023) to cases without stayers. It identifies a parameter (θ0), a weighted average marginal effect, by leveraging the limit (as treatment change η approaches zero) of a difference-in-differences approach comparing all units and quasi-stayers. The estimation of a key component (ζ0) is framed as a nonparametric additive model estimation problem, utilizing methods from Linton and Nielsen (1995) and Kong, Linton and Xia (2010), achieving an n^(2/5) convergence rate. Second, a parametric root-n consistent estimator is proposed. This approach assumes a known functional form for the conditional expectation function g(d1,δ) = E[∆ Y|D1 = d1, ∆ D= δ]. It employs a two-step GMM estimator: the first step estimates the parameters of the assumed functional form (λ0) using least squares or GMM, and the second step uses these estimated parameters to construct the estimator for θ0.",
        "experimental_setup": "The proposed parametric estimator is applied to data from Deschênes and Greenstone (2012) to re-evaluate their findings. The dataset consists of a balanced panel of 2,342 US counties, with observations from the years 1987, 1992, 1997, and 2002. For this specific application, the data is restricted to years 1997 and 2002. The treatment variable of interest is 'growing season degree days,' while the outcome variable is 'annual agricultural profits.' All regressions and estimations are weighted by counties’ farmland acres. The assumed parametric model for the conditional expectation function of slopes includes linear terms in d1 and δ, allowing for some non-linearity and heterogeneity in effects. The results are compared against the original two-way fixed effects (TWFE) regression coefficients.",
        "limitations": "A significant limitation of the nonparametric estimator is its slower rate of convergence (n^(2/5)) compared to the standard n^(1/2) rate achieved when stayers are present. This reduction is attributed to the need to consider quasi-stayers with infinitesimally small treatment changes, where the proportion of units used as controls tends to zero as the sample size increases. For the parametric approach, the main limitation is the assumption of a known parametric form for the conditional expectation function g(d1,δ), which is 'arguably restrictive.' Additionally, proposing placebo estimators for validating the parallel trends assumption is more challenging in this no-stayers context, and these placebo estimators may converge at an even slower rate than the actual estimator, potentially affecting their reliability.",
        "future_research_directions": "The paper opens several avenues for future research. One key area is the development and refinement of placebo estimators for the parallel trends assumption in settings without stayers. Specifically, addressing the potential for these placebo estimators to converge at an even slower rate than the main estimator is crucial for robust validation. Further exploration of different parametric specifications for the conditional expectation function g(d1,δ) could also be beneficial, allowing for more flexible and less restrictive modeling of heterogeneous and nonlinear effects. Additionally, investigating the robustness of the proposed methods to relax some of the core assumptions (e.g., static model, bounded-Lipschitz potential outcomes) could broaden their applicability.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Chain-of-Thought Predictive Control",
      "full_text": "Elastic Analysis of Augmented Curves and Constrained Surfaces Esfandiar Nava-Yazdani[0000−0003−4895−739X] Zuse Institute Berlin, Berlin, Germany navayazdani@zib.de https://www.zib.de/members/navayazdani Abstract. The square root velocity transformation is crucial for efficiently em- ploying the elastic approach in functional and shape data analysis of curves. We study fundamental geometric properties of curves under this transformation. Moreover, utilizing natural geometric constructions, we employ the approach for intrinsic comparison within several classes of surfaces and augmented curves, which arise in the real world applications such as tubes, ruled surfaces spherical strips, protein molecules and hurricane tracks. Keywords: Elastic shape analysis · Tube · Manifold-valued · Ruled surface · Hurricane track. 1 Introduction Metric comparison of curves is a core task in a wide range of application areas such as morphology, image and shape analysis, computer vision, action recognition and signal processing. Thereby, a Riemannian structure is highly desirable, since it naturally provides powerful tools, beneficial for such applications. In the recent years, the use of Riemannian metrics for the study of sequential data, such as shapes of curves, trajectories given as longitudinal data or time series, has rapidly grown. In elastic analysis of curves, one considers deformations caused from both bending and stretching. A Riemannian metric, which quantifies the amount of those deformations is called elastic (cf. [18,19]). Therein, in contrast to landmark-based approaches (cf. [12,20,22]), one considers whole continuous curves instead of finite num- ber of curve-points. Consequently, the underlying spaces are infinite dimensional and computational cost becomes a significant issue. The square root velocity (SRV) frame- work provides a convenient and numerically efficient approach for analysing curves via elastic metrics and has been widely used in the recent years (cf. [14,11,4,3] and the comprehensive work [24]). In many applications the curves are naturally manifold-valued. For instance, Lie groups such as the Euclidean motion group, or more generally, symmetric spaces includ- ing the Grassmannian and the Hadamard-Cartan manifold of positive definite matrices are widely used in modelling of real world applications. Extensions of SRV framework from euclidean to general manifold-valued data can be found in [13,27,25,9,26]. arXiv:2402.04944v3  [math.DG]  28 Mar 20242 E. Nava-Yazdani Our contributions are the following. We expose for plane curves the behaviour of speed and curvature under the SRV transformation and geometric invariants. More- over, we apply the elastic approach to augmented curves, determining certain classes of surfaces, tubes, ruled surfaces and spherical strips, as well as hurricane tracks consid- ered with their intensities. We recall that with distance and geodesic at hand, signifi- cant ingredients of statistical analysis such as mean and principal geodesic components as well as approximation and modelling concepts such as splines can be computed. This paper is organized as follows. Section 2, presents the Riemannian setting and notations. Section 3 is devoted to applications. Therein, we consider time series, for which in addition to spatial data, auxiliary information give rise to augmented curves and some classes of surfaces generated by them. Thereby, we apply the elastic approach to both euclidean and spherical trajectories. Future prospects and concluding remarks are presented in 4. For the convenience of those readers primary interested in the applications, we mention that, advanced parts and details from differential geometry, presented in 2, can be skipped. Thereby, the essential point is the use of a framework (SRV) for computation of shortest paths on the spaces of curves and their shapes. 2 Riemannian Framework 2.1 Preliminaries For the background material on Riemannian geometry, we refer to [8] and [10]. Let (M, g) be a finite dimensional Riemannian manifold and M the Fr´ echet manifold of smooth immersed curves from D in M, where D denotes either the unit circle S1 or the unit interval I := [0, 1] for closed or open curves respectively. Moreover, we denote the group of orientation preserving diffeomorphisms on D by Diff +. The following reparametrization invariance is crucial for a Riemannian metric G on M: Gc◦φ(h ◦ φ, k◦ φ) = Gc(h, k), for any c ∈ M, h, k∈ TcM and φ ∈ Diff +. The above equivariance ensures that the induced distance function satisfies the following, which is often desirable in applica- tions: d(c0 ◦ φ, c1 ◦ φ) = d(c0, c1), for any two curves c0 and c1 in M. Similarly, denoting the isometry group of M by Isom(M) and the tangent map of F ∈ Isom(M) by T F, the invariance GF◦c(T F◦ h, TF◦ k) = Gc(h, k), ensures that d(F ◦ c0, F◦ c1) = d(c0, c1). With the above invariances, we can divide out the spaces Isom(M) and Diff +, and consider the natural induced distance dS on the quotient space S = M/(Diff + × Isom(M))Elastic Analysis of Augmented Curves and Constrained Surfaces 3 given by dS([c0], [c1]) = inf {d(c0, f◦ c1 ◦ φ) : φ ∈ Diff +, f∈ Isom(M)} = inf {d(f ◦ c0 ◦ φ, c1) : φ ∈ Diff +, f∈ Isom(M)}. In the context of shape analysis of curves, M and S are called the pre-shape and shape space, respectively. Note that the order of quotient operations does not matter, since the left action of Isom(M) and the right action of Diff + commute. M/Diff + is the space of unparametrized curves and its inherited distance reads inf {d(c0, c1 ◦ φ) : φ ∈ Diff +}. We remark that particular essential challenges are due to the fact that some basic concepts and results from finite dimensional differential geometry such as Hopf-Rinow theorem, do not carry over to the infinite dimensional case. Now, let ∇ be the Levi- Civita connection of M and denote the arc length parameter, speed and unit tangent of c by θ, ω and T respectively. Thus, we have ω = |˙c|, dθ = ωdt and T = ˙c ω , where dot stands for derivation with respect to the parameter t. Due to a remarkable result in [16] the geodesic distance induced by the simplest natural choice, the L2-metric GL2 c (h, k) = Z D gc(h, k) dθ, always vanishes. Consequently, some stronger Sobolev metrics have been considered in several works including [17,7,5]. They are given by Gc(h, k) = nX i=0 Z D aigc(∇i T h, ∇i T k) dθ, with a1 non-vanishing and all ai non-negative, distinguish the curves. We consider first order metrics with constant coefficients. We remark that the coefficients ai can be chosen such that the metric is scale invariant, which is a desired property for some applications in shape analysis. A family of certain weighted Sobolev-type metrics, the so-called elastic metrics, based on a decomposition of derivatives of the vector fields into normal and tangent components, has been introduced in [18,19]: Ga,b c (h, k) = Z D agc((∇T h)⊤, (∇T k)⊤) + bgc((∇T h)⊥, (∇T k)⊥) dθ, with 4b ≥ a >0. In this work, we use the square root velocity (SRV) framework, which allows for a convenient and computationally efficient elastic approach. The main tool in this framework is the square root velocity transformation, which for euclidean M reads q : c 7→ ˙cp |˙c| .4 E. Nava-Yazdani It isometrically maps curves modulo translations, with the metric G1,1/4 to M with the flat L2-metric given by G0(v, w) = Z D g(v(t), w(t))dt. This metric is frequently called (cf. [15,2,6]) flat, to emphasize its footpoint indepen- dence. Note that the elastic metric G1,1 corresponds to the first order Sobolev metric with a0 = 0 and a1 = 1. We remark, that for plane curves, the work [23] has extended the SRV transformation to general parameters a, b >0. For further reading on the SRV framework and applications in shape analysis, we refer to [14], [11] (numerical aspects), the survey [6] and particularly, the comprehensive work [24]. 2.2 Plane Curves A natural question that arises is, how essential geometric characteristics of a curve behave under the SRV transformation. In the following, we provide an answer for speed and curvature in the case of plane curves. Let M = R2, ˜c := q(c) and denote the curvature of c by κ. Note that ˜c does not need to be an immersion. Proposition 1. Denoting the speed of˜c by ˜ω, we have ˜ω = r ˙ω2 4ω + ω3κ2. (1) Moreover, ˜c is an immersion if and only ifκ and ˙ω have no common zeros. In this case, ˜κ˜ω = κω + ˙φ, (2) where ˜κ denotes the curvature of˜c and φ := arctan \u00122ω2κ ˙ω \u0013 . Proof. Let N denote the unit normal of c. With the shorthand notations α := √ω and β := α3κ, a straightforward application of the Frenet equations ˙T = ωκN and ˙N = −ωκT , yields ˙˜c = ˙αT + βN, ¨˜c = (¨α − β2 α )T + ( ˙β + ˙αβ α )N. Thus, we have ˜ω = p ˙α2 + β2, immediately implying (1). Obviously, zeros of ˜ω are common zeros of κ and ˙ω. Thus, ˜c is an immersion if and only if κ and ˙ω have no common zeros. In this case, ˜ κ and φ = arctan (β/ ˙α) = arctan \u0010 2ω2κ ˙ω \u0011 are well-defined and ˜κ˜ω3 = ˜ω2β/α + ˙α ˙β − ¨αβ, which immediately implies the curvature formula (2).Elastic Analysis of Augmented Curves and Constrained Surfaces 5 Next, we apply the proposition to study some geometric quantities, which are invariant under the SRV transformation. For closed curves, integrating the curvature formula above over D = S1 (note that in this case, ˜ω >0 almost everywhere), we see that the SRV transformation preserves the total curvature and particularly the turning number. Moreover, κω is preserved if and only if κ = a d dt \u00001 ω \u0001 with a constant a. Clearly, with κ and ω at hand, utilizing Frenet equations, we can compute c up to rigid motions. The following explicit solution is an immediate application of the above proposition. In light of the above proposition, immersed curves, which are mapped to straight lines, can easily be determined as follows. Example 1. Let a, b, Abe constants withab, A >0, ω(t) = A/ sin2(at+b) and κ = a/ω. A straightforward computation, utilizing the curvature formula (2), implies ˜κ = 0. 2.3 Curves in Homogeneous Spaces For the background material on Lie groups and homogeneous spaces, we refer to [10]. The works [13,27] provide extensions of the SRV framework for euclidean curves to the case of general manifolds. The former has high computational cost, while the latter, transported SRV, depends on a reference point and also suffers from distortion or bias caused by holonomy effects. We use the natural extension to homogeneous spaces exposed in [26,9]. For reader’s convenience, we sketch the core ingredients of the approach and refer to the mentioned works for details and some applications. Let M be a homogeneous space, i.e., M = H/K, where K is a closed Lie subgroup of a Lie Group H. Let ∥·∥ denote the induced norm by a left invariant metric on H, L the tangent map of the left translation, and Imm(D, H) the space of immersed curves from D to H. The SRV transformation is given by Q(α) = (α(0), q(α)), where q(α) = Lα−1 ˙αp ∥ ˙α∥ Here, α−1(t) denotes the inverse element of α(t) in H and H the Lie algebra of H. The map Q is a bijection from Imm(D, H) onto H × L2(D, H). Now, M can be equipped with the Riemannian metric given by the pullback of the product metric of H × L2(D, H) using the map Q and horizontal lifting. Let c1 and c2 be immersed curves in M with horizontal lifts α1 and α2 respectively. The induced distance on M reads d(c1, c2) = inf \u001aq d2 H(α1(0), α2(0)x) + ∥q(α1) − Adx−1 (q(α2)∥2 L2 : x ∈ K \u001b . 3 Applications Frequently, besides spatiotemporal data, represented by a curve γ in a manifold M, there are additional or auxiliary information associated with the curve, thus with the same time-correspondence. These can jointly with γ be comprised and represented as a6 E. Nava-Yazdani so-called augmented curve ˜γ in a higher dimensional manifold ˜M. In some applications, the curve ˜γ uniquely determines a submanifold N of M via a natural construction. An important example is provided, when ˜M is a submanifold of the tangent bundle of M, where the auxiliary information is represented as a vector field along γ and the con- struction is given by the Riemannian exponential map. Significant special cases occur, when M is R3 or the unit two-sphere S2 and N a surface. In the next two subsections, we consider certain classes of surfaces in R3, which often arise in applications and are determined by augmented curves in R4. In the last two subsections, we consider certain spherical regions as well as hurricane tracks together with their intensities. In both cases, we utilize the Riemannian distance from subsection 2.3 to S2 × R, which is a homogeneous space (recall that S2 can be identified with SO(3)/SO(2)). For our example applications, we present geodesic paths representing deformations, minimizing the elastic energy within the SRV framework. We remark, that in a Rie- mannian setting, distance and geodesics are essential Building blocks for many major issues in the morphology and shape analysis, such as computation of mean and test statistics as well as principal component or geodesic analysis. Moreover, besides sta- tistical analysis, also some methods for clustering and classification use Riemannian metrics and geodesics. For the code implementing our approach, which particularly includes Riemannian optimization for the computation of geodesic paths, we utilized our publicly available python package https://github.com/morphomatics, introduced in [1]. 3.1 Tubes A tube or canal surface c is a one-parameter family of circles, whose centers constitute a regular curve γ such that the circles are perpendicular to γ. More precisely, denoting the radii of the circles by r, c(s, .) = γ + r(N cos s + B sin s), 0 ≤ s ≤ 2π, where N and B are the normal and binormal of the curve γ = γ(t), t∈ D, resp. Due to the unique correspondence of c to (γ, r), comparison of tubes reduces to comparison of curves in R4. Figure 1 shows some examples of shortest paths of tubes. Real world applications include a variety of fields such as examination of vein, pipes, capsules and plant roots. Clearly, tubes include surfaces of revolution. 3.2 Ruled Surfaces A ruled surface is formed by moving a straight line segment (possibly with varying length) along a base curve. More precisely, let γ be a curve in R3 and v a unit vector field along γ. Then c(s, .) = γ + sv, s∈ I, parametrizes a ruled surface generated by ( γ, v). Figure 2 depicts an example, where each surface consists of straight line segments connecting the blue (for better visibility) curves γ and γ + v. The class of ruled surfaces includes many prominent surfacesElastic Analysis of Augmented Curves and Constrained Surfaces 7 Fig. 1.Two shortest paths of tubes such as cone, cylinder, helicoid (a minimal surface) and M¨ obius strip. They arise in manufacturing (construction by bending a flat sheet), cartography, architecture and biochemistry (secondary and tertiary structure of protein molecules). Fig. 2.Shortest path of ruled surfaces 3.3 Spherical Strips Let exp denote the exponential map of the unit two-sphere S2. We recall that for any non-zero tangent vector to S2 at a point x: expx(v) = cos(|v|)x + sin(|v|) v |v|8 E. Nava-Yazdani and expx(0) = x. Now, let γ be a curve in S2 with binormal B (cross product of γ and its unit tangent), and r a scalar function along γ. Then, the map c given by c(s, .) := expγ s(rB), s∈ I, parametrizes a spherical strip with bandwidth r. Figure 3 depicts an example of the shortest path between two spherical curves comprised with their bandwidth functions visualised as strips. Fig. 3.Shortest path of spherical strips 3.4 Hurricane Tracks Hurricanes belong to the most extreme natural phenomena and can cause major im- pacts regarding environment, economy, etc. Intensity of a hurricane is determined by the maximum sustained wind (maxwind), monotonically classifying the storms into categories (due to Saffir–Simpson wind scale; for instance, maxwind ≥ 137 knots cor- responds to category 5). Due to their major impacts on economy, human life and envi- ronment, as well as extreme variability and complexity, hurricanes have been studies in a large number of works. For our example, we used the HURDAT 2 database provided by the U.S. National Oceanic and Atmospheric Administration publicly available on https://www.nhc.noaa.gov/data/, supplying latitude, longitude, and maxwind on a 6 hours base of Atlantic hurricanes. Fig. 4.2010 Atlantic hurricane tracks (left) and the shortest path between two of them (right) with color-coded maximum sustained wind (in knots)Elastic Analysis of Augmented Curves and Constrained Surfaces 9 We represent the tracks as discrete trajectories in S2. For further details and com- parison with other approaches, we refer to [24,25] and the recent work [21]. The latter, also provides statistical analysis and a classification of hurricane tracks in terms of their intensities. Fig. 4 illustrates this data set with a visualization of the 2010 hurricane tracks and a shortest path, where the intensities, considered as auxiliary information, are color-marked. 4 Conclusion In this paper, we analysed the behaviour of speed and curvature under the square root velocity framework for elastic approach to plane curves. Moreover, we applied an extension of this framework to homogeneous Spaces, to metrically compare augmented curves and special surfaces, generated by those curves, using a natural construction via the Riemannian exponential map. Our approach, allows for computationally efficient determination of geodesic paths in the shape spaces of the respective classes of surfaces. Our example applications include tubes, ruled surfaces, spherical strips and hurricane tracks. Future work includes further real world applications, particularly concerning statistical analysis of longitudinal data such as comparison of group wise trends within a hierarchical model as well as classification and prediction. Acknowledgements This work was supported through the German Research Foun- dation (DFG) via individual funding (project ID 499571814). References 1. Ambellan, F., Hanik, M., von Tycowicz, C.: Morphomatics: Geometric morphomet- rics in non-Euclidean shape spaces (2021). https://doi.org/10.12752/8544, https:// morphomatics.github.io/ 2. Bauer, M., Bruveris, M., Marsland, S., Michor, P.: Constructing reparametrization in- variant metrics on spaces of plane curves. arXiv: Differential Geometry (2012), https: //arxiv.org/pdf/1207.5965.pdf 3. Bauer, M., Bruveris, M., Charon, N., Møller-Andersen, J.: A relaxed approach for curve matching with elastic metrics. ESAIM: Control, Optimisation and Calculus of Variations 25 (03 2018). https://doi.org/10.1051/cocv/2018053 4. Bauer, M., Bruveris, M., Harms, Philipp Michor, P.W.: Soliton solutions for the elastic metric on spaces of curves. Discrete & Continuous Dynamical Systems - A 38, 1161–1185 (2018). https://doi.org/10.3934/dcds.2018049 5. Bauer, M., Bruveris, M., Michor, P.W.: Overview of the geometries of shape spaces and diffeomorphism groups. Journal of Mathematical Imaging and Vision 50(1-2), 60–97 (2014) 6. Bauer, M., Charon, N., Klassen, E., Brigant, A.L.: Intrinsic riemannian metrics on spaces of curves: theory and computation. arXiv preprint (2020), https://arxiv.org/abs/ 2003.05590 7. Bauer, M., Harms, P., Michor, P.W., et al.: Sobolev metrics on the manifold of all rie- mannian metrics. Journal of Differential Geometry 94(2), 187–208 (2013)10 E. Nava-Yazdani 8. do Carmo, M.P.: Riemannian Geometry. Mathematics: Theory and Applications, Birkh¨ auser Boston, Cambridge, MA, USA, 2 edn. (1992) 9. Celledoni, E., Eidnes, S., Schmeding, A.: Shape analysis on homogeneous spaces: a gener- alised srvt framework. In: Computation and Combinatorics in Dynamics, Stochastics and Control: The Abel Symposium, Rosendal, Norway, August 2016. pp. 187–220. Springer (2018) 10. Gallot, S., Hullin, D., Lafontaine, J.: Riemannian Geometry. Universitext, Springer, Berlin, 3 edn. (2004) 11. Huang, W., Gallivan, K.A., Srivastava, A., Absil, P.A.: Riemannian optimization for registration of curves in elastic shape analysis. Journal of Mathematical Imaging and Vision 54(3), 320–343 (2016) 12. Kendall, D., Barden, D. Carne, T., Le, H.: Shape and Shape Theory. John Wiley & Sons (1999) 13. Le Brigant, A.: Computing distances and geodesics between manifold-valued curves in the srv framework. Journal of Geometric Mechanics 9(2) (2017) 14. Liu, W., Srivastava, A., Zhang, J.: Protein structure alignment using elastic shape anal- ysis. In: Proceedings of the First ACM International Conference on Bioinformatics and Computational Biology. pp. 62–70 (2010) 15. Michor, P., Mumford, D., Shah, J., Younes, L.: A metric on shape space with explicit geodesics. Atti Accad. Naz. Lincei Cl. Sci. Fis. Mat. Natur. Rend. Lincei (9) Mat. Appl. 19 (07 2007). https://doi.org/10.4171/RLM/506 16. Michor, P.W., Mumford, D.: Vanishing geodesic distance on spaces of submanifolds and diffeomorphisms. Documenta Mathematica 10, 217–245 (2005) 17. Michor, P.W., Mumford, D.: An overview of the riemannian metrics on spaces of curves using the hamiltonian approach. Applied and Computational Harmonic Analysis 23(1), 74–113 (2007) 18. Mio, W., Srivastava, A., Joshi, S.H.: On shape of plane elastic curves. International Journal of Computer Vision 73, 307–324 (2006) 19. Mio, W., Srivastava, A., Joshi, S.: On shape of plane elastic curves. International Journal of Computer Vision 73, 307–324 (07 2007). https://doi.org/10.1007/s11263-006-9968-0 20. Nava-XYazdani, E., Hege, H.C., Sullivan, T.J., von Tycowicz, C.: Geodesic analysis in kendall’s shape space with epidemiological applications. Journal of Mathematical Imaging and Vision pp. 1–11 (2020). https://doi.org/10.1007/s10851-020-00945-w 21. Nava-Yazdani, E., Ambellan, F., Hanik, M., von Tycowicz, C.: Sasaki metric for spline models of manifold-valued trajectories. Computer Aided Geometric Design 104, 102220 (2023). https://doi.org/10.1016/j.cagd.2023.102220 22. Nava-Yazdani, E., Hege, H.C., von Tycowicz, C.: A hierarchical geodesic model for longi- tudinal analysis on manif olds. Journal of Mathematical Imaging and Vision 64(4), 395 – 407 (2022). https://doi.org/10.1007/s10851-022-01079-x 23. Needham, T., Kurtek, S.: Simplifying transforms for general elastic metrics on the space of plane curves. SIAM Journal on Imaging Sciences 13(1), 445–473 (2020). https://doi.org/10.1137/19M1265132 24. Srivastava, A., Klassen, E.P.: Functional and shape data analysis, vol. 1. Springer (2016) 25. Su, Z., Klassen, E., Bauer, M.: The square root velocity framework for curves in a ho- mogeneous space. 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) pp. 680–689 (2017) 26. Su, Z., Klassen, E., Bauer, M.: Comparing curves in homogeneous spaces. Differential Geometry and its Applications 60, 9–32 (2018) 27. Zhang, Z., Su, J., Klassen, E., Le, H., Srivastava, A.: Rate-invariant analysis of covariance trajectories. Journal of Mathematical Imaging and Vision 60, 1306–1323 (2018)",
      "references": [
        "Morphomatics: Geometric morphometrics in non-Euclidean shape spaces",
        "Constructing reparametrization in- variant metrics on spaces of plane curves",
        "A relaxed approach for curve matching with elastic metrics",
        "Soliton solutions for the elastic metric on spaces of curves",
        "Overview of the geometries of shape spaces and diffeomorphism groups",
        "Intrinsic riemannian metrics on spaces of curves: theory and computation",
        "Sobolev metrics on the manifold of all rie- mannian metrics",
        "Riemannian Geometry",
        "Shape analysis on homogeneous spaces: a gener- alised srvt framework",
        "Riemannian optimization for registration of curves in elastic shape analysis",
        "Shape and Shape Theory",
        "Computing distances and geodesics between manifold-valued curves in the srv framework",
        "Protein structure alignment using elastic shape anal- ysis",
        "A metric on shape space with explicit geodesics",
        "Vanishing geodesic distance on spaces of submanifolds and diffeomorphisms",
        "An overview of the riemannian metrics on spaces of curves using the hamiltonian approach",
        "On shape of plane elastic curves",
        "Geodesic analysis in kendall’s shape space with epidemiological applications",
        "Sasaki metric for spline models of manifold-valued trajectories",
        "A hierarchical geodesic model for longi- tudinal analysis on manif olds",
        "Simplifying transforms for general elastic metrics on the space of plane curves",
        "Functional and shape data analysis, vol. 1",
        "The square root velocity framework for curves in a ho- mogeneous space",
        "Comparing curves in homogeneous spaces",
        "Rate-invariant analysis of covariance trajectories"
      ],
      "meta_data": {
        "arxiv_id": "2402.04944v3",
        "authors": [
          "Esfandiar Nava-Yazdani"
        ],
        "published_date": "2024-02-07T15:25:20Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper analyzes the fundamental geometric properties of curves under the square root velocity (SRV) transformation. It introduces an elastic approach for intrinsic comparison within various classes of augmented curves and surfaces, including tubes, ruled surfaces, spherical strips, and hurricane tracks. Key contributions include exposing the behavior of speed and curvature under the SRV transformation for plane curves and extending the elastic approach to homogeneous spaces for metrically comparing augmented curves and special surfaces generated by them, allowing for computationally efficient determination of geodesic paths in their shape spaces.",
        "methodology": "The core methodology relies on the square root velocity (SRV) framework, which provides a computationally efficient elastic approach to curve analysis by transforming curves into a space where an L2-metric is flat. The paper extends this framework to homogeneous spaces (M = H/K) using an approach from [26,9], which avoids high computational costs and holonomy-induced distortions found in previous extensions. For augmented curves, a natural construction via the Riemannian exponential map is utilized. Geodesic paths, representing deformations, are computed by minimizing elastic energy within the SRV framework, with the implementation utilizing the publicly available `morphomatics` Python package.",
        "experimental_setup": "The experimental setup primarily involves showcasing geodesic paths (shortest paths) for various real-world applications. These include visualizations of shortest paths for tubes, ruled surfaces, and spherical strips. For hurricane tracks, the HURDAT 2 database from the U.S. National Oceanic and Atmospheric Administration (NOAA) was used, providing latitude, longitude, and maximum sustained wind data for Atlantic hurricanes. Tracks are represented as discrete trajectories in S2, and examples include a visualization of 2010 Atlantic hurricane tracks and a shortest path between two of them, with intensities color-coded. The underlying code for Riemannian optimization is part of the `morphomatics` Python package.",
        "limitations": "A notable limitation arises from the fact that some fundamental concepts and results from finite-dimensional differential geometry, such as the Hopf-Rinow theorem, do not directly apply to the infinite-dimensional spaces of curves considered. For plane curves, the SRV-transformed curve (˜c) is not necessarily an immersion unless its curvature (κ) and the derivative of its speed (˙ω) have no common zeros. Earlier extensions of the SRV framework to general manifold-valued data were noted to have high computational costs or suffer from distortion/bias due to holonomy effects, which the chosen extension to homogeneous spaces aims to mitigate.",
        "future_research_directions": "Future work includes further exploration of real-world applications, particularly in the statistical analysis of longitudinal data. Specific suggested directions involve comparing group-wise trends within hierarchical models, as well as developing methods for classification and prediction tasks based on the elastic analysis of augmented curves and surfaces.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Demystifying Long Chain-of-Thought Reasoning",
      "full_text": "Solving Nonlinear Absolute Value Equations Aris Daniilidis, Mounir Haddou, Tr´ı Minh Lˆe, Olivier Ley Abstract. In this work, we show that several problems, that are naturally represented as Nonlinear Absolute Value Equation problems (in short, NAVE), can be reformulated as Nonlinear Comple- mentarity Problems (in short, NCP) and, under mild conditions, they can be efficiently solved using smoothing regularizing techniques. To the best of our knowledge, this is the first numerical approach that deals directly with Nonlinear Absolute Value Equations. We also identify a commonly used technical assumption in smoothing techniques and prove its equivalence to a classical  Lojasiewicz inequality at infinity, confirming in particular that this assumption is not restrictive. The effective- ness of our approach is illustrated in several problems, including asymmetric ridge optimization and nonlinear ordinary differential equations. Keywords: Nonlinear Absolute Value Equality, Complementarity problem, P0-map, Numerical methods AMS Classification: Primary: 90C33, 65K10 ; Secondary: 15B48, 65K15, 90C59. 1 Introduction The last two decades, absolute value equation problems (in short, AVE problems) have been ex- tensively studied in the literature. This interest is justified by the fact that this class of problems already covers a wide spectrum of applications: indeed, numerous problems stemming from real-life applications, as for instance all mixed integer linear programming problems, can be reformulated as AVE problems. It is also well-known, see [24, 32] e.g., that AVE problems admit an equivalent description as Linear Complementarity problems (in short, LCP). The exact definitions of an AVE problem and a LCP are recalled below. Dealing efficiently with these problems is thus paramount. The literature on this subject contains several theoretical results for existence as well as conditions guaranteeing uniqueness of the solution [22, 23, 34, 35]. Concurrently, there are also various nu- merical approaches to solve an AVE problem. Generally speaking, these methods can be divided into at least three categories [3] : iterative linear algebra based methods (also known as projective methods), semi-smooth Newton-like methods and smoothing methods. The aforementioned meth- ods generally require some assumption on the matrix involved in the AVE problem. In particular, the classes of P0-matrices and P-matrices (recalled below) turn out to be particularly relevant in this study [1]. In this work, we consider a natural generalization of (linear) AVE problems to nonlinear ones, known as Nonlinear Absolute Value Equations (in short, NAVE). This more general framework encompasses new applications including ridge regression models, bounded constrained nonlinear systems of equations, and stiff Ordinary Differential Equations (in short, stiff ODE). This approach to deal with the aforementioned problems, based on NAVE, is to the best of our knowledge, com- pletely new in the literature. Our main contribution is the following: we first show that similarly to the way that an AVE problem 1 arXiv:2402.16439v2  [math.OC]  2 Oct 2024is linked to a LCP, nonlinear absolute value equations can also be associated to nonlinear comple- mentarity problems (in short, NCP). Indeed, any NCP can be reformulated as NAVE. The converse is also true, but the association is generally given in an implicit way. Then, taking profit from the huge literature concerning existence, uniqueness and numerical resolution of NCP (see [9,23,35,37] e.g.), we propose a new method to solve a NAVE problem, based on the smoothing technique proposed in [10] and further developments in the follow-up work [30]. The proposed approach is explained in Section 2, while in Section 3 we discuss applications. To ease the reading we start with some definitions and settings. The Absolute Value Equality problem (in short, AVE) is defined as follows: find x ∈ Rd : Ax − |x| = b, (AVE) where A is a ( d × d)-matrix and b ∈ Rd. Throughout this work, given x = (x1, . . . , xd)T ∈ Rd, we use the notation |x| := (|x1|, . . . ,|xd|)T componentwise to denote a vector in Rd +. Denoting by I the identity matrix of Rd and assuming that either A − I or A + I is invertible, the (AVE) problem can be transformed to a Linear Complementarity Problem (in short, LCP). Indeed, setting (coordinate by coordinate) ( y = x+ = max { x, 0} z = x− = max {−x, 0} (1.1) and performing the transformation x = y − z and |x| = y + z we obtain: (A − I) y − (A + I) z = b. Therefore for ( M := (A + I)−1(A − I) q := (A + I)−1(−b) or respectively ( fM := (A − I)−1(A + I) eq := (A − I)−1b (1.2) we obtain ( z = My + q 0 ≤ y ⊥ z ≥ 0 or respectively ( y = fMz + eq 0 ≤ y ⊥ z ≥ 0 (LCP) The above problem can be solved providedM (respectively fM) is a P-matrix (see below for details). It is important to notice that this property can be traced back to the matrix A; in particular, the property is ensured whenever the singular values of A are all greater than 1. Notice that this condition guarantees invertibility of both A − I and A + I. Solving (LCP) under the assumption that M (respectively fM) is a P-matrix has been treated in several works (see [4]). In this case, it can be shown that the problem has a unique solution (¯ y, ¯z) yielding that ¯x := ¯y − ¯z is the (unique) solution of (AVE). Moreover, this solution can be obtained numerically, via smoothing regularization techniques (see [1,10,30] and Section 2.3 below). In this work, we propose a new method of solving a nonlinear generalization of (AVE), namely the following Nonlinear Absolute Value Equality problem (in short, NAVE) Find x ∈ Rd : F(x) − |x| = 0 (coordinatewise), (NAVE) 2where F : Rd → Rd is a (nonlinear) mapping. By introducing new variables y = x+ and z = x− (cf. (1.1)) so that x = y − z and |x| = y + z, (NAVE) becomes F(y − z) − (y + z) = 0. By setting z = H(y) (which is possible under regularity assumptions on F, see Lemma 2.5), we can transform (NAVE) to a Nonlinear Complementarity Problem (NCP): ( H(y) = F(y − H(y)) − y 0 ≤ y ⊥ H(y) ≥ 0. (1.3) As we shall see in Section 2.3, even though the functionH is only defined implicitly, it is still possible to solve (1.3) numerically provided we are able to guarantee thatH is a P0-map (see Definition 2.2), a notion which generalizes P0–matrices (c.f. Lemma 2.7). This is the first work that directly addresses nonlinear absolute value equations, besides the fact that a variety of real problems and applications can naturally assume this form. Another important contribution related to smoothing techniques for NCP is that we clearly charac- terize a technical assumption commonly employed in functions–smoothing and prove its equivalence with the classical  Lojasiewicz inequality at infinity (see Theorem 2.8). This clearly reveals that the former assumption is not at all restrictive. 2 Setting of the problem and description of the method 2.1 Definitions and preliminaries Given a ( d × d) matrix A and I ⊂ {1, 2, ··· , d}, we denote by AII the submatrix made up of the rows and columns of I. Definition 2.1 (P0-matrix and P-matrix). A matrix A is called a P0–matrix (respectively, P– matrix) if one of the following equivalent properties holds (i) for every I ⊂ {1, 2, ··· , d}, det(AII ) ≥ 0 (respectively det(AII ) > 0); (ii) for every x = (x1, ··· , xd)T ∈ Rd, x ̸= 0, max 1≤j≤d (Ax)jxj ≥ 0 \u0012 respectively max 1≤j≤d (Ax)jxj > 0 \u0013 ; (iii) for every I ⊂ {1, 2, ··· , d}, the real eigenvalues of AII are nonnegative (respectively strictly positive). The notion of P-matrix can be generalized to general nonlinear maps H : Rd → Rd as follows: Definition 2.2 (P0–map and P–map). A mapping H : Rd → Rd is called P0-map (respectively, P-map), if for every x, y∈ Rd, x ̸= y, it holds: max j∈{1,...,d} \u0000 H(y)j − H(x)j\u0001 (yj−xj) ≥ 0 \u0012 respectively, max j∈{1,...,d} \u0000 H(y)j − H(x)j\u0001 (yj − xj) > 0 \u0013 ; 3If H is of the form H(x) = Ax + b for some ( d × d) matrix A and vector b ∈ Rd, then it follows directly that H is a P0–map (respectively, a P–map) if and only if A is a P0–matrix (respectively, a P–matrix). More generally, we have the following result: Lemma 2.3 ([27, Corollary 5.3, Theorem 5.8]) . Let H : Rd → Rd be C1. Then H is a P0-map if and only if, for every x ∈ Rd, ∇H(x) is a P0-matrix. We refer the reader to [8,27] for further results about P0– and P–matrices and maps. We finish this section with the following useful lemma. Lemma 2.4 (A characterization of P0–matrices). Let A be a (d×d) matrix. Then A is a P0-matrix if and only if, for every diagonal matrix ∆1 with strictly positive entries and for every nonnegative diagonal matrix ∆2, the matrix ∆1 + ∆2A is invertible. Proof. Let A be a P0-matrix. Then for every diagonal matrix ∆ 2 with nonnegative entries, the matrix ∆2A is also P0, while for every diagonal matrix ∆ 1 with strictly positive entries, the matrix ∆1 + ∆2A is a P-matrix, therefore, in particular, it is invertible. Conversely, let us assume that A is not a P0–matrix. Then there exists v ∈ Rd, v ̸= 0 such that (Av)ivi < 0, for every i ∈ {1, ··· , d}. (2.1) Let ∆1 = diag(δ1 1, ··· , δd 1) and ∆2 = diag(1, ··· , 1). Then (∆1 + ∆2A)v = (δ1 1v1 + (Av)1, ··· , δd 1vd + (Av)d)T , and by setting, for every i, δi 1 := −(Av)i/vi (which is well-defined and strictly positive thanks to (2.1)) we deduce that (∆ 1 + ∆2A)v = 0 and therefore ∆ 1 + ∆2A is not invertible. 2 2.2 Transforming a (NAVE) problem to a (NCP) problem Given a nonlinear mapping F : Rd → Rd, we consider the (NAVE) problem Find x ∈ Rd : F(x) − |x| = 0. By introducing new variables y = x+ and z = x− so that x = y −z, |x| = y + z, y ⊥ z, the (NAVE) problem becomes F(y − z) − (y + z) = 0. (2.2) The following lemma gives conditions under which (2.2) may be written as a (NCP) problem by setting y = H(z) or z = eH(y) for some suitable maps H or eH. Lemma 2.5. Assume that the mapping F is C1 in a neighborhood of the point x∗ = y∗ − z∗ ∈ Rd which is assumed to be solution of (2.2). Then it holds: (i). If F −I is a P0-map, then there exists a C1 map H : Rd → Rd defined in a neighborhood of y∗ such that z∗ = H(y∗) and y∗ is a solution to the following (NCP) problem: ( H(y) = F(y − H(y)) − y 0 ≤ y ⊥ H(y) ≥ 0. (2.3) 4(ii). If −(F + I) is a P0-map, then there exists a C1 map eH : Rd → Rd defined in a neighborhood of z∗ such that y∗ = eH(z∗) and z∗ is a solution to the following (NCP) problem: ( eH(z) = F( eH(z) − z) − z 0 ≤ z ⊥ eH(z) ≥ 0. (2.4) Proof. We consider the C1 map F : R2d → R defined by F(y, z) = F(y − z) − (y + z). We are going to apply the implicit function theorem at the point ( y∗, z∗) ∈ R2d. Notice that (∇yF(y∗, z∗), ∇zF(y∗, z∗)) = (∇F(y∗ − z∗) − I, −∇F(y∗ − z∗) − I). If F − I is a P0-map, then ∇F(x∗) − I is a P0-matrix by Lemma 2.3. Applying Lemma 2.4, we obtain that 2 I + ∇F(x∗) − I = ∇F(x∗) + I is invertible. Therefore ∇zF(y∗, z∗) is invertible and, by the implicit function theorem, we obtain a map H such that (i) holds. Similarly, if −(F + I) is a P0-map, then 2I − (∇F(z∗) + I) = I − ∇F(z∗) = −∇yF(y∗, z∗) is invertible and we obtain a map eH such that (ii) holds. 2 Remark 2.6. (i) The condition F − I (respectively, −(F + I)) being a P0-map is actually quite natural since it is exactly the requested assumptions to solve the (NCP) problem, see Section 2.3. (ii) (NAVE vs AVE). At this stage, the reader may have already noticed an analogy with the (LCP) reformulation of (AVE). Indeed, if F(x) = Ax−b, then (NAVE) coincides with (AVE), and if either A −I or −(A + I) is a P0-matrix (which is automatically satisfied if, e.g., the singular values of the matrix A are greater than 1), then the functions H and eH are explicitly given by the formulae H(y) = My + q and ˜H(y) = fMz + eq, where M, fM, q and eq appear in (1.2). Consequently, in this case it is possible to solve (AVE) as explained in the introduction. 2.3 Smoothing techniques to solve (NCP) As already mentioned, even though the functions H and eH are only implicitly defined, we can still solve (2.3)–(2.4) numerically (we shall do so below), whenever it is guaranteed that H, eH are P0-maps. This is the aim of the following lemma, yielding a criterium based on F. Lemma 2.7 (Guaranteeing P0-property for H, eH). (i). If F − I is a P0-map, then so is H. (ii). If −(F + I) is a P0-map, then so is eH. Proof. We now focus on the case of (2.3), the case of (2.4) can be adapted accordingly. Let y1, y2 ∈ Rd, with y1 ̸= y2. We infer from (2.3) that 2H(yk) = F (yk − H(yk)) − (yk − H(yk)) , k ∈ {1, 2}. 5Setting tk := yk − H(yk), it follows 2H(yk) = (F − I)(tk). Using the fact that F −I is a P0-map, we deduce that for some coordinate j = j(t1, t2) ∈ {1, . . . , d}, we have 2 \u0000 H(y1)j − H(y2)j\u0001 (tj 1 − tj 2) ≥ 0, from which we infer \u0000 H(y1)j − H(y2)j\u0001 \u0010 (yj 1 − yj 2) − (H(y1)j − H(y2)j) \u0011 ≥ 0, yielding \u0000 H(y1)j − H(y2)j\u0001 (yj 1 − yj 2) ≥ \u0000 H(y1)j − H(y2)j\u00012 ≥ 0. This is the desired property for the map H. 2 To solve (2.3), we will apply the smoothing approach proposed in [10] and more precisely the non-parametric technique introduced in [30]. The overall approach of [10] is based on functions θ satisfying the following properties: • the function θ : R → (−∞, 1) is concave, continuous and nondecreasing; • θ(t) < 0, for all t ∈ (−∞, 0), θ(0) = 0 and lim t→+∞ θ(t) = 1. These functions are used as certificate of positivity, that is, they “detect” whether t = 0 or t >0 holds in a “continuous way”, in the sense of the following characterization: t >0 ⇐⇒ lim r→0 θ \u0012t r \u0013 = 1. The authors in [10] used these functions to regularize the (nonsmooth) (NCP) 0 ≤ x ⊥ H(x) ≥ 0, (2.5) by means of a sequence of smooth systems (indexed by r >0) of the form Gr(x, H(x)) = \u0010 Gr(x, H(x))1, ··· , Gr(x, H(x))d \u0011T = (0, ··· , 0)T , (2.6) where Gr(x, H(x))i := rψ−1 \u0014 ψ \u0012xi r \u0013 + ψ \u0012H(x)i r \u0013\u0015 with ψ := 1 − θ. Then they eventually take the limit as r tends to 0. Several convergence results have been established under the assumption that the problem has at least one solution and H is a P0–map. Although this approach is efficient numerically, it suffers from two drawbacks: • There is no clear or optimal strategy to drive the parameter r to 0. 6• The following ad hoc technical assumption on the function ψ has been used without rigorous explanation: there exist a ∈ (0, 1) and Ra > 0 such that: ψ(t) 2 ≥ ψ \u00121 at \u0013 , for all t ∈ (Ra, +∞). (2.7) The first drawback has been addressed in [30] by considering a larger system of equations ( Gr(x, H(x)) = (0, ··· , 0)T , 1 2 ∥x−∥2 + 1 2 ∥H(x)−∥2 + r2 + εr = 0, (2.8) where ε >0 is some positive parameter. The second drawback will be the subject of the following result which proves that this technical assumption (2.7) corresponds to a well-known property. Theorem 2.8 (asymptotic behavior) . Let ψ : (0 , ∞) → (0, ∞) be a convex decreasing function satisfying lim x→∞ ψ(x) = inf ψ = 0. The following assertions are equivalent: (i). ( Lojasiewicz inequality at infinity) There exists c >0 such that lim inf x→∞ x|ψ′(x)| ψ(x) > c >0. (ii). There exist m, n >1 and R >0 such that: ψ(x) m ≥ ψ(nx), for all x ∈ (R, +∞) (2.9) (iii). For every m >1 there exist n >1 and R >0 such that: ψ(x) m ≥ ψ(nx), for all x ∈ (R, +∞) Notice that the technical assumption (2.7) corresponds to (ii). Therefore, the above result shows that it is equivalent to assume that ψ satisfies the  Lojasiewicz inequality at infinity. This latter condition is always satisfied if the function ψ is semialgebraic: Indeed, in this case, the correspond- ing Hardy field (that is, the field of germs of real semialgebraic functions at infinity) has rank one, and consequently, for any non-ultimately zero semi-algebraic function ψ in the single variable x, the function x 7→ xψ′(x)/ψ(x) has a non-zero limit as x goes to infinity (see [7, Remark 2.9]). The same argument applies also for the more general case of functions ψ that are definable in some polynomially bounded o-minimal structure (we refer to [6] for the corresponding definitions). Proof. (i)⇒(ii). Let us assume that (ii) fails. We define inductively a sequence {yn}n ⊂ [1, +∞) such that lim n→∞ yn = +∞ and lim n→∞ yn|ψ′(yn)| ψ(yn) = 0. 7To this end, we set x1 = y1 = 1. Since (ii) fails, for every n ≥ 2, taking m = 1 + 1 n and R = yn−1 we obtain the existence of some xn > Rsuch that for yn := nxn it holds ψ(xn) m < ψ(yn) yielding ψ(xn) ψ(yn) − 1 < m− 1 = 1 n. (2.10) Using convexity we also deduce that \f\fψ′(yn) \f\f ≤ ψ(xn) − ψ(yn) yn − xn = \u0012 n n − 1 \u0013\u0012 ψ(xn) − ψ(yn) yn \u0013 , whence, from (2.10), 0 ≤ yn|ψ′(yn)| ψ(yn) ≤ \u0012 n n − 1 \u0013\u0012 ψ(xn) ψ(yn) − 1 \u0013 < 1 n − 1. Taking the limit as n → ∞we conclude that (i) also fails to hold, which establishes the desired implication. (ii)⇒(iii). Assume that (2.9) holds for some m0 > 1, n0 and R0 > 1, that is, for all x > R0 we have ψ(x) ≥ m0 ψ(n0x). Then since n0x > x > Rwe also have: ψ(n0x) ≥ m0 ψ(n2 0x) yielding ψ(x) ≥ m2 0 ψ(n2 0x). We conclude that (2.9) also holds for m1 = m2 0 (under the choice of n1 = n2 0 and R1 = R0). Repeating this argument we deduce that (2.9) holds for all mk = mk 0, k≥ 1 (taking nk = nk 0 and Rk = R0). Since mk → ∞, in order to establish (iii) it is sufficient to observe that if (2.9) holds for some ¯m >1 (together with some ¯n >1 and ¯R >0) then it also holds for all m ∈ (1, ¯m], since ψ(x) m ≥ ψ(x) ¯m . (iii)⇒(i). Fix m >1, n >1 and R >0 such that (2.9) holds and set c := \u0012m − 1 m \u0013\u0012 1 n − 1 \u0013 > 0. Using convexity of ψ and (2.9), we deduce that for all x > Rwe have: \f\fψ′(x) \f\f ≥ ψ(x) − ψ(nx) nx − x =⇒ x|ψ′(x)| ψ(x) ≥ \u0012 1 n − 1 \u0013\u0012 1 − ψ(nx) ψ(x) \u0013 ≥ c. This establishes (i) and finishes the proof. 2 Remark 2.9. As already mentioned, assertion (i) ( Lojasiewicz inequality at infinity) holds true whenever the function ψ is semialgebraic (or more generally, definable in a polynomially bounded o-minimal structure). This already provides a broad assembly of examples of functions satisfy- ing (i), together with straightforward criteria to detect easily whether the property holds, based on certificates of semialgebricity or o-minimality (see [6, Theorem 1.13] e.g.). This being said, let us draw reader’s attention to the fact that besides what is asserted in [7, Propo- sition 2.7], the assumption of polynomial boundedness is essential for the validity of (i). Indeed, as shown in [21, Remark 8], the convex function ψ(x) = (log(1 + x))−1 is definable in the log-exp o-minimal structure but fails to satisfy (i). 82.4 Algorithm and numerical results To solve the system of equation (2.8), we will apply the Newton-like method proposed in [30]. However, since H is defined implicitly, we first need to reformulate the problem as follows:    z − F(y − z) + y = 0 rψ−1 \u0010 ψ(yi r ) + ψ(zi r ) \u0011 = 0 i = 1 . . . d, 1 2 ∥y−∥2 + 1 2 ∥z−∥2 + r2 + εr = 0, (2.11) where the variable z plays the role of H(y). Remark 2.10. In this new system of equations we assume that case (i) of Lemma 2.5 holds. (One can proceed in a similar way if (ii) holds.) In the definition of the following algorithm, we set X := (y, z, r)T and H(X) :=    z − F(y − z) + y rψ−1 \u0010 ψ(yi r ) + ψ(zi r ) \u0011 i = 1 . . . d, 1 2 ∥y−∥2 + 1 2 ∥z−∥2 + r2 + εr (2.12) so that (2.11) is reduced to H(X) = 0. This algorithm corresponds to a Newton method under a standard Armijo line search. Algorithm 1. Chose X0 = (X0, r0), X0 ∈ Ξ, r0 = ⟨y0, z0⟩/n, τ ∈ (0, 1/2), ϱ∈ (0, 1). Set k = 0. 2. If H(Xk) = 0, stop. 3. Find a direction dk ∈ R2n+1 such that H(Xk) + ∇XH(Xk)dk = 0. 4. Choose ζk = ϱjk ∈ (0, 1), where jk ∈ N is the smallest integer such that Θ(Xk + ϱjk dk) − Θ(Xk) ≤ τϱjk ∇Θ(Xk)T dk. 5. Set Xk+1 = Xk + ζkdk and k ← k + 1. Go to step 2 . The merit function used in the line search corresponds to the square of the global error: Θ(X) = 1 2∥H(X)∥2. To get a well defined algorithm, the initial point ( y0, z0)T must be an interior point, and the initial value for r must be positive r0 > 0. 93 Applications In this section we show that several problems, which can be naturally restated as (NAVE) and can be solved efficiently thanks to the above transformation. We present in this section numerical experiments, in which the smoothing functions are restricted to two specific cases θ1(t) :=    t t + 1, t ≥ 0 t, t < 0 and θ2(t) := 1 − e−t. The numerical experiments are conducted in an ordinary computer. All program codes are written and executed in MATLAB R2023a. In Subsection 3.1 and 3.3, we employ a similar stopping criterion for every numerical method, by using a tolerance T ol= 1e−10 and fixing the maximum number of iterations to Nmax = 2000. Since the NAVE problems may have multiple solutions, in the following, the error will be computed by Error = ∥F(xapproximate) − |xapproximate|∥ in Subsection 3.1 and respectively by Error = ∥F(xapproximate) − |xapproximate| −b∥) in Subsection 3.3). 3.1 Ridge Regression Ridge regression adds to the loss function L(x), x∈ Rd a penalty term in order to avoid overfitting: historical development and the applications in data science of ridge regression can be found e.g. in [12, 14]. This penalty term usually consists of adding the squared magnitude of the coefficients (traditionally denoted by w). We hereby consider an asymmetric ridge regression of the form: min x∈Rd   L(x) + dX j=1 \u0000 λj max{xj, 0}2 + µj max{−xj, 0}2\u0001   , (3.1) where the penalization parameters λj and µj satisfy λj − µj ̸= 0 for all j ∈ {1, ··· , d}. The case λj = µj = λ for every j corresponds to the classical ridge regression, which will not be considered here. On the other hand, the case λj = 0 for all j and µj > 0, corresponds to a penalization of the negativity of the coefficients, promoting solutions with positive coefficients. The necessary condition for optimality reads as follows: ∇L(x) + 2λ max{x, 0} −2µ max{−x, 0} = 0, where the two vectorsλ max{x, 0} and µ max{−x, 0} are to be understood componentwise. Noticing that 2 max{x, 0} = |x|+x and 2 max{−x, 0} = |x|−x, we end up with the following (NAVE) problem F(x) − |x| = 0 with F(x) = \u0012 1 µ − λ \u0013 ∇L(x) + \u0012µ + λ µ − λ \u0013 x (coordinatewise) Therefore, one can solve the previous problem if either F − I or −(F + I) is a P0–map, that is either ( µ − λ)−1 (∇L + 2λI) or − (µ − λ)−1 (∇L + 2µI) is a P0–map. To illustrate for asymmetric ridge regression, we consider the loss function L(x) = 1 2∥Ax − b∥2, where A ∈ Rm×d and b ∈ Rd. (3.2) 10We performed numerical experiments, fixing λj = ¯λ and µj = ¯µ for every j ∈ {1, ··· , d}. These parameters, matrix A ∈ Rm×d and vector b ∈ Rd were randomly generated with values in [ −5, 5]. As shown in Table 1, considering the average number of iterations with similar tolerance, using the function θ2 is better, while in an exceptional case m = 20 > d = 10 and ( ¯λ, ¯µ) = (0 , 100), θ2–smoothing performs worse. On the other hand, while the parameters ¯λ and ¯µ become greater, which can be compared to the ascent of the (classical) ridge parameter, θ2–smoothing performs within a better tolerance in a small number of iterations. Table 1: Comparing (asymmetric) ridge regression with different smoothing functions Error Iterations Running time( ×e − 2(s)) (¯λ, ¯µ) ( m, d) θ1 θ2 θ1 θ2 θ1 θ2 (0, 100) (3 , 10) 1 .9e − 11 7 .3e − 15 18 21 4 .81 2 .96 (5, 10) 5 .8e − 11 1 .3e − 16 17 71 4 .78 6 .26 (10, 10) 5 .4e − 11 1 .9e − 16 18 24 5 .78 3 .45 (20, 10) 3 .8e − 11 3 .5e − 3 17 2000 4 .94 819 (200, 1000) (3 , 10) 8 .9e − 11 1 .4e − 17 18 23 5 .62 2 .99 (5, 10) 2 .7e − 11 3 .6e − 18 19 22 5 .12 2 .9 (10, 10) 6 e − 11 2 .7e − 17 18 33 4 .93 3 .87 (20, 10) 4 .18e − 11 1 .2e − 10 18 40 5 .41 4 .17 To end this part, we give a heuristic observation on a sparse optimization problem (see e.g. [13,36]). Let us consider the following problem min x∈Rd L(x) + λ∥x∥1. (3.3) The first order optimality condition for (3.3) has the form 0 ∈ ∇L(x) + λ ∂∥ · ∥1(x), (3.4) where the subdifferential of ℓ1 norm can be written explicitly as q ∈ ∂∥ · ∥1(x) if and only if ( qi = sign(xi), if xi ̸= 0, |qi| ≤1, if xi = 0. Using the fact that α sign(α) = |α| for every α ∈ R, the inclusion (3.4) can be transformed into x∇L(x) + λ|x| = 0 (coordinatewise). The above equation just provides a necessary condition for optimal solution. In the following, we give a short numerical observation to guarantee its potential utility in sparse optimization. In order to apply results from previous sections, it is necessary to ensure that one of the following maps is a P0-map −1 λ x ∇L(x) − I and 1 λ x ∇L(x) − I. 11In the following figures, we use the same quadratic loss function (3.2), where matrix A and vector b are randomly generated ranging from −1 to 1 and −0, 05 to 0, respectively. Figure 1 shows the behavior of each coefficient while increasing the tuning parameter λ >0. (a) θ1–smoothing  (b) θ2–smoothing. Figure 1: Problem in dimension m = 20 and d = 40. 3.2 Nonlinear ordinary differential equations A NAVE problem also naturally arises when we deal with a discretization of a nonlinear ordinary differential equation (ODE, for short) involving rough velocity, for example ˙ γ(t) = p |γ(t)| as well as an ODE of the form Φ(X(2k), X(2k−1), . . . ,˙X) = |X| In this subsection we provide two examples (one being a stiff ODE) to illustrate the effectiveness of smoothing techniques when using finite difference schemes for ODEs. Example 3.1. We consider a stiff ODE with initial value as follows ( ¨x + 1001 ˙x − 1000|x| = 0, t >0 x(0) = x0 < 0, ˙x(0) = 0, (3.5) whose exact solution is xexact(t) = x0 \u0012 − 1 999e−1000t + 1000 999 e−t \u0013 ≈ x0e−t. Let us consider problem (3.5) in time domain I = [0, T]. We use a uniform mesh ttt = (ti), where ti = ih for i ∈ {0, ··· , N} and h = T/N , and the approximation solution will be xxx = (xi) where xi ≈ x(ti). For the first and the second derivative, we use the 2nd–order approximation ¨x(ti) ≈ xi−2 − 2xi−1 + xi h2 and ˙x(ti) ≈ xi+1 − xi−1 2h . Remarkably, at the final time, the first derivative ˙x(tN ) will be computed via the 2nd–order backward formula ˙x(tN ) ≈ (xN−2 − 4xN−1 + 3xN )/2h. Since the initial velocity is zero, using 1st–order backward approximation, we note that x−1 = x0. The discretization of (3.5) can be written as 1 1000AAAxxx + 1001 1000BBBxxx − |xxx| = bbb, 12where AAA,BBB ∈ RN×N is determined by AAA = 1 h2   1 0 0 ··· 0 0 0 −2 1 0 ··· 0 0 0 1 −2 1 ··· 0 0 0 ··· ··· ··· ··· ··· ··· ··· 0 0 0 ··· 1 0 0 0 0 0 ··· − 2 1 0 0 0 0 ··· 1 −2 1   and BBB = 1 2h   0 1 0 ··· 0 0 0 −1 0 1 ··· 0 0 0 0 −1 0 ··· 0 0 0 ··· ··· ··· ··· ··· ··· ··· 0 0 0 ··· 0 1 0 0 0 0 ··· − 1 0 1 0 0 0 ··· 1 −4 3   (3.6) Here, vector bbb = (bi) ∈ RN is defined by bi = 0 for i ≥ 2, b1 = x0(1/(1000h2) + 1001/(2000h)) and b2 = −x0/(1000h2). In Figure 2.(a), we approximate the solution of equation (3.5) with initial condition x0 = −1 and time interval I = [0, 5]. The finite difference scheme was computed with mesh size h = 0.05 and the error is 9 .22e − 4 when applying θ1 and θ2 smoothing funtions. To get convergence rate in Figure 2.(b), we apply difference mesh sizes in the same time interval I = [0, 1] and intial condition x0 = −2. (a) Approximate solutions.  (b) Convergence rate O(h1/2). Figure 2: Solving equation 3.5 Let us now make some comments on the utility of NAVE for boundary value problems: we consider a boundary value problem related to (3.5) ( ¨x + 1001 ˙x − 1000|x| = 0, t∈ (0, T), x(0) = x0 < 0, x(T) = y0 ∈ R. (3.7) In order to illustrate this case, we consider the time interval I = [0, 2] and exact solution is deter- mined by xexact. Using similar time mesh as above, the first and second derivatives are approximate as follows ¨x(ti) ≈ xi−1 − 2xi + xi+1 h2 and ˙x(ti) ≈ xi − xi−1 h . Figure 3 shows the convergence rate for the boundary value problem (3.7). The smoothing technique used in this problem presents a better accuracy compared to the above initial value problem, which 13seems to be natural because of the stiffness of the problem (3.5). It is noteworthy that Figure 3 also depicts an expected convergence rate since we have used a first order approximation for ˙ x. Figure 3: Convergence rate O(h) for a boundary value problem Example 3.2. For a continuous function f : [0, +∞) → R, we consider an ODE ( ¨x + arctan(x) − |x| = f(t), t >0, x(0) = x0 ∈ R, ˙x(0) = 0. (3.8) Using a similar discretization as in Example 3.1, the unknown variable xxx ≈ x(ttt) solves a NAVE problem as follows AAAxxx + arctan(xxx) − |xxx| = bbb, (3.9) where the matrix AAA is determined as in Example 3.1 and the vector bbb ∈ RN is defined by bi = f(ti) for i ≥ 2 and b1 = f(t1) + x0 h2 and b2 = f(t2) − x0 h2 . To illustrate for this example, we consider problem (3.8) with source term f(t) = arctan(cos(πt)) − |cos(πt)| −π2 cos(πt), whose exact solution is xexact(t) = cos( πt). Figure 4.(a) shows the approximate solution on the time interval I = [0, 1] with mesh size h = 0.0125. The error between θ1 (resp. θ2) approximation and exact solution is 0 .06 (resp. 0 .0725). Besides, Figure 4.(b) displays convergence rate of the combination of finite difference scheme and the θ–smoothing applying for the associated NAVE problem. 14(a) Approximate solutions.  (b) Convergence rate O(h). Figure 4: Solving equation 3.8. 3.3 Comparison of methods for NAVE Instead of smoothing procedure considered in Section 2, one can solve a NCP via other numerical methods. In this subsection we give examples to compare the efficiency of four methods • Newton–like method with smoothing functions θ1 and θ2; • approximating by Soft–Max function, in which the main idea is to approximate the comple- mentarity condition via the limit max i∈{1,···,d} xi = lim r↘0 r log  dX i=1 exi/r ! . which have been widely used in many optimization problems, for example [31, Example 1.30], [19,20,28]; • using interior point method, for example, one can find the use of interior point method for complementarity problems in [11,15,17,29]. Now, in the following examples, we solve the system ˜F(x) −|x| = b, especially, Example 3.4 and 3.5 can be found in [2,18]. Example 3.3. We consider ˜F(x) = Ax, where A = tridiag(−1, 4, −1) ∈ Rd×d, x∗ ∈ Rd, b= Ax∗ − |x∗|. (3.10) Example 3.4. ˜F : R3 → R3 is defined by ˜F(x) :=   2x1 − 2 2x2 + x3 2 − x3 + 3 x2 + 2x3 + 2x3 3 − 3  . 15Example 3.5. ˜F : R4 → R4 is defined by ˜F(x) :=   3x2 1 + x1 + 2x1x2 + 2x2 2 + x3 + 3x4 2x2 1 + x1 + x2 2 + x2 + 10x3 + 2x4 3x2 1 + x1x2 + 2x2 2 + 3x3 + 9x4 x2 1 + 3x2 2 + 2x3 + 4x4  . Table 2 compares the four methods: smoothing method with θ1 and θ2, Soft Max (denoted SM) and Interior Point (denoted IP) method on the NAVE problem associated to Example 3.3–3.5. In the first example, the vector b is randomly generated with values in [−5, 5] and the problem is considered in dimensions d = 10, 50, 200. In Example 3.4 and 3.5, we respectively consider b1 = (−1, −5, 10)T , b2 = (9, −100, 10)T , b3 = (200, 0, 900)T , b∗ 1 = (10, 10, −12, 0)T , b∗ 2 = (20, −100, −12, 1)T and b∗ 3 = (200, 10, −5, −5)T . We observe that the smoothing method (especially with θ2–smoothing function) is the most robust among the considered methods. In connection with convergence speed, the interior point method performs much less competitively than the others, while it only reaches 1e−2 after N = 2000 iterations. Another point that can be recognized from Table 2 is that the Soft Max method could only solve problems with small size, for example for problems in dimension N = 50 and N = 200 the singularities appear after less than 100 iterations. Table 2: Several methods to solve NAVE Error Iterations Example Vector b θ 1 θ2 SM IP θ1 θ2 SM IP 3.3 d = 10 9 .3e − 11 1 .5e − 11 5 .5e − 12 1 .6e − 2 20 13 173 2000 d = 50 1 .4e − 11 5 .3e − 15 NaN 8.96e − 3 29 41 41 2000 d = 200 8 .39e − 11 1 .32e − 14 NaN 9.55e − 3 45 76 96 2000 3.4 b1 4.7e − 11 1 .7e − 11 7 e − 14 5 .5e − 2 14 9 8 2000 b2 1.7e − 11 3 .6e − 11 1 .8e − 12 4 e − 1 22 16 15 2000 b3 9.3e − 11 1 .4e − 13 1 .4e − 13 2 e + 2 211 205 205 2000 3.5 b∗ 1 5.5e − 11 1 .3e − 14 2 e − 14 7 e − 2 16 12 12 2000 b∗ 2 5.5e − 11 2 .2e − 14 4 .7e − 14 9 .1e − 1 26 22 16 2000 b∗ 3 6.1e − 11 1 .4e − 10 3 e + 1 3 .8e − 0 50 43 2000 2000 Figure 5(a) and 5(b) display the performance time between different methods for Example 3.3 with the size n = 20 and Example 3.4, respectively. We did the observation with 50 samples and the vector b is randomly generated with values in [ −10, 10]. At a first sight, the interior point method appears to be the slowest one in comparison with the other three methods. As shown in Figure 5(a), the θ1–smoothing performs the best choice among all the methods. If we look carefully, in lower dimension as Example 3.4, the Soft Max and θ2–smoothing performs slightly better than θ1–smoothing method. 16(a) Example 3.1 in dimension n = 20.  (b) Example 3.2. Figure 5: Performance time. 4 Conclusion and discussion In this work, we applied smoothing techniques commonly used for Nonlinear Complementarity Problems (NCP) to Nonlinear Absolute Value Equations (NAVE). We first showed that a NAVE can be formulated as a NCP with an implicitly known corresponding mapping. We then established that a NAVE can be effectively addressed under a mild direct assumption on the NAVE function. Additionally, we clarified a technical assumption used in some smoothing approaches, proving its equivalence to a  Lojasiewicz inequality, thus showing its broad applicability. Last but not least, we provided illustrative examples and applications that through numerical verification reveal effective- ness and potential of this approach. In a future work, we aim to establish error bounds or estimations and study the complexity of the proposed method in favorable situations, such as when the implicitly corresponding mapping in the NCP formulation is monotone or strongly monotone. Acknowledgement. This work was initiated during a research stay of Aris Daniilidis and Tr´ ı Minh Lˆ e to INSA Rennes (February 2023). These authors thank their hosts for hospitality. The first author acknowledges support from the Austrian Science Fund (FWF, P–36344-N). References [1] L. Abdallah, M. Haddou, T. Migot, Solving absolute value equation using complemen- tarity and smoothing functions. J. Comput. Appl. Math. 327 (2018), 196–207. [2] J. H. Alcantara, J-S. Chen. A new class of neural networks for NCPs using smooth per- turbations of the natural residual function, J. Comput. Appl. Math. 407 (2022), Paper No. 114092, 22 pp. [3] J. H. Alcantara, J.-S. Chen, M. K. Tam,Method of alternating projections for the general absolute value equation. J. Fixed Point Theory Appl. 25 (2023), Paper No. 39, 38 pp. 17[4] I. Ben Gharbia J.C. Gilbert,Nonconvergence of the plain Newton-min algorithm for linear complementarity problems with a P-matrix, Mathematical Programming 134 (2012), 349–364. [5] J. Y. Bello Cruz, O. P. Ferreira, L. F. Prudente, On the global convergence of the inexact semi-smooth Newton method for absolute value equation, Comput. Optim. Appl. 65 (2016), 93–108. [6] M. Coste, An Introduction to o-minimal Geometry , Instituti Editoriali e Poligrafici Inter- nazionali, Pisa (2000). [7] D. Dacunto, V. Grandjean, A gradient inequality at infinity for tame functions, Rev. Mat. Complut. 18 (2005), 493–501. [8] M. Fiedler, V. Pt ´ak, On matrices with nonpositive off-digagonal elements and positive principal minors, Czech. Math. J. 12 (1962), 382–400. [9] S.-L. Hu and Z.-H. Huang, A note on absolute value equations, Optim. Lett. 4 (2010), 417–424. [10] M. Haddou, P. Maheux, Smoothing methods for nonlinear complementarity problems, J. Optim. Theory Appl. 160 (2014), 711–729. [11] M. Haddou, T. Migot, J. Omer, A generalized direction in interior point method for monotone linear complementarity problems, Optim. Lett. 13 (2019), 35–53. [12] T. Hastie , Ridge regularization: An essential concept in data science, Technometrics 62 (2020), 426–433. [13] T. Hastie, R. Tibshirani, J. Friedman, The elements of statistical learning. Data mining, inference and prediction, Springer (New York, 2001). [14] R. W. Hoerl, Ridge regression: A historical context, Technometrics 62 (2020), 420–425. [15] A. N. Iusem, An interior point method for the nonlinear complementarity problem, Appl. Numer. Math. 24 (1997), 469–482. [16] C. Kanzow, A new approach to continuation methods for complementarity problems with uniform P-functions, Oper. Res. Lett. 20 (1997), 85–92. [17] M. Kojima, N. Megiddo, T. Noma, A. Yoshise, A unified approach to interior point algorithms for linear complementarity problems , Lecture Notes in Comput. Sci. 538, Springer (Berlin, 1991). [18] M. Kojimam S. Shindo, Extension of Newton and quasi-Newton methods to systems of PC 1 equations, J. Oper. Res. Soc. Jpn. 29 (1986), 352–374. [19] X. Li, An entropy–based aggregate method for minimax optimization, Eng. Opt. 18 (1992), 277–285. [20] Y. Li, T. Tan, X. Li, A log-exponential smoothing method for mathematical programs with complementarity constraints, Appl. Math. Comput. 218(2012), 5900—5909. 18[21] T. L. Loi ,  Lojasiewicz inequalities in o-minimal structures, Manuscripta Math. 150 (2016), 59–72. [22] T. Lotfi and H. Veiseh, A note on unique solvability of the absolute value equation, 2013. [23] O. L. Mangasarian, RR. Meyer, Absolute value equations, Linear Algebra Appl. 419 (2006), 359–367. [24] O. L. Mangasarian, Linear complementarity as absolute value equation solution, Optim. Lett. 8 (2014), 1529–1534. [25] MATLAB R2023A, Natick, Massachusetts: The MathWorks Inc., 2010. [26] J.-J. Mor´e, Global methods for nonlinear complementarity problems, Math. Oper. Res. 21 (1996), 589–614. [27] J.-J. Mor´e , W.C. Rheinboldt,On P- and S- functions and related classes ofn- dimensional nonlinear mappings, Linear Algebra Appl. 6 (1973), 45–68. [28] Y. Nesterov, Smooth minimization of nonsmooth functions, Math. Program. Ser. A 103 (2005), 127–152. [29] F. A. Potra, Y. Ye, Interior-point methods for nonlinear complementarity problems, J. Optim. Theory Appl. 88(1996), 617–642. [30] E. H. Osmani, M. Haddou, N. Bensalem, L. Abdallah, A new smoothing method for nonlinear complementarity problems involving P0-function. Stat. Optim. Inf. Comput. 10 (2022), 1267–1292. [31] R. T. Rockafellar, J.-B. R. Wets, Variational analysis, Grundlehren Math. Wiss. 317, Springer (Berlin, 1998). [32] O. Prokopyev, On equivalent reformulations for absolute value equations, Comput. Optim. Appl. 44 (2009), 363–372. [33] J. Rohn, A theorem of the alternatives for the equation ax + b|x| = b, Linear Mult. Algebra 52 (2004), 421–426. [34] J. Rohn, On unique solvability of the absolute value equation,Optim. Lett. 3 (2009), 603–606. [35] J. Rohn, V. Hooshyarbakhsh, and R. Farhadsefat, An iterative method for solving absolute value equations and sufficient conditions for unique solvability, Optim. Lett. 8 (2014), 35–44. [36] R. Tibshirani, Regression shrinkage and selection via the lasso, J. Roy. Statist. Soc. Ser. B 58 (1996), 267–288. [37] S.-L. Wu and C.-X. Li, A note on unique solvability of the absolute value equation, Optim. Lett. 14 (2020), 1957–1960. 19Aris Daniilidis, Tr´ ı Minh Lˆ e Institut f¨ ur Stochastik und Wirtschaftsmathematik, VADOR E105-04 TU Wien, Wiedner Hauptstraße 8, A-1040 Wien E-mail: {aris.daniilidis, minh.le}@tuwien.ac.at https://www.arisdaniilidis.at/ Research supported by the grants: Austrian Science Fund (FWF P-36344N) (Austria) Mounir Haddou, Olivier Ley Univ Rennes, INSA, CNRS, IRMAR - UMR 6625, F-35000 Rennes, France E-mail: {mounir.haddou, olivier.ley}@insa-rennes.fr http://{haddou, ley}.perso.math.cnrs.fr/ Research supported by the Centre Henri Lebesgue ANR-11-LABX-0020-01. 20",
      "references": [
        "Solving absolute value equation using complementarity and smoothing functions.",
        "A new class of neural networks for NCPs using smooth perturbations of the natural residual function",
        "Method of alternating projections for the general absolute value equation.",
        "Nonconvergence of the plain Newton-min algorithm for linear complementarity problems with a P-matrix",
        "On the global convergence of the inexact semi-smooth Newton method for absolute value equation",
        "An Introduction to o-minimal Geometry",
        "A gradient inequality at infinity for tame functions",
        "On matrices with nonpositive off-digagonal elements and positive principal minors",
        "A note on absolute value equations",
        "Smoothing methods for nonlinear complementarity problems",
        "A generalized direction in interior point method for monotone linear complementarity problems",
        "Ridge regularization: An essential concept in data science",
        "The elements of statistical learning. Data mining, inference and prediction",
        "Ridge regression: A historical context",
        "An interior point method for the nonlinear complementarity problem",
        "A new approach to continuation methods for complementarity problems with uniform P-functions",
        "A unified approach to interior point algorithms for linear complementarity problems",
        "Extension of Newton and quasi-Newton methods to systems of PC 1 equations",
        "An entropy–based aggregate method for minimax optimization",
        "A log-exponential smoothing method for mathematical programs with complementarity constraints",
        "Lojasiewicz inequalities in o-minimal structures",
        "A note on unique solvability of the absolute value equation",
        "Absolute value equations",
        "Linear complementarity as absolute value equation solution",
        "Global methods for nonlinear complementarity problems",
        "On P- and S- functions and related classes ofn- dimensional nonlinear mappings",
        "Smooth minimization of nonsmooth functions",
        "Interior-point methods for nonlinear complementarity problems",
        "A new smoothing method for nonlinear complementarity problems involving P0-function",
        "Variational analysis",
        "On equivalent reformulations for absolute value equations",
        "A theorem of the alternatives for the equation ax + b|x| = b",
        "On unique solvability of the absolute value equation",
        "An iterative method for solving absolute value equations and sufficient conditions for unique solvability",
        "Regression shrinkage and selection via the lasso"
      ],
      "meta_data": {
        "arxiv_id": "2402.16439v2",
        "authors": [
          "Aris Daniilidis",
          "Mounir Haddou",
          "Tri Minh Le",
          "Olivier Ley"
        ],
        "published_date": "2024-02-26T09:36:31Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This work introduces the first numerical approach to directly solve Nonlinear Absolute Value Equations (NAVE) by reformulating them as Nonlinear Complementarity Problems (NCP). The authors demonstrate that, under mild conditions, NAVE problems can be efficiently solved using smoothing regularizing techniques. A significant contribution is the identification and proof of equivalence between a commonly used technical assumption in smoothing techniques and the classical Lojasiewicz inequality at infinity, confirming this assumption is not restrictive.",
        "methodology": "The core methodology involves transforming NAVE problems into NCPs by introducing new variables y=x+ and z=x-. This transformation allows leveraging existing literature on NCPs. The paper then applies smoothing techniques, specifically the non-parametric method from [30], to the reformulated NCP. This approach utilizes concave, continuous, nondecreasing functions θ to regularize the nonsmooth NCP into a sequence of smooth systems. These systems are subsequently solved using a Newton-like method with an Armijo line search. The method requires the underlying mapping F-I (or -(F+I)) to be a P0-map to guarantee the P0-property for the implicitly defined functions H, eH.",
        "experimental_setup": "The effectiveness of the proposed smoothing technique is illustrated through numerical experiments on several problems, including asymmetric ridge optimization and nonlinear ordinary differential equations (ODEs). For asymmetric ridge regression, the loss function L(x) = 1/2 ||Ax - b||^2 is used, with randomly generated matrices A and vectors b, and varying penalization parameters. For ODEs, a stiff ODE and an ODE involving arctan(x) are discretized using finite difference schemes (2nd-order for derivatives). Two specific smoothing functions, θ1(t) and θ2(t), are compared. The stopping criterion for all methods is set with a tolerance of 1e-10 and a maximum of 2000 iterations. Error is computed as ||F(x_approx) - |x_approx|| or ||F(x_approx) - |x_approx| - b||. The method's performance is also compared against Soft-Max and Interior Point methods on various NAVE problems, including linear and nonlinear examples. All programs are written and executed in MATLAB R2023a on an ordinary computer.",
        "limitations": "The smoothing regularization approach, prior to the improvements in [30], lacked a clear or optimal strategy for driving the regularization parameter 'r' to zero. Additionally, an ad hoc technical assumption on the smoothing function ψ (later proven equivalent to the Lojasiewicz inequality) was used without rigorous explanation. The transformation of NAVE to NCP generally results in an implicitly defined mapping. The comparative analysis showed that the Soft Max method can encounter singularities with larger problem sizes, and the Interior Point method exhibited less competitive convergence speed and performance time compared to the smoothing methods.",
        "future_research_directions": "Future work will focus on establishing error bounds or estimations for the proposed method. Additionally, the complexity of the method will be studied in favorable scenarios, particularly when the implicitly corresponding mapping in the NCP formulation exhibits properties such as monotonicity or strong monotonicity.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Chain-of-Thought Reasoning Without Prompting",
      "full_text": "Formation and Optimisation of Vein Networks inPhysarum Rodrigo Fernandes Ferreira de Almeida Thesis to obtain the Master of Science Degree in Engineering Physics Supervisors: Prof. Dr. Rui Manuel Agostinho Dilão Examination Committee Chairperson: Prof. Dr. Ilídio Lopes Supervisor: Prof. Dr. Rui Manuel Agostinho Dilão Member of the Committee: Profa. Dra. Ana Maria Ribeiro Ferreira Nunes July 2021 arXiv:2305.12244v1  [physics.flu-dyn]  20 May 2023iiTo my mother and my girlfriend. iiiivAcknowledgments I would like to express my profound appreciation and gratitude to my supervisor Professor Rui Dilão for the opportunity, his immense dedication and continued guidance, and for all the valuable insights provided. Most importantly, for never giving up on me. I am extremely grateful to my amazing and truly supportive girlfriend which has always been there for me, helped me with my struggles and celebrated my victories. Without your love, constant support and motivation I deﬁnitely couldn’t have done it. I’m also eternally grateful to my parents and family for their sacriﬁces and the unconditional support they provided me with in every moment of my life. In particular, to my mother, for always believing in me and for all the wise words and encouragement in this last year. Last but not least, I would like to thank all my dearest friends for helping me keep my sanity, and for all the pep talks and constant support throughout this long journey. I truly admire you all, and you inspired me to be who I am today. I couldn’t have asked for better. vviResumo OPhysarum polycephalumé um bolor limoso acelular que se desenvolve como uma rede de veias altamente adaptativa onde circula o protoplasma. À medida que procura nutrientes, o Physarum mostra uma contínua reestruturação da sua rede como resposta a estímulos locais, otimizando assim as ligações entre fontes de alimento. Este comportamento de alto nível já foi explorado para resolver vários problemas de otimização. Este trabalho foca-se na construção de um modelo para a formação da rede adaptativa do Physarum que resolve algumas inconsistências de modelos anteriores. Começamos por derivar uma classe geral de equações que descrevem a adaptação e o ﬂuxo de uma rede estática, composta por canais elásticos que transportam um ﬂuido incompressível submetido a um escoamento de Hagen-Poiseuille. Uma parametrização especíﬁca do modelo é obtida através da minimização da potência dissipada pela rede. Considerando uma parametrização mais genérica, descobrimos uma transição de fase no sistema. O modelo é aplicado à resolução de labirintos, e na construção de redes eﬁcientes e robustas numa geometriaquerepresentaPortugalcontinental. Comparandoasredesresultantescomosistemaferroviário português, veriﬁcou-se que as redes produzidas pelo modelo são em geral mais eﬁcientes quando são consideradas ﬂutuações nos ﬂuxos dos canais. Finalmente, o modelo de adaptação é estendido para incorporar o crescimento da rede na presença de múltiplas fontes de alimento. O acoplamento de ambos os processos produz redes com características semelhantes a vários sistemas de rede encontrados na natureza. Os resultados revelam que, quando as fontes de alimento operam alternadamente, o modelo consegue replicar as conexões diretas entre fontes de alimento observadas noPhysarum. Palavras-chave: Physarum polycephalum, Rede adaptativa, Optimisação de redes, Cresci- mento de redes, Hagen-Poiseuille viiviiiAbstract Physarum polycephalumis an acellular slime mould that grows as a highly adaptive network of veins ﬁlled with protoplasm. As it forages,Physarum dynamically rearranges its network structure as a response to local stimuli information, optimising the connection between food sources. This high-level behaviour was already exploited to solve numerous optimisation problems. We develop a ﬂow-based model for the adaptive network formation ofPhysarum, which solves some inconsistencies of previous models. We ﬁrst derive a general class of equations describing the adaptation and ﬂow dynamics of a static network comprised of elastic channels ﬁlled with an incompressible ﬂuid undergoing a Hagen-Poiseuille ﬂow. An explicit form of the model is obtained by minimising the total power dissipated by the network. Considering a more general functional form of the adaptive equations, a phase transition in the system is also found. The model is used for maze-solving and to build eﬃcient and resilient networks in an arena mimicking mainland Portugal. By comparing the resulting networks with the real Portuguese railway system, we found that the model produced networks with a better overall performance when considering ﬂuctuations in the network ﬂows. Finally, the adaption model is extended to incorporate the network growth in the presence of multiple food sources. The coupling of both processes produces networks with similar traits to several network systems found in nature. We found that when the food sources operate alternately, the model can replicate the direct connec- tions between the food sources observed inPhysarum. Keywords: Physarum polycephalum, Adaptive network, Network optimisation, Network growth, Hagen-Poiseuille ﬂow ixxContents Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v Resumo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv List of Abbreviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxiii 1 Introduction 1 1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2 Phenomenology of Physarum polycephalum 5 2.1 Life-cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 Network Flows Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2.1 Shuttle streaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2.2 Physarum Oscillator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3 Physarum’s Intelligent Behaviours . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3.1 Maze-Solving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3.2 Network Optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.4 Modelling Physarum’s Network Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.4.1 The Multi-Agent System Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.4.2 Physarum Solver . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3 Physarum’s Network Adaptation Model 15 3.1 Hagen-Poiseuille Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.1.1 Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.2 Model Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.3 Model Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.3.1 Physarum as a Flow Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.3.2 Adaptation Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.4 Minimisation of Energy Dissipation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 xi3.5 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 3.5.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 3.5.2 Computation of the network ﬂows . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 3.5.3 Numerical Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.5.4 Mesh Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.6 Exploration of the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.6.1 Network temporal evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.6.2 Choice of sources and sinks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3.6.3 Initial Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3.6.4 Phase Transition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 4 Applications 43 4.1 Maze Solving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 4.2 Finding the Shortest Path . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 4.3 Approximating Portugal’s Rail System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 4.3.1 Dependence of the performance on γ . . . . . . . . . . . . . . . . . . . . . . . . . . 50 4.3.2 Dependence of the performance on the stochastic choice of terminals . . . . . . . . 53 5 Modelling Physarum’s Growth 59 5.1 Growth Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 5.1.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 5.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 5.2.1 One food source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 5.2.2 Multiple food sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 5.2.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 6 Concluding remarks 69 6.1 Achievements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 6.2 Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 Bibliography 73 xiiList of Tables 4.1 Comparison between the initial mesh (mesh), real railway (railway), minimum spanning tree (MST) and complete graph (CG) in terms of the total length (TL), transport eﬃciency (TE) and fault tolerance (FT). The columnsXG denotes the metricX of the graph of each row normalised by the one of the graphG. . . . . . . . . . . . . . . . . . . . . . . . . . . 48 4.2 The mean and corresponding standard error of the performance metrics for diﬀerent meth- ods of choosing the sources and sinks in each step of the algorithm. The methods are described in Figure 4.8. The metrics are normalised to the corresponding values of the CG (Figure 4.5c). The results are based on 10 runs for each method and are sorted by the beneﬁt-cost ratio of the network’s transport eﬃciency, BCRTE. At the bottom is pre- sented the same metrics of the MST, railway graph and CG for comparison. The stochastic methods produce networks with much better performance than the case of ﬁxed terminals. Overall, the stochastic networks also have a better beneﬁt-cost relationship than the MST, CG and the real railway. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 xiiixivList of Figures 2.1 The life-cycle of Physarum polycephalum. The life-cycle begins with a haploid phase, followed by a diploid phase where Physarum reaches its main stage, the plasmodium. Source: [13]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 Physarum polycephalum’s plasmodium phase.(a) Physarum growing radially from a cen- tral food source. As it grows, a highly ramiﬁed network is formed, which is subject to continuous optimisation. Some veins are reinforced while others shrink, resulting in a hi- erarchical and reticulated structure of veins.(b) Close look atPhysarum’s growth fronts with a fan-like shape. Images adapted from [14]. . . . . . . . . . . . . . . . . . . . . . . . 6 2.3 Physarum polycephalummaze solving experiment. (a) The slime mould (yellow) was let cover all the maze. Brown blocks correspond to the maze walls and blue lines indicate the segments of the possible solutions of the maze,α1 (41 ±1 mm), α2 (33 ±1 mm), β1 (44 ±1 mm), β2 (45 ±1 mm). (b) After placing the agar blocks (AG), it explored all possible routes and shrank the vessels which led to dead ends.(c) Four hours later, only the shortest path remained. Source: [5]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.4 Physarum polycephalum Tokyo Experiment. (a) Initially a plasmodium sample (yellow) was placed at Tokyo location in an area surrounded by Paciﬁc coastline (white border) and ﬁlled with other 35 food sources representing the neighbour major cities (white dots). (b to f) The plasmodium progressively colonised each food source, and simultaneously optimised the connections between them. Adapted from [6]. . . . . . . . . . . . . . . . . . 9 2.5 Comparison of the Physarum networks with the Tokyo rail network.(a)Resultant network without geographical constraints. (b) In another experiment, an illumination mask was applied to simulate the geographical constraints of the Japan rail network.(c and d)The resultant network (c) was compared with the real rail network (d). Adapted from [6]. . . . 10 2.6 Agent morphology. An agent consists of three sensors and a body. Source: [11]. . . . . . . 11 2.7 Formation and self-organisation of the multi-agent networks in the absence of external positive stimuli. Simulation on a200 ×200 lattice, with15% of the area ﬁlled with agents initially placed at random locations. Simulation parameters:RA = 45◦, SA = 22.5◦, SO = 9. Adapted from [11]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 xv2.8 Formation and self-organisation of the multi-agent networks under the inﬂuence of external positive stimuli (black dots). Simulation on a200 ×200 lattice, with2% of the area ﬁlled with agents initially placed at random locations. Simulated withRA = 45◦, SA = 22.5◦, SO = 9. In the case of two stimuli sources (top row), the optimisation process converges to the shortest path connecting them. In the presence of multiple stimuli, the ﬁnal network closely approximates the Steiner minimum tree solution. Adapted from [11]. . . . . . . . . 12 2.9 Simulation of the Physarum Solver (γ = 1.8, I0 = 2.). Food sources are represented by blue dots. (a) Initially the space was populated with a ﬁnely meshed network of thin tubes. (b,c) Over time, many of these tubes died out, and the other few were selectively thickened resulting in a stable optimised network(d). Adapted from [6]. . . . . . . . . . . 14 3.1 Simulations of the Physarum Solver for diﬀerent choices of the adaptation function f in (3.10), diﬀerent distribution of sources and sinks, consideringµ = 1 and a total ﬂux ﬂowing through the networkI0 = 1. The simulations were carried out in planar graphs with 5×5 nodes, considering initial homogeneous conductivities,Dij(0) = 1. In each case, the left image depicts the initial network geometry, the middle image corresponds to the steady state of the adaptation mechanism, and the right plot represents the volume of the network over time,V, normalised to the initial volume,V0. The thickness of the black lines is proportional to the radius of the edges(∼D1/4 ij ). The plots show that the volume is not conserved throughout any simulation, which implies that the Hagen-Poiseuille ﬂow can no longer be applied as the ﬂuid is compressible. . . . . . . . . . . . . . . . . . . . . . 19 3.2 Example of the calculation of the Laplacian matrix for a small weighted graph. On the left, the circles represent the nodes, labelled by their index, and lines represent the edges labelled by the corresponding weight. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 3.3 Simulation examples of the model (3.34), depicting the network optimisation process at diﬀerent time steps, for two diﬀerent choices of terminals. The simulations start from an initial Delaunay triangulation of 900 nodes, shown in red, where the edges start with homogeneous conductivities,Dij(0) = 1. The thickness of the black lines is proportional to the radius of the channels (∼D1/4 ij ). At the bottom right of each group, it’s plotted the networks’ volume throughout the simulation, which shows that the volume is conserved in both cases. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 3.4 Simulation of the model (3.34) replicating the Physarum’s network adaptation, starting from initial homogeneous conductivities,Dij(0) = 1. The labels t designate the iteration number in which the snapshots were taken. The ﬂux is driven by a central source (yellow) and the sinks (red) placed at the boundary of the organism, which represent stimulated regions with high metabolic activity. The source givesqsource = 1, which is evenly dis- tributed between the sinks. The adaptation dynamics results in a tree-like steady-state network (cf. Figure 2.2). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 xvi3.5 Dependency of the optimal network geometry on the choice of sources and sinks. (a) Several simulations of the adaptation dynamics (3.34) were carried on an initial network (red mesh) withDij(0) = 1, considering a ﬁxed Y-shaped arrangement of 4 terminals (blue circles). (b-o) steady-state networks reached considering all the diﬀerent combinations of sources (yellow circles) and sinks (red triangles) states of the arrangement of terminals (a). The thickness of the black lines is proportional to the radius of the channels. In each case, the distribution of node ﬂuxes follows (3.42) (I0 = 1 ). The geometry of the steady-state network varies greatly with the choice of sources and sinks. By interchanging the sources with the sinks, the same steady state is obtained. Some conﬁgurations can lead to apparently disconnected solutions(j-o). . . . . . . . . . . . . . . . . . . . . . . . 33 3.6 Dependency of the steady states of the dynamics (3.34) on a global scaling of the nodes’ net ﬂuxes, considering the same conﬁguration of terminals (3 sources and 4 sinks), initial conditions (Dij(0) = 1 ) and initial mesh. The images show the steady states obtained considering the nodes ﬂux distribution (3.42) for diﬀerent choices of the parameterI0. The dynamics converged to the same steady-state conductivities regardless of the value of I0, which implies that it’s invariant for scaling transformation ofq. . . . . . . . . . . . . . 34 3.7 (a-b)Examples of (apparently) disconnected steady-state networks of the dynamics (3.34) for two diﬀerent choices of the nodes’ net ﬂux distribution. The labels near each terminal represent their net ﬂux, while the remaining nodes haveq = 0. (c-d) Examples of con- nected steady states for the same conﬁgurations of(a) and (b), although disconnected solutions would be possible in principle. Note that the nodes ﬂuxes are identical in cases (a) and (c), apart from a permutation of the sources, and the same goes for the cases(b) and (d), apart from a permutation of the sinks. All the simulations were done considering initial conductivities Dij(0) = 1 . The thickness of the black lines is proportional to the radius of the channels. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3.8 Dependency of the geometry of the optimal network on the initial conductivities of the channels. Each group of simulations,(a) and (b), contains examples of the steady-state networks of (3.34) for a given conﬁguration of terminals and same initial mesh (red lines). Each steady state was obtained by considering a diﬀerent set of initial random conductiv- ities uniformly distributed in the interval[0,2]. In both cases, the geometry of the ﬁnal networks is highly sensitive to the initial conditions, which implies that the system may have multiple steady states. The thickness of the black lines is proportional to the radius of the channels. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 xvii3.9 Dependency of the optimal network geometry on the scaling of the initial conductivities. The images correspond to the steady states of (3.34) obtained for a diﬀerent set of homo- geneous initial conductivities,Dij(0) = D0, withD0 ∈{0.1,1,5,15}, considering the same conﬁgurations of Figure 3.8. A scaling of the initial conductivities is reﬂected in a scaling of the steady-state conductivities by the same factor. In each group of simulations, only the thickness of the channels (∼D1/4 0 ) is increased asD0 increases, but the geometry of the network is left unchanged. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 3.10 Dependency of the steady-state networks of the dynamics (3.45) onγ, for the same con- ﬁguration of Figure 3.4. Note the transition in the network topology fromγ = 0 .47 to γ = 0.51. Forγ <1/2 the networks have a redundant structure with loops and no hierar- chical organisation, while forγ >1/2 the networks have a tree-like structure with a clear hierarchy of tubes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 3.11 Histogram of frequencies of the steady-state conductivities, for diﬀerent values ofγ, consid- ering the same settings as in Figure 3.10. The distributions are diﬀerent from each other. For γ = 0 .1, the distribution is very similar to the initial one. In the remaining cases, most of the conductivities are concentrated near zero, as most of the edges disappear. For clarity, the histograms focus on lower probabilities. . . . . . . . . . . . . . . . . . . . . . 41 3.12 Plots of diﬀerent metrics as functions of gammaγ, evaluated at the steady state of the simulations presented in Figure 3.10. The vertical dashed line marks the transition at γ = 1/2. Note the discontinuity of the slopes of the network’s dissipation, total length and loop density. The correspondent graphs for other choices of initial conductivities and distribution of terminals showed the same tendency, thus supporting the universality of the phase transition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 4.1 Simulation of the model (3.34) in a maze similar to that of Figure 2.3, starting from diﬀerent sets of conductivities. The snapshots were taken at diﬀerent iterationst of the algorithm. (a) All possible solution paths are selected since they have the same total length and the initial conditions are the same.(b) First the dead ends disappear like in (a), but due to the initial uneven distribution of conductivities, only the path which is initially favoured remains. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 4.2 Maze solving considering non-homogeneous initial conductivities, Dij(0) ∼U(1/2,3/2). The images represent the ﬁnal networks for diﬀerent initial random states. The system converges to the initially favoured path, i.e., the path which has the highest average con- ductivity. All four possible paths can be reached by changing the random seed. . . . . . 44 xviii4.3 (a) Probability of the system converging to the shortest path that connects two terminals as a function of the parameterγ, considering the dynamics (3.45).(b)Average percentage error of the steady-state total path length,L, relatively to the length of the shortest path solution, LSP, as a function ofγ, i.e., εSP = (L−LSP)/LSP. The computation of the average error only includes the ﬁnal states diﬀerent from the shortest path. The blue shaded region corresponds to the95% conﬁdence interval of the mean error. The results are based on 150 realisations for eachγ value, each realisation corresponding to a diﬀerent initial mesh with 300 nodes. The source and the sink were always placed in diagonally opposite corners of the square. In all the cases, the system starts from homogeneous distribution of conductivities,Dij(0) = 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 4.4 Approximation of the part of the real Portuguese rail network used in this study, which connects all the 18 district capitals (green nodes) and 5 additional terminal cities (red nodes). Note that Mangualde was used to represent the capital district Viseu. Some rail lines are no longer active but were included for consistency. . . . . . . . . . . . . . . . . . 47 4.5 (a)Graph of Portugal’s rail system (Figure 4.4) with all the city nodes (terminals) marked in blue. (b) The minimum spanning tree (MST) connecting the same set of cities, i.e., the tree graph spanned by the city nodes with the minimal possible total length (4.2).(c) The complete graph (CG) connecting every pair of cities by a distinct edge. The legend of each graph refers to the metrics(TL, MD, FT), where TL and MD are given in kilometres. 49 4.6 Topology of the networks resulting from the adaptation dynamics (3.45) as a function of the parameter γ, considering a stochastic choice of the source-sink pair from the set of terminals in each step of the algorithm (I0 = 1 and Dij(0) = 1 ). For γ <1/2 the dynamics result in poorly-optimised networks very close to the initial mesh, similar to the case of ﬁxed terminals. Asγ is increased, the networks slowly evolve towards the minimum spanning tree (Figure 4.5b), losing all the redundant paths which provide robustness to the network. The legend of each image refers to the network metrics(γ, TL, MD, FT), where TL and MD are given in kilometres. The top 3 networks with the best overall performance are highlighted in green. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 xix4.7 Network performance of the adaptation dynamics (3.45) as a function of the parameterγ, with the other parameters ﬁxed, including the random seed.(a-b) Transport eﬃciency (4.3) and fault tolerance (4.4) plotted against the total length of the network (4.2) (cost). The metrics are normalised to those of the complete graph (CG) connecting the city nodes, yieldingTLCG, TECG, FTCG. Thecolouredcirclesrepresentthesimulationresultsas γwas varied from 0.55 to 2.00, considering the stochastic choice of the source-sink pair withI0 = 1, and initial conditionsDij(0) = 1. The results were compared with the same normalised metrics of the real railway (green triangles) and MST network (red squares).(c) Plots of the beneﬁt-cost ratios, deﬁned as BCRTE= TECG/TLCG and BCRFT= FTCG/TLCG, as the function ofγ, compared with the ones of the real railway, MST and CG. The proposed optimal models (i.e., which result in networks with the best performance trade-oﬀ) are highlighted in green. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 4.8 Topology of the optimised networks considering diﬀerent methods of choosing the sources and sinks in each step. Note that the examples given are not fully representative, since the topology may change signiﬁcantly due the stochastic nature of the algorithm.(a) “Random pair” -Model (3.45) withγ = 2/3, considering the choice of random source- sink pair from the set of terminals in each step.(b) “Random source” -Model (3.45) with γ = 2 /3, where in each step a random terminal is chosen as the source, and the remaining terminals are sinks receiving the same amount of ﬂux.(c) “All random” - Model (3.45) with γ = 2 /3, where in each step all the terminals are randomly chosen either as sources or sinks. The terminals net ﬂux are random variables subject to the constraint (4.5). (d) “Fixed Terminals” -Model (3.45) withγ = 2/3, considering ﬁxed sources and sinks from the beginning. In this case, the capital Lisbon acts like a source (blue circle) of ﬂux, while all the remaining cities are sinks (blue triangles). (e) “PS - random pair” -Physarum Solver model (3.10) with the choice of a sigmoid update function f(|Qij|) = |Qij|1.8/(|Qij|1.8 + 1) typically used in literature andµ = 1 . The choice of terminals in each step is the same as “Random pair” method. Note how much thinner are the channels of the ﬁnal network comparing to the remaining cases, due to the total volume not being conserved in this case. In all the cases the total inlet ﬂux isI0 = 2, and the same initial conditions were used,Dij(0) = 1. The legend of each image refers to the network metrics(TL, MD, FT), where TL and MD are given in kilometres. . . . . 55 4.9 Typical networks obtained when considering a static set of sources and sinks, forγ = 2/3. Diﬀerent choices of sources and sinks lead to completely diﬀerent networks, which are always trees. Clearly, these topologies are far from being optimal due to excessive cost of connecting all the terminals and the zero fault tolerance, resulting in low transport eﬃciency. Only the adaptation subject to time-dependent sources and sinks can result in eﬃcient and resilient networks. The legend of each image refers to the network metrics (TL, MD, FT), where TL and MD are given in kilometres. . . . . . . . . . . . . . . . . 56 xx4.10 Performance of the networks for diﬀerent methods of choosing the sources and sinks over time, as described in Figure 4.8. (a-c) Plots showing the trade-oﬀ between the cost (total length), transport eﬃciency and fault tolerance of the networks. The metrics are normalised to those of the CG graph. Each type of marker corresponds to a diﬀerent method. The results were compared with the same normalised metrics of the real railway (green triangles) and MST network (red squares)(d) The beneﬁt-cost ratios, deﬁned as BCRTE= TECG/TLCG and BCRFT= FTCG/TLCG, plotted against each other, measuring the overall compromise between the three metrics for the diﬀerent methods of choosing the terminals. The simulations which achieved the best trade-oﬀ between each pair of metrics are highlighted in green. Overall, the stochastic networks are more robust and eﬃcient than any other network, especially comparing to the case of ﬁxed terminals. . . . . . . . 57 5.1 Physarum growing from a food source considering the adaptation dynamics (3.45) with γ = 2/3. The growth is driven by a central food source (yellow) which pumps nutrients to the moving boundary (red) where there is a constant uptake of nutrients to create new channels (with an initial conductivityD0). In each time step, an amount ofI0∆τ nutrients given by the source is distributed evenly between the boundary sinks. AsPhysarum grows, the channels adapt their thickness according to the ﬂux of nutrients ﬂowing through. Simulation carried out withD0 = 0 .1, ∆τ = 0 .02, I0 = 0 .1. The dynamics result in a tree-like network that resembles the networks produced by the real organism (Figure 2.2), although no stable cross-links between the main veins are formed. The labelst designate the time step in which the snapshots were taken. . . . . . . . . . . . . . . . . . . . . . . . 62 5.2 Physarum growing from a food source considering the adaptation dynamics (3.45) with γ = 0 .45. Same settings as in Figure 5.1. For γ <1/2 the dynamics result in highly redundant networks very close to the underlying mesh. Despite the redundancy, due to the lack of optimisation and a clear hierarchy of vein thickness, the networks don’t resemble the ones produced by the real organisms. The labelst designate the time step in which the snapshots were taken. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 5.3 Physarum growing from a food source considering the adaptation dynamics (3.45) with γ = 2 /3 and the sinks receiving a random amount of ﬂux over time. Same settings as in Figure 5.1, except that in each time step, the amount of nutrients given by the source is distributed randomly between the boundary sinks. The network evolution is nearly identical to the case of the nodes receiving uniformly, and therefore a random distribution of nutrients can’t explain the formation of stable loops observed inPhysarum networks. . 64 xxi5.4 Physarum growing in the presence of two food sources with constant input ﬂux. Same parameters as in Figure 5.1. Starting from the left food source,Physarum accommodates a second food source (blue circle) as it forages. From that moment, both sources are always active, each injecting a constantI0∆τ amount of nutrients per time step, which is evenly distributed between the boundary sinks (red circles). Yellow circles represent active food sources in a given moment. The synchronous activation of both food sources leads to their “repulsion”, meaning that no direct connection is formed between them. This is unrealistic in the context ofPhysarum. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 5.5 Real Physarum growinginapresenceoftwofoodsources(agarblocks). Adirectconnection is established between them. Adapted from [42]. . . . . . . . . . . . . . . . . . . . . . . . 66 5.6 Physarum growing in the presence of two food sources supplying nutrients alternately. Same settings as in Figure 5.4, but once the second food source is covered, at a given time step only one of them is randomly selected to supply the nutrients to the boundary sinks (red circles). The yellow circles represent the activated food source in a given instant, while the blue circles represent the inactive one. This asynchronous operation of the food sources allows the formation of a short direct connection between them, similarly to what is observed inPhysarum (Figure 5.5). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 xxiiNomenclature Acronyms MST Minimum Spanning Tree CG Complete Graph TL Total Length TE Transport Eﬃciency FT Fault Tolerance Variables Lij Length of the edge(i,j) Dij Conductivity of the edge(i,j) Qij Volumetric ﬂux ﬂowing through the edge(i,j) Vij Volume of the edge(i,j) V Network’s total volume Pij Power dissipated by the channel(i,j) P Total power dissipated by the network pi Pressure of the nodei qi Net volumetric ﬂux of the nodei I0 Total inlet ﬂux given by the source nodes E Set of edges of a graph V Set of vertices (nodes) of a graph η Fluid’s dynamic viscosity β √8πη ∆τ Time step of the network optimisation algorithm xxiiixxivChapter 1 Introduction 1.1 Motivation Transport networks appear in a variety of forms in nature, from river networks to the organ systems of multicellular organisms. Examples of the latter include the leaf venation in plants, composed by the xylem and phloem; the vascular system in animals made up of arteries, veins and capillaries; the mycelial cords of fungi, and the plasmodial veins of slime moulds. In all these cases, the networks play a key role in distributing resources and information throughout the entire organism in a rapid and eﬃcient manner, overcoming the size limitations of purely diﬀusive transport. These ﬂow systems are thus indispensable for the organisms’ development, ﬁtness and survival. These ﬂow networks are composed of tubular vessels with diﬀerent lengths and thicknesses which are typically organised in a hierarchical tree fashion. In general, they contain redundant connections, forming loopy structures that make them more robust and tolerant to damage and also improve transport eﬃciency under ﬂuctuating loads [1, 2]. The architecture of the networks depends on its functionality and generally has a decentralised nature, in the sense that it emerges from local responses to environmental stimuli. The vessel dimensions and hierarchical organisation have a profound impact on the transport eﬃciency of the nutrients and other resources by aﬀecting the local ﬂuid ﬂow. Transport networks are also present in diﬀerent aspects of human life, from road, railway and com- munication networks, to irrigation systems and power grids, which are crucial for industrial development and to satisfy human needs. Recently, attention has focused on the acellular slime mould,Physarum polycephalum, as an ideal model organism to study the interplay between structure and function in biological transport networks, and to understand the mechanism underlying several complex behaviours displayed by simple organisms, such as the adaptive network formation. Physarum is a single-celled amoeboid organism that grows as an extensive and highly adaptive network of protoplasmic veins. As it forages and progressively accommodates new food sources, Physarum dynamically optimises the connections between them, by adapting the thickness of the network veins. The adaptation is believed to be related to local changes in the ﬂux ﬂow, driven by rhythmical contractions of the veins walls whose frequency and amplitude are 1regulated by food sources and other external stimuli [3]. However, the main biochemical and physical oscillator underlying the rhythmic behaviour and mobility ofPhysarum has not been yet identiﬁed [4]. Despite lacking any kind of neural circuit,Physarum displays high-level behaviours, arising from the network adaptation. For instance, it can solve mazes [5], and in the presence of multiple food sources, it can build networks with a trade-oﬀ between total length, transport eﬃciency and fault tolerance, comparable with real human-made networks [6]. In this regard, diﬀerent toy models have been proposed to describe the network optimisation, based on the current-reinforcement principle [6, 7], where the ﬂow modiﬁes the network architecture, which in turn aﬀects the ﬂows. But in general, they reveal to be inconsistent with their own assumptions and don’t incorporate the network formation. Understanding the basic rules underlying its complex behaviours is of interdisciplinary interest. Not only as a guide to design eﬃcient decentralised networks in diﬀerent domains, but the slime mould computational abilities have been already proved useful to solve more complex network optimisation problems [8, 9], design bioelectronic circuits and unconventional computing devices[10]. Furthermore, network formation and amoeboid locomotion are intrinsic features to processes of wound healing and metastasis formation. The study of these mechanisms inPhysarummay thus reveal possible insights into cancer research. Until now, there isn’t a single model which can capture all the features ofPhysarum’s network self-organisation, as it’s a complex task and the mechanism underlying its behaviour is not clear yet. 1.2 Objectives The goal of this thesis is to build a generic model for the network formation and optimisation as observed during the growth of Physarum polycephalum. The current state-of-the-art models lack many of the Physarum’s biological features, such as growth, or violate some basic physical principles [6]. Some modelling approaches are even purely phenomenological [11], lacking, therefore, any biological insights. We aim to construct a more realistic ﬂow-based model which addresses these issues. Physarum will be modelled as a ﬂow network composed of adaptive channels ﬁlled with an incom- pressible and viscous ﬂuid (protoplasm) and whose diameters change in response to the ﬂux ﬂowing through them. The ﬂow is assumed to have a Hagen-Poiseuille proﬁle and is driven by a set of ﬂux sources and sinks which mimic stimulated regions of the organism. The model is inspired by a previous ﬂow-based model, but takes the conservation of volume of the circulating ﬂuid into account, as required by a Hagen-Poiseuille ﬂow. Previous models [6] describe the channels’ adaptation throughad hoclocal evolution laws which violate the above physical assumptions. We will begin by modelling the adaptation of static organisms, where the network growth is at ﬁrst neglected. Afterwards, weexploresomeofthemodel’smainfeaturesandapplications. Finally, weattempt to mimic thePhysarum’s dynamic adaptive network formation, by incorporating a growth mechanism into the adaptation model explicitly dependent on nutrients supplied by food sources. 21.3 Thesis Outline This thesis is organised into six chapters. In chapter 2 we begin with a biological description ofPhysarum and discuss its main high-level behaviours. The two seminal models proposed to describe the adaptive network formation are also explained in detail. In chapter 3, we formulate the new model for the network optimisation, founded on physical principles of a Hagen-Poiseuille ﬂow. As a ﬁrst insight, we start by performing simple tests on the model, considering a ﬁxed set of terminals and present some of its main features. In particular, we study the adaptation dynamics for a particular functional form of the model on a conﬁguration of terminals mimicking the case ofPhysarum. In chapter 4, we explore some applications of the model to path-ﬁnding and network design. We ﬁrst test its ability to solve mazes and to ﬁnd the shortest path between two nodes in an arbitrary graph. Then, on an arena with the shape of mainland Portugal, we study the importance of ﬂux ﬂuctuations to build eﬃcient and resilient networks similar to those ofPhysarum by considering time-dependent distributions of sources and sinks. The terminals represent the geographic location of major Portuguese cities. The performance of the resultant networks is compared with that of the real railway system connecting those cities. In chapter 5, we extend the adaptation model to include the network formation, giving a better representation ofPhysarum’s foraging behaviour. In particular, we simulate the growth and optimisation in the presence of one or more food sources. Finally, in chapter 6, we give a general overview of the results and discuss some of the main conclusions made and possible improvements to the model. 34Chapter 2 Phenomenology ofPhysarum polycephalum 2.1 Life-cycle Physarum polycephalumis a macroscopic slime mould of the familyPhysaraceae, orderPhysarales, class Myxomycetes. Despite their name, slime moulds are not fungi, but rather amoeboid protists (phylum Amoebozoa, infraphylumMycetozoa), sharing, therefore, common traits with plants, fungi and animals. These organisms exhibit a complex life cycle (Figure 2.1), which provide them with great adaptability to environmental changes [10, 12]. Figure 2.1: The life-cycle ofPhysarum polycephalum. The life-cycle begins with a haploid phase, followed by a diploid phase wherePhysarum reaches its main stage, the plasmodium. Source: [13]. 5In the main vegetative phase of their life cycle (plasmodium), the slime moulds of classMyxomycetes, commonly known as the true or plasmodial slime moulds, exist as a syncytium, i.e., a giant cell enclosed by a single membrane which contains typically millions of diploid nuclei. ThePhysarum plasmodium consists of an amorphous yellow mass endowed with an amoeba-like behaviour. While foraging, it spreads as a network of vein-like protoplasmic tubes in the direction of the food source, being able to move at speeds higher than 1 cm/h [10]. The food source is covered by extensions of its protoplasm, and it’s digested with the help of enzymes. Typical foodstuﬀs include bacteria, fungal spores and decaying matter. The spreading fronts adopt a fan-like shape, and the number of fronts increases with the nutrient level of the environment, providing it with a larger and more eﬃcient area of absorption. (a)  (b) Figure 2.2: Physarum polycephalum’s plasmodium phase.(a) Physarum growing radially from a central food source. As it grows, a highly ramiﬁed network is formed, which is subject to continuous optimisation. Some veins are reinforced while others shrink, resulting in a hierarchical and reticulated structure of veins. (b) Close look atPhysarum’s growth fronts with a fan-like shape. Images adapted from [14]. Under extreme adverse environmental conditions, the plasmodium changes into an inactive dormant state, forming a hard compact mass know as sclerotium. This hibernation state can be sustained for several years, and when moist, the plasmodium state is gradually recovered [10]. Like all true slime moulds,Physarum reproduces by sporulation. Certain stimuli like starvation and light irradiation trigger the plasmodium to grow sporangia: clusters of black globulose enclosures that produce and protect the spores (haploid cells), that are then released. When the conditions become favourable, a spore hatches into an amoeba-like single-cell (myxamoeba), or, if in the presence of water, into a ﬂagellated version of the former (swarm cell), being these two forms exchangeable [10]. Each of these cells can mate with another through the fusion of their protoplasm, forming a diploid zygote, which after successive nuclear divisions develops into a new plasmodium. 62.2 Network Flows Dynamics The most commonly observed form ofPhysarum is the plasmodium. The network tubes are made of a gel-like outer layer (ectoplasm) that contains the cytoskeleton and encloses the endoplasmic ﬂuid. The cytoskeleton consists of a system of proteins, essentially composed of actin and myosin. The ﬁlaments of actin provide the structural support of the tube walls, and, together with the myosin, the motor protein, are responsible for the unique mobility and growth ofPhysarum [10, 12]. 2.2.1 Shuttle streaming The interactions between actin and myosin generate relaxation-contraction cycles of the walls, which result in a rhythmic back-and-forth propulsion of the endoplasm over the entire network within a period of around two minutes [3, 4]. This type of periodic back-and-forth streaming, known as shuttle streaming, enables the transport and distribution of food supplies, organelles and other substances throughout the organism. Furthermore, these oscillations are locally well-coordinated in such a manner that allows the net movement of the slime mould as it forages. Collectively, they establish a gradient of pressure so that the ﬂow is driven towards the leading edges (anterior margin), where the growth of structural proteins occurs simultaneously. The amplitude and frequency of contractions are regulated according to external stimuli: attractants (e.g., food source) increase them, swelling the stimulated edges which thrive the network, while repellents (e.g., light exposure) decrease both, resulting in the shrinkage of the aﬀected tubes to avoid those negative stimuli [15]. In this way,Physarum can dynamically rearrange the structure of its network and optimise it, in response to local stimuli information. The timescale of morphological rearrangements (∼1h) is much larger than the timescale of ﬂow generation (∼2min) [3]. 2.2.2 Physarum Oscillator It’s clear that the rhythmic contractions and force generation are produced by the interactions between the actin and myosin. However, the underlying mechanism which regulates these interactions isn’t well understood. Synchronous oscillations of the membrane potential, intracellular Ca2+ and other chemicals, like ATP, NADH, H+, are observed in real organisms along with the contraction-relaxation cycles. But the set of essential and independent variables responsible for generating the rhythm and the collective movement ofPhysarum hasn’t been identiﬁed [4]. What is certain is that Ca2+ plays a prominent role in the contraction-relaxation cycle, like in smooth muscles contraction. The plasmodium contains vesicles capable of sequestering and releasing calcium through stretch-activated channels, and data conﬁrms that a rise in Ca2+ concentration can trigger the contractions [4, 12, 16]. 2.3 Physarum’s Intelligent Behaviours Despite lacking any kind of neural circuit, the morphological adaptation displayed byPhysarum provides it with high-level behaviours. For instance, the slime mould can ﬁnd the shortest path between two food sourcesinamaze[5], andtoreplicateoptimised, man-madetransportnetworks[6]. Incontrasttoanimals, 7where complex behaviours can be assigned to their evolved nervous system, this type of intelligence displayed by many brainless organisms, like other species of slime moulds and even fungi, is still poorly understood. In particular, in the case ofPhysarum, the means of communication through its entire body, and how it processes that information to coordinate its movement and growth remain unknown. Recent studies [17] suggest that the transport of signalling molecules is involved in the coordination of the ﬂuxes. The control of the internal ﬂuid ﬂow is likely crucial to the coordination of its behaviour, including the continuous network self-organisation [3]. 2.3.1 Maze-Solving Nakagaki et al. [5] were the ﬁrst to report the maze-solving skills ofPhysarum, showing its ability to ﬁnd the shortest path between two food sources. The maze (Figure 2.3) consisted of an agar substrate with plastic ﬁlms as walls, which are dry surfaces that the slime mould tends to avoid. (a)  (b)  (c) Figure 2.3: Physarum polycephalum maze solving experiment. (a) The slime mould (yellow) was let cover all the maze. Brown blocks correspond to the maze walls and blue lines indicate the segments of the possible solutions of the maze,α1 (41 ±1 mm), α2 (33 ±1 mm), β1 (44 ±1 mm), β2 (45 ±1 mm). (b) After placing the agar blocks (AG), it explored all possible routes and shrank the vessels which led to dead ends.(c) Four hours later, only the shortest path remained. Source: [5]. Initially, a sample of plasmodium was allowed to ﬁll the entire maze (Figure 2.3a), and only then, two nutrient-rich agar blocks were placed on the endpoints (Figure 2.3b). There were four possible routes connecting the two,α1 −β1, α1 −β2, α2 −β1, α2 −β2, beingα2 (β1) about22% (2%) shorter thanα1 (β2). The slime mould quickly restructured its network, removing the redundant vessels and the ones branching along the dead ends, and reinforcing the optimal ones, until only one path connecting the food sources remained. The path that survived was diﬀerent between experiments, but the shortest segment α2 was always selected. The segmentsβ1 and β2 were chosen about the same number of times, due to their negligible diﬀerence in length, which is lost by the natural undulations of the tubular trajectory. This means that in all the experiments, the path selected was always approximately the shortest one. 2.3.2 Network Optimisation In later experiments, Tero et al. [6] studied the robustness of the slime mould network adaptability when it distributes itself over several food sources. From an evolutionary perspective, this appears as a 8competitive advantage for organisms that forage as large contiguous networks, since, to optimise their strategy, they must be able to balance the network eﬃciency with the cost of producing it. (a)  (b)  (c) (d)  (e)  (f) Figure 2.4: Physarum polycephalum Tokyo Experiment. (a) Initially a plasmodium sample (yellow) was placed at Tokyo location in an area surrounded by Paciﬁc coastline (white border) and ﬁlled with other 35 food sources representing the neighbour major cities (white dots).(b to f)The plasmodium progressively colonised each food source, and simultaneously optimised the connections between them. Adapted from [6]. Thirty-six food sources were arranged in a substrate, portraying the geographic locations of the surrounding cities of Tokyo (Figure 2.4). The plasmodium grew out from Tokyo’s food source and explored gradually the surroundings with a contiguous front until it accommodated all food sources, and simultaneously optimised its network in a way that only the more eﬃcient food source connections survived. In a more realistic setting, the geographical constraints, namely, high-altitude areas, lakes and the Paciﬁc Ocean, were replicated by increasing the luminosity of those regions, restricting the growth of the plasmodium to shaded areas that it tends to prefer. The resultant minimal networks achieved by Physarum were compared to the real rail network of Tokyo in terms of cost-eﬃciency and fault tolerance. Here an optimal cost-eﬃciency means a low total length of vessels with a short average minimum distance between the food sources, whereas fault tolerance is deﬁned as the probability of disconnecting part of the network when a single link is removed, evaluating, therefore, its robustness. As shown in Figure 2.5, the shape of the optimised network displayed by thePhysarum is quite similar 9(a)  (b) (c) (d) Figure 2.5: Comparison of thePhysarum networks with the Tokyo rail network.(a) Resultant network without geographical constraints.(b) In another experiment, an illumination mask was applied to sim- ulate the geographical constraints of the Japan rail network.(c and d)The resultant network (c) was compared with the real rail network (d). Adapted from [6]. to the actual man-made railway system. Surprisingly, the authors concluded thatPhysarum networks showed a slightly better cost-eﬃciency, yet marginally lower robustness. This is an astonishing achieve- ment given thatPhysarum builds the networks without centralised control, in contrast to human-made infrastructure networks. Since then, other real-world transportation networks have also been approxi- mated byPhysarum, such as the highways of the UK, Germany and USA [18, 19]. 2.4 Modelling Physarum’s Network Adaptation Until now, there is no single model which can describe the whole behaviour ofPhysarum, even considering only the plasmodium stage. Due to the complexity of the task, current approaches focus on modelling a speciﬁc issue at a time, namely the growth mechanism, the contraction patterns of the actin-myosin cortex, and the network formation and adaptation. Naturally, these mechanisms are all inter-connected, and together are responsible for the complex behaviours exhibited by Physarum. In particular, the coordination of the ﬂows arising from the synchronisation of the contractions, as a response to external stimuli, seems to be a key feature underlying the network optimisation. This work focuses on modelling the network formation and optimisation. In this respect, diﬀerent modelling techniques were proposed, including cellular automaton models [20], agent-based models [11], and mathematical ﬂow-based models [6]. Each one assumes that the optimisation is achieved through a diﬀerent set of simple rules and, in general, don’t take the coordination of contractions into account. Some are even purely phenomenological models or bio-inspired algorithms designed to solve complex graph problems [8, 9], lacking therefore meaningful biological insight. In the following, we describe the two main models found in the literature. 102.4.1 The Multi-Agent System Model Jones [11] proposed a multi-agent bottom-up approach to model the formation and the optimisation of Physarum transport networks. According to this method, the macroscopic network adaptation arises as an emergent phenomenon from simple microscopic interactions between small portions of the plas- modium. The plasmodium is represented by a hypothetical population of mobile particle-like agents, each occupying a single cell of a two-dimensional lattice. The lattice stores the current agent positions and local concentrations of stimuli which diﬀuse through the environment and may evaporate. Positive stimuli, referred to as chemoattractants, are released by food sources and by moving agents. Hazardous stimuli that agents tend to avoid may be also considered. Each agent uses three forward sensors (FL, F, FR) to sense the local concentration of these stimuli in three regions in front of its position, and re- sponds to this information by moving towards the strongest local source of chemoattractant. The sensed regions are parametrized by the width of the sensors,SW (usually one cell), the distance from the agent, SO (sensor oﬀset), and the angle relative to the direction of the agent,SA (Figure 2.6). By depositing stimuli while they move, the agents not only adapt to the environment but also inﬂuence each other’s behaviours. A minimum SO of 3 cells is required for strong local coupling to occur and for complex patterns to emerge. Increasing theSO value results in thicker networks, faster network adaptation, and coarser-grained networks. Figure 2.6: Agent morphology. An agent con- sists of three sensors and a body. Source: [11]. Algorithm 1Multi-Agent Algorithm 1: procedureMotorStage 2: Attempt move forward in the current direction 3: if (moved successfully)then 4: Deposit trail in new location 5: else 6: Choose random new orientation 7: end procedure 8: procedureSensoryStage 9: Sample trail map values 10: if (F > FL) and (F > FR) then 11: Stay Facing Same Direction 12: else if(F < FL) and (F < FR) then 13: Rotate randomly left or right by RA 14: else if(FL < FR) then 15: Rotate right by RA 16: else if(FR < FL) then 17: Rotate left by RA 18: else 19: Continue facing the same direction 20: end procedure The simulation starts with a chosen number of agents placed at random unoccupied locations and with random orientation (from zero to 360 degrees). At each time step, based on perceived sensory information, an agent rotates itself towards the direction covered by the sensors with the highest chemoattractant concentration. The agent is allowed to move one step forward in that direction, only if the new site is not already occupied, and deposits a constant concentration of chemoattractant, which attracts nearby agents. Otherwise, if the movement isn’t allowed, the agent remains in its current position without leaving any trail, and in the next step, a new orientation is randomly selected. In this way, mobile agents can be interpreted as the endoplasmic ﬂux, while immobile agents represent the actin-myosin cortex. A time step is concluded when every agent is given a change move. The update of the agents is performed randomly at each step to avoid any correlation from sequential ordering. After a long run, a stable collective pattern of 11the positions of agents may arise, forming a continuous network that connects the food sources. Typical simulations of the model in the absence and the presence of external positive stimuli are shown in Figure 2.7 and Figure 2.8, respectively. Figure 2.7: Formation and self-organisation of the multi-agent networks in the absence of external positive stimuli. Simulationona 200×200 lattice, with15% oftheareaﬁlledwithagentsinitiallyplacedatrandom locations. Simulation parameters:RA = 45◦, SA = 22.5◦, SO = 9. Adapted from [11]. However, the model is biologically unrealistic, since the agents act autonomously and not as a con- tinuous network, especially in the ﬁrst stages when they randomly ﬁll the lattice. Even ignoring this issue, the agents are always biased towards gradients of positive stimuli, which doesn’t depict the true foraging behaviour of the slime mould. Nevertheless, the model has been successfully applied to solve several graph problems [9, 10]. Figure 2.8: Formation and self-organisation of the multi-agent networks under the inﬂuence of external positive stimuli (black dots). Simulation on a200 ×200 lattice, with 2% of the area ﬁlled with agents initially placed at random locations. Simulated withRA = 45◦, SA = 22.5◦, SO = 9. In the case of two stimuli sources (top row), the optimisation process converges to the shortest path connecting them. In the presence of multiple stimuli, the ﬁnal network closely approximates the Steiner minimum tree solution. Adapted from [11]. 122.4.2 Physarum Solver Tero et al. [7] proposed a ﬂow-based model known asPhysarum Solver, initially to describe the maze- solving ability of the slime mould [5] (Figure 2.3). According to the model, the path-ﬁnding is achieved through feedback loops between the thickness of the tube and the rate of internal ﬂows: higher ﬂuxes trigger an increase of the tube radius, while lower ﬂuxes lead to their shrinkage. The plasmodium is modelled as a graph, where the edges represent the tubes in which endoplasm ﬂows, and the nodes the connections between them. The nodes i and j are kept at pressures pi and pj, and are linked by a cylindrical tube of constant lengthLij and a variable radiusrij. The problem is reduced to ﬁnding the shortest path between two special nodes,N1 and N2, that represent the food sources (FS). It’s assumed that the ﬂow in each tube is laminar and follows the Hagen-Poiseuille Law, and so the ﬂux through tube (i,j) is given by Qij = πr4 ij(pi −pj) 8ηLij = Dij(pi −pj) Lij , (2.1) where η is the viscosity of the ﬂuid andDij = πr4 ij/8η is the conductivity of the tube. Experimental observations show that most of the plasmodium lies over the FS and that the ﬂuid is rhythmically exchanged between those regions through the network. Hence, it’s assumed that only the FS nodes can drive the ﬂow and that the tubes are passive elements that regulate their thickness accordingly [7]. One of the FS always acts like the source (N1) and the other as the sink (N2). At each time step, a constant ﬂuxI0 ﬂows from the source and into the sink. Since the total ﬂux must be conserved, the inﬂow and outﬂow at each node must be balanced, which implies ∑ j Qij =    0 for i̸= 1,2 −I0 for i= 1 I0 for i= 2 . (2.2) Letting the pressure at the sink node be 0 and knowing the tube lengths,Lij, and conductivities,Dij, the ﬂux through each one can be computed in each step by solving the linear system (2.2) together with the equation (2.1). The plasmodium adapts its network through the change of the tubular thickness,rij, in response to the magnitude of the ﬂux|Qij|ﬂowing through each vessel. In the model, this adaptation is captured by the conductivitiesDij ∝r4 ij, which are updated at each step according to dDij dt = f(|Qij|) −µDij , (2.3) where f is a monotonously increasing function, satisfyingf(0) = 0, that describes the tubes’ expansion response to the ﬂux, andµis a positive constant. The second term represents the rate of tube shrinkage, implying that in absence of ﬂux the tube disappears at an exponential rateµ. Together this implies that conductivities tend to increase in edges with big ﬂux. New conductivities are fed back to (2.1) to calculate new ﬂuxes and pressures. The network’s ﬂux conversation induces competition between edges 13for more ﬂux. Shorter tubes are favoured through a positive feedback loop: shorter tubes carry more ﬂow, and therefore grow, which in turn increases their ﬂow in subsequent iterations and so on. This iterative process continues until the network ﬁnally converges to a steady state. This algorithm was readily extended to include the network adaptation in the presence of multiple FS [6], as an attempt to explain the Tokyo experiments (Figure 2.4). In this case, the plasmodium network is initialized as a random graph, and at each time the source and sink are randomly chosen from FS nodes. Regarding the update rule (2.3) two functional forms off are mainly used in literature: f(|Q|) = |Q|γ f(|Q|) = Qγ 1 + Qγ (γ >0) . (2.4) While the ﬁrst performs better on maze solving (i.e., when there is only one source and one sink), the second one is more adequate to design eﬃcient networks in the presence of multiple FS. Note that the latter describes a sigmoid response of the tube diameter to the ﬂux ﬂowing through. Biologically this is more realistic since the saturation mimics the maximum distensibility of the tubes. A typical simulation of thePhysarum solveris presented in Figure 2.9. Despite the resulting networks resembling the ones produced byPhysarum (Figure 2.4), the model breaks some physical principles, namely the conservation of volume of the circulating ﬂuid, which will be discussed in more detail in the following chapter. Furthermore, the model only describes the network optimisation behaviour of full- grown plasmodium, represented by the initial random mesh, and thus, can’t account for the formation of the network itself. (a)  (b)  (c)  (d) Figure 2.9: Simulation of thePhysarum Solver (γ = 1.8, I0 = 2.). Food sources are represented by blue dots. (a) Initially the space was populated with a ﬁnely meshed network of thin tubes.(b,c) Over time, many of these tubes died out, and the other few were selectively thickened resulting in a stable optimised network (d). Adapted from [6]. Nevertheless, the basic principle of the feedback mechanism between the ﬂux and the veins thickness in which the model is based seems to agree with the experimental observations. Also, the modelling approach ofPhysarum as a ﬂow network is naturally more biologically realistic comparing to agent-based models which are in general purely phenomenological. For these reasons, in the following chapter, we propose a new ﬂow-based model for the network optimisation, inspired by thePhysarum Solverprinciples, as an attempt to solve some of its problems. 14Chapter 3 Physarum’s Network Adaptation Model In this chapter, we formulate a new ﬂow-based model to describePhysarum’s network optimisation. As a starting point, we ignore the network growth and begin to model the network adaptation of static individuals. Note that we don’t attempt to model the contractile activity ofPhysarum, which is outside the scope of this work. This new model is inspired by the previousPhysarum Solver model, but we propose a generic class of evolution laws for the channels conductivities consistent with the assumptions of a Hagen-Poiseuille ﬂow, namely the volume conservation of the circulating ﬂuid. We start by showing how this is violated in the previous model, and from there we derive the new class of adaptation rules. Then, based on the principle of least dissipation of energy, we derive an explicit form of the adaptive equations. Finally, we explore some of the model features as a ﬁrst overview. 3.1 Hagen-Poiseuille Flow Physarum can be naturally modelled as a ﬂow transport network made of hydraulic-coupled channels with adapting radius inside which the protoplasm ﬂows. ThePhysarum Solver model presented in the last chapter, and other similar ﬂow-based models, typically assume that the protoplasm follows a Hagen- Poiseuille ﬂow. The Hagen-Poiseuille equation describes the steady-state laminar ﬂow of incompressible, Newtonian ﬂuids through a channel with a constant, and much smaller than its length, cross-section. The ﬂow through the channel is driven by a pressure drop between the two ends and the friction between the ﬂuid layers due to the viscosity of the ﬂuid,η. The counterbalance between the two forces results in a parabolic velocity proﬁle of the ﬂow. For a cylindrical channel(i,j) with radiusrij and lengthLij, the Hagen-Poiseuille equation states that the relation between the volumetric ﬂux ﬂowing through it,Qij, and the pressures at both ends,pi and pj, is given by Qij = πr4 ij(pi −pj) 8ηLij = Dij(pi −pj) Lij , (3.1) 15where Dij = πr4 ij/8η is the conductivity of the channel. A laminar ﬂow is characterised by a low Reynolds number (Re), which is deﬁned as the ratio of the inertial forces to the viscous forces acting on the ﬂuid. The protoplasm is well described as a low- Reynolds-number incompressible ﬂuid, i.e., with a constant density. Recent measurements [21] estimate that the maximum Reynolds number of protoplasmic ﬂow, obtained using the top speed of the shuttle streaming inside large veins ofPhysarum, is Re∼0.1, which is four orders of magnitude lower than that required for the onset of turbulence in a cylindrical tube. Furthermore, the results of the last study and of former experiments [22, 23] show that the ﬂow velocity proﬁle across the diameter of the vein is always parabolic regardless of the speed of shuttle streaming. These ﬁndings indicate that the protoplasm viscous forces dominate over the inertial forces, and the ﬂow is indeed laminar, thus supporting the assumption of a Hagen-Poiseuille ﬂow. 3.1.1 Derivation In the following, the Hagen-Poiseuille equation is derived from ﬁrst principles [24, 25]. Consider a cylindrical ﬂuid element of radiusr and lengthdz. The surrounding ﬂuid exerts pressure on the end faces of the cylinder, which is assumed to be constant over any chosen cross-section of the pipe (i.e., p = p(z)). At one end is acted a pressurep and at the other, the same pressure lower bydp >0, p−dp. The resultant force on the volume element arising from the pressure, responsible for driving the ﬂow, is given by dFp = [ pπr2 −(p−dp)πr2] ˆ z = πr2dpˆ z. (3.2) The ﬂow is assumed to be laminar, meaning that can be described as the relative motion of thin concentriccylindricallayersofﬂuidthatslideovereachotherintheaxialdirection, withouttheoccurrence of lateral mixing. The velocity of the ﬂuid is thenv = vˆ z. Due to the viscosity of the ﬂuid,η, there is friction between the ﬂuid layers, which acts as ﬂow resistance. The viscous drag force, dFvis, is proportional to the area of contact between the layers, 2πrdz, and to the shear stress, which for a Newtonian ﬂuid varies linearly with velocity gradient perpendicular to the ﬂow direction, dFvis = ( −ηdv dr ) 2πrdz(−ˆ z) = ηdv dr2πrdz ˆ z. (3.3) The velocity gradient can be derived from the steady-ﬂow condition, which requires the balance of the two counteracting forces, dFp + dFvis = 0 ⇔ ⇔ − πr2dp= ηdv dr2πrdz ⇔ ⇔ dv dr = −1 2η dp dzr. (3.4) It’s assumed that the ﬂow is fully developed (dv dz = 0 ) and axisymmetric (dv dθ = 0 ) . The former 16implies that right-hand side of the above equation can’t depend onz, and thus pressure gradient,dp dz, is a constant. For a channel of lengthL, and denoting the pressure diﬀerence between the two ends by ∆p >0 (high pressure minus low pressure), follows thatdp dz = ∆p L . By integrating the above equation, we ﬁnd that the velocity proﬁle,v(r), is given by v(r) = ∫ dv drdr= − ∫ 1 2η ∆p L rdr = −1 4η ∆p L r2 + C . (3.5) The constant of integrationC is found by imposing no-slip boundary conditions at the wall. In other words, it’s assumed that the ﬂuid particles adhere to the wall, and thus, there is no relative motion between the two, i.e.,v(R) = 0. This implies that v( r= R) = −1 4η ∆p L R2 + C = 0 ⇔ ⇔ C = 1 4η ∆p L R2 . (3.6) Substituting C back in equation 3.5 leads to v(r) = 1 4η ∆p L (R2 −r2) , (3.7) which shows that the velocity of the ﬂuid displays a parabolic proﬁle, being zero at the wall, and attaining its maximum at the center of the pipe,vmax = v(0) = 1 4η ∆p L R2. To compute the volumetric ﬂow rate, consider a ring of ﬂuid with thicknessdrat a distancerfrom the centre of the pipe. Within a timedt, the ﬂuid ﬂowing through the ring covers a volumedV = 2πrdrv(r)dt. The ﬂow ratedQ of the ring ﬂuid element is therefore dQ= dV dt = 2πrv(r)dr. (3.8) The total ﬂow rate is ﬁnally obtained by integrating 3.8 over the entire cross-section of the pipe, yielding Q= ∫ dQ= ∫ R 0 2πrv(r)dr = 2π 4η ∆p L ∫ R 0 r(R2 −r2)dr = π 2η dp dz [ R2 r2 2 −r4 4 ]⏐⏐⏐⏐ r=R r=0 = πR4 8η ∆p L . (3.9) 3.2 Model Motivation As seen in section 2.4.2, thePhysarum Solver model assumes that the ﬂow insidePhysarum networks can be described by the Hagen-Poiseuille equation 3.1. A ﬂuid undergoing a Hagen-Poiseuille ﬂow is 17assumed to be incompressible, which means that it has a constant density. Since the amount of ﬂuid ﬂowing through the network remains constant, this implies that no changes of the network’s total volume may occur during the optimisation process. Therefore the adaptation dynamics proposed by Tero, dDij dt = f(|Qij|) −µDij , (3.10) should ensure that the volume of the network is conserved over time. Each vessel is approximated by a cylindrical tube with radiusrij and length Lij, corresponding to a volume Vij = πr2 ijLij. Given that Dij = πr4 ij/8η, the volume of each vessel can be rewritten as Vij = √8πηLij √ Dij. Hence, the conservation of the network’s total volume reads V= ∑ (i,j)∈E Vij = β ∑ (i,j)∈E √ DijLij = const , (3.11) where β = √8πη. According to the model (3.10), the change of the network volume over time is given by dV dt = β ∑ (i,j)∈E 1 2 √ Dij dDij dt Lij ∝ ∑ (i,j)∈E [ f(|Qij|) −µDij ] Lij√ Dij . (3.12) Since the choice off(|Qij|) is arbitrary, as long asf(0) = 0 and f′(|Qij|) > 0, one can expect that not every choice results in (3.12) being identically zero, and thus doesn’t guarantee that the volume of the ﬂuid is conserved. This is conﬁrmed by the results of Figure 3.1, showing four simulations of thePhysarum Solver in a small graph, for diﬀerent distributions of sources and sinks, and diﬀerent choices of adaptation functions, f, like the ones typically found in literature (2.4). In the ﬁrst two (Figure 3.1a and Figure 3.1b) we have considered ﬁxed conﬁgurations of sources and sinks, while in the other two (Figure 3.1c and Figure 3.1d), in each time step a source and a sink are randomly picked from the set of terminals, as described in section 2.4.2. In each case, it’s shown the plot of the network volume over the simulation time. As the plots show, the volume isn’t conserved throughout the simulation, but rather tends to decrease over time, converging to a much lower value than the initial one, which corresponds to the volume of the steady-state solution. Note that in the last two simulations the volume exhibits some ﬂuctuations due to the stochastic choice of the source-sink pair in each step, but the asymptotic behaviour is the same. We conclude that the adaptation dynamics proposed by Tero violates the volume conservation of the ﬂuid, and thus the assumption that the ﬂuid is incompressible. Therefore the model is physically inconsistent. This contradiction has been already reported in [26]. In the following, we propose a more realistic evolution law for the channels conductivities that takes this assumption into account. 18iterations = 0  Parameters γ = 1     fs = fixed I0 L = 5 mesh = d noise = 0.5 G_seed = 31 I0 = 1  D0 ij = 1 dt = 0.1 fs_seed = None thickness = 2 V0 = 15.45 Tero Model (γ= 1)   dDij dt = ∥Qij∥γ−Dij iterations = 3995 Steady state reached   max|Dt + 1 ij − Dt ij|  1e-08  Parameters γ = 1     fs = fixed I0 L = 5 mesh = d noise = 0.5 G_seed = 31 I0 = 1  D0 ij = 1 dt = 0.1 fs_seed = None thickness = 2 V0 = 15.45 Tero Model (γ= 1)   dDij dt = ∥Qij∥γ−Dij 0 500 1000 1500 2000 2500 3000 3500 4000 Time steps 0.0 0.2 0.4 0.6 0.8 1.0V / V0 (a) Simulation with ﬁxed terminals: 1 source (yellow circle) and 1 sink (red triangle) with intensitiesqsource = −qsink = 1. Adaptation function:f(|Qij|) =|Qij|. iterations = 0  Parameters γ = 1.5     fs = fixed I0 L = 5 mesh = d noise = 0.5 G_seed = 336 I0 = 1  D0 ij = 1 dt = 0.1 fs_seed = None thickness = 2 V0 = 15.44 Tero Model (γ= 1.5)   dDij dt = ∥Qij∥γ 1 + ∥Qij∥γ −Dij iterations = 300 Steady state reached   max|Dt + 1 ij − Dt ij|  1e-08  Parameters γ = 1.5     fs = fixed I0 L = 5 mesh = d noise = 0.5 G_seed = 336 I0 = 1  D0 ij = 1 dt = 0.1 fs_seed = None thickness = 2 V0 = 15.44 Tero Model (γ= 1.5)   dDij dt = ∥Qij∥γ 1 + ∥Qij∥γ −Dij 0 50 100 150 200 250 300 Time steps 0.0 0.2 0.4 0.6 0.8 1.0V / V0 (b) Simulation with 5 ﬁxed terminals: 2 sources (yellow circles) and 3 sinks (red triangles), with intensities qsource = −qsink = 1. Adaptation function:f(|Qij|) =|Qij|1.5/(1 +|Qij|1.5). iterations = 0  Parameters γ = 1.8     fs = Tero L = 5 mesh = d noise = 0.5 G_seed = 31 I0 = 1  D0 ij = 1 dt = 0.1 fs_seed = None thickness = 2 V0 = 15.45 Tero Model (γ= 1.8)   dDij dt = ∥Qij∥γ 1 + ∥Qij∥γ −Dij iterations = 500 Steady state reached   max|Dt + 1 ij − Dt ij|  1e-06  Parameters γ = 1.8     fs = Tero L = 5 mesh = d noise = 0.5 G_seed = 31 I0 = 1  D0 ij = 1 dt = 0.1 fs_seed = None thickness = 2 V0 = 15.45 Tero Model (γ= 1.8)   dDij dt = ∥Qij∥γ 1 + ∥Qij∥γ −Dij 0 100 200 300 400 500 Time steps 0.0 0.2 0.4 0.6 0.8 1.0V / V0 (c) Simulation with 5 terminals (blue) and random choice of a source-sink pair in each step (qsource = −qsink = 1) like the original algorithm. Adaptation function:f(|Qij|) =|Qij|1.8/(1 +|Qij|1.8). iterations = 0  Parameters γ = 1.2     fs = Tero L = 5 mesh = d noise = 0.5 G_seed = 242 I0 = 1  D0 ij = 1 dt = 0.1 fs_seed = None thickness = 2 V0 = 15.39 Tero Model (γ= 1.2)   dDij dt = ∥Qij∥γ−Dij iterations = 1500 Max iterations reached   max_frames = 1500  Parameters γ = 1.2     fs = Tero L = 5 mesh = d noise = 0.5 G_seed = 242 I0 = 1  D0 ij = 1 dt = 0.1 fs_seed = None thickness = 2 V0 = 15.39 Tero Model (γ= 1.2)   dDij dt = ∥Qij∥γ−Dij 0 200 400 600 800 1000 1200 1400 Time steps 0.0 0.2 0.4 0.6 0.8 1.0V / V0 (d) Simulation with 6 terminals (blue) and random choice of a source-sink pair in each step (qsource = −qsink = 1) like the original algorithm. Adaptation function:f(|Qij|) =|Qij|1.2. Figure 3.1: Simulations of the Physarum Solver for diﬀerent choices of the adaptation functionf in (3.10), diﬀerent distribution of sources and sinks, consideringµ = 1 and a total ﬂux ﬂowing through the network I0 = 1. The simulations were carried out in planar graphs with5 ×5 nodes, considering initial homogeneous conductivities,Dij(0) = 1. In each case, the left image depicts the initial network geometry, the middle image corresponds to the steady state of the adaptation mechanism, and the right plot represents the volume of the network over time,V, normalised to the initial volume,V0. The thickness of the black lines is proportional to the radius of the edges(∼D1/4 ij ). The plots show that the volume is not conserved throughout any simulation, which implies that the Hagen-Poiseuille ﬂow can no longer be applied as the ﬂuid is compressible. 193.3 Model Formulation 3.3.1 Physarum as a Flow Network Similarly to the previous model, the geometry ofPhysarum’s vein network is described as an undirected, planar and connected graph,G= (V,E), embedded in the Euclidean plane, whereV is the set ofN nodes or vertices with coordinates(xi,yi) for i∈V, andE is the set ofM straight edges(i,j), connecting the node i and j. The edges represent the network veins (channels), and the nodes the junctions between them. Each node i is characterised by a pressurepi. An edge (i,j) is assumed to be a cylindrical elastic channel with a ﬁxed lengthLij, and a radiusrij which can change in response to the magnitude of the ﬂux ﬂowing through it. The ﬂuid in the network is viscous and incompressible, and undergoes a Hagen- Poiseuille ﬂow, being the channel ﬂuxesQij given by 3.1. IfQij > 0, then the ﬂuid ﬂows fromi to j, while Qij <0 means that the ﬂow is fromj to i. We assume that the network ﬂows are driven by a set of sources and sinks (terminals), located at ﬁxed nodes, which mimic stimulated regions ofPhysarum. Each node i is thus characterised by a net ﬂux qi. If a nodei is a source, it injects ﬂow in the system, andqi >0. If the node is a sink, it removes ﬂow from the system, andqi <0; otherwiseqi = 0. The volume conservation of the ﬂuid imposes that ∑ i∈V qi = ∑ i∈sources qi + ∑ i∈sinks qi = 0 . (3.13) The channel ﬂuxes can be determined by the conservation of the ﬂux at each vertexi, which is also a direct consequence of the incompressibility of the ﬂuid, ∑ j∈N(i) Qij = ∑ j∈N(i) Dij(pi −pj) Lij = qi , i ∈V (3.14) where N(i) = {j : ( i,j) ∈E}is the set of the neighbour nodes of the nodei, and Dij = Dji is the conductivity of the channel (i,j). 3.3.2 Adaptation Dynamics Experimental observations seem to support the current-reinforcement optimisation principle behind the Physarum Solvermodel, i.e., the feedback between the ﬂux and the vessel thickness. The conservation of the ﬂuid’s volume should play a crucial role in this regard, since it implies that vessels with higher ﬂow rates expand at the expense of vessels with lower ﬂow rates, which consequently shrink and eventually collapse. In this way, the volume conservation acts as a global constraint which enables the adaptation dynamics at one part of the network to aﬀect the dynamics across the whole network. Therefore, the optimisation can’t be described by a local mechanism, as inPhysarum Solver, and the role of volume constraint in the process must be acknowledged in the model. In general, this problem is overlooked in the literature, so we aim to construct a more realistic model which tackles this issue by deriving a general adaptation rule which intrinsically conserves the volume 20(3.11). According to our knowledge, the only solutions found in literature consist in projecting in each time step the solution of the adaptive equation (3.10) onto the surface deﬁned by the volume constraint [27], or through a simple global rescaling of the conductivities in each time step [28]. These constitute mathematical ad hocsolutions which obfuscate the physical interpretation of the adaptation mechanism, and introduce unpredictable changes in the dynamics. The new adaptation rule should be able to capture the same current-reinforcement feedback dynamics as that ofPhysarum Solver, but with the constraint that the total volume of the network must remain constant over time. This suggests basing our model on an adaptation rule very similar to the one proposed by Tero et al. (3.10). However, since we assume that the adaptation is described by variations of the vessels cross-section,πr2 ij ∝ √ Dij, in response to the ﬂux ﬂowing through, it’s more meaningful to derive an equation for the √ Dij rather than for the conductivitiesDij. Therefore we make theansatz that d dt √ Dij = f(Q) −µ √ Dij , (3.15) wheref(Q) is an unknown function of all the network ﬂuxesQ, withf(0) = 0, which generically describes the channel expansion due to the ﬂux. The second term represents the tube shrinkage at a rateµ> 0 in the absence of ﬂux. Below, following equation (3.35), we discuss the consistency of expression (3.15). Now we can ask under what conditions this adaption rule conserves the volume. Naturally, this limits the choice of the functionf, in contrast to Tero’s model, where the choice is roughly arbitrary. By diﬀerentiating both sides of (3.11), the conservation of the volume can be described by the following constraint dV dt = β ∑ (i,j)∈E Lij d dt √ Dij = 0 . (3.16) Replacing (3.15) in the last equation and using (3.11) yields ∑ (i,j)∈E Lijf(Q) = µ ∑ (i,j)∈E Lij √ Dij    V/β = µV β . (3.17) To satisfy this condition, it’s suﬃcient to deﬁnef in terms of a new functiong by the relation f(Q) := µ βV g(|Qij|)∑ (k,m)∈E Lkmg(|Qkm|) . (3.18) By introducing the last expression into theansatz (3.15), and redeﬁning the time scaleτ = µt we obtain d dτ √ Dij = V β gij∑ (k,m)∈E Lkmgkm − √ Dij , (i,j) ∈E (3.19) where gij ≡g(|Qij|). Therefore, we conclude that for any choice of the functiong, the volume of the ﬂuid in a network with adaptive channel conductivities evolving according to (3.19) is conserved over 21time. Thus, it correctly describes an optimisation process of a network ﬁlled with an incompressible ﬂuid subjected to a Hagen-Poiseuille ﬂow. Note that, the adaptation rule (3.19) depends on the overall structure of the ﬂows through the term Z ≡∑ (k,m) Lkmg(|Qkm|), making the adaptation explicitly a non-local process, in contrast tothe Physarum Solvermodel (3.10), where the coupling of the system’s dynamics stems only from conservation of the ﬂux at the nodes (3.14). This global coupling factor can be seen as a measure of an eﬀective network length, where the contribution of each channel is weighted by a function of its local ﬂux,g. For a better comparison with the previous model (3.10), given thatd √ Dij = (2 √ Dij)−1dDij, the expression (3.19) can be rewritten in terms of the conductivitiesDij as dDij dτ = 2V β √ Dij gij∑ (k,m)∈E Lkmgkm −2Dij , (i,j) ∈E . (3.20) 3.4 Minimisation of Energy Dissipation In the previous section, we have derived a general class of adaption models describing the ﬂow of incom- pressible ﬂuids in networks of elastic channels with arbitrary geometry. However, to analyse the temporal evolution of a network following the adaptation dynamics (3.19), the functiong must be chosen. The choice of g can’t be completely arbitrary in order to preserve the physical meaning off in (3.15). By (3.18) it’s required thatg(0) = 0 so thatf(0) = 0. Also, g should be such that∂f/∂|Qij|≥ 0 for a given channel (i,j), since a channel should expand if its local ﬂux is increased until it eventually saturates. This last condition is harder to ensure, given thatf now depends on all the channel ﬂuxes, which are inherently coupled through the conservation laws (3.14). Here, the choice ofgis made by introducing the criterion of minimisation of the total power dissipated during the ﬂow (dissipation), subject to the constraint of the ﬁxed volume of ﬂuid, assuming a steady ﬂow imposed by a ﬁxed set of sources and sinks [29]. When a ﬂuid ﬂows through a channel some energy is lost due to friction. The energy dissipation rate depends on parameters such as the ﬂuid’s speed and viscosity, and for a steady-state ﬂow through a channel(i,j) is given byPij = ∆pijQij. The total dissipation of a ﬂow network,P, is the sum of the dissipation at each channel, P= ∑ (i,j)∈E ∆pijQij = ∑ (i,j)∈E Q2 ij Dij Lij . (3.21) The principle of least energy dissipation is widely used in the context of the optimisation of biological transport networks, such as vascular systems, leaf venation in plants, and river networks. The notion of optimal network is typically deﬁned as the one which minimises the energy dissipation under an optional set of constraints, e.g., a limited amount of resources [2, 29, 30], or other cost functionals involving the energy [31]. The functional being minimised and the constraints to which it is subject have a profound impact on the structure and properties of the optimal networks [30]. In the case ofPhysarum, although the mechanism underlying the network adaptation is not well- understood, and is certainly more complex than that, it’s reasonable to admit that is somehow related to 22the optimisation of its network’s energy consumption. However not in a straightforward way as we assume here since the organism is far from a state of equilibrium.Physarum displays a continuous adaptation to the environmental stimuli mediated by the shuttle streaming, which introduces ﬂuctuations in the ﬂux. This means that the ﬂuctuations should be accounted for in the minimisation process [2], but for simplicity, we ignore that and consider the minimisation under a steady ﬂow regime generated by a constant set of sources and sinks. We seek to minimise the dissipation rate,P, of a steady ﬂow network with respect to √ D≡{ √ Dij : (i,j) ∈E}, subject to the local constraints of ﬂux conservation (3.14), and the additional global con- straint of a constant volume,V (incompressible ﬂuid). The problem consists of minimising the following Lagrangian L= P− λ(V− β ∑ (k,m)∈E √ DkmLkm) = ∑ (k,m)∈E Q2 km Dkm Lkm −λ(V −β ∑ (k,m)∈E √ DkmLkm) , (3.22) where λ is a Lagrangian multiplier. Note that the channel ﬂuxes can’t be regarded as independent variables in the minimisation process as they are uniquely determined for a given distribution of the nodes’ ﬂuxes,q, and channels conductivities through (3.14). The set of conductivities that minimisesL is the solution of ∂L ∂ √ Dij = 0 for (i,j) ∈E , ∂L ∂λ = 0 . (3.23) We start by considering the derivative with respect to √ Dij. Using the chain rule we have that ∂L ∂ √ Dij = 2 √ Dij ∂L ∂Dij , (3.24) where ∂L ∂Dij = ∂P ∂Dij −λ ∂ ∂Dij (V− β ∑ (k,m)∈E √ DkmLkm) = ∂P ∂Dij + λβ Lij 2 √ Dij . (3.25) Expanding the ﬁrst term of the last expression yields ∂P ∂Dij = ∂ ∂Dij ∑ (k,m)∈E Q2 km Dkm Lkm = −Q2 ij D2 ij Lij + 2 ∑ (k,m)∈E Qkm Dkm ∂Qkm ∂Dij Lkm . (3.26) Denoting the adjacency matrix of the network byA= [Aij], i.e., the matrix with entriesAij = 1 if (i,j) ∈E and Aij = 0 otherwise, the second term of (3.26) can be written as 232 ∑ (k,m)∈E Qkm Dkm Lkm    pm−pk ∂Qkm ∂Dij = ∑ k∈V ∑ m∈V Akm(pm −pk)∂Qkm ∂Dij . (3.27) Using the symmetry of the adjacency matrix,Aij = Aji, (undirected graph), and the antisymmetry of the ﬂux matrix,Qij = −Qji, this can be further simpliﬁed to (cf. Lemma 2.1 in [32]) ∑ k∈V ∑ m∈V Akm(pm −pk)∂Qkm ∂Dij = ∑ k∈V ∑ m∈V Amkpk ∂Qmk ∂Dij    k↔m − ∑ k∈V ∑ m∈V Akmpk ∂Qkm ∂Dij = ∑ k∈V ∑ m∈V Akmpk ∂(−Qkm) ∂Dij − ∑ k∈V ∑ m∈V Akmpk ∂Qkm ∂Dij = −2 ∑ k∈V pk ∑ m∈V Akm ∂Qkm ∂Dij = −2 ∑ k∈V pk ∂ ∂Dij ∑ m∈N(k) Qkm = −2 ∑ k∈V pk ∂qk ∂Dij , (3.28) where the conservation of the ﬂux (3.14) was used in the last step. For ﬁxed sources and sinks,∂qk ∂Dij = 0, which implies that the second term in (3.26) is zero. Therefore, within the assumption of constant sources and sinks, the derivatives ofLwith respect to √ Dij are given by ∂L ∂ √ Dij = ( −Q2 ij D2 ij Lij + λ′ Lij 2 √ Dij ) 2 √ Dij . (3.29) where we’ve redeﬁned the Lagrangian multiplierλ′= βλ. The minima ofLsatisfy ∂L ∂ √ Dij = 0 ⇔    Dij = (2 λ′ )2/3 Q4/3 ij Dij = 0 . (3.30) The constant of proportionality of the non-trivial minima,α≡ (2 λ′ )2/3 , can be determined by direct substitution in the constraint equation (3.11), resulting from∂L ∂λ = 0, ∂L ∂λ = 0 ⇔ α=   V/β ∑ (k,m)∈E Q2/3 kmLkm   2 . (3.31) Therefore the non-trivial values of conductivities that minimise the total dissipation of the network are 24Dij =   V β Q2/3 ij ∑ (k,m)∈E Q2/3 kmLkm   2 . (3.32) This scaling relation between the conductivities and the ﬂuxes in the minimal conﬁguration of energy is very similar to the one obtained in [29]. On the other hand, we ﬁnd that the non-trivial steady states of the volume-preserving adaptation law (3.19) satisfy d dτ √ D∗ ij = 0 ⇔ D∗ ij =  V β gij∑ (k,m)∈E Lkmgkm   2 . (3.33) Comparing the results (3.32) and (3.19), we conclude that for the choice ofgij = Q2/3 ij , the total dissipation of the network at the steady state is minimal, assuming a constant distribution of nodes ﬂux, q, during the adaptation process. With these choices, we obtain the following dynamics which preserves the volume and converges to a state of minimal dissipated energy, d dτ √ Dij = V β Q2/3 ij ∑ (k,m)∈E LkmQ2/3 km − √ Dij , (i,j) ∈E . (3.34) As a side note, for the simplest case of a single elastic channel with lengthL12 and conductivityD12, the equation (3.19) reduces to d dτ √ D12 = V β 1 L12 − √ D12 , (3.35) which is independent of the choice of the functiong. This diﬀerential equation has a unique stable ﬁxed point for √ D∗ 12 = V/(βL12) which coincides with (3.32). Therefore, according to our model (3.19), for a Hagen-Poiseuille ﬂow on a single elastic tube, the radius of the tube at the steady state minimises the power dissipated by the ﬂow, regardless of the choice ofg. This property is consistent with the distribution of channel ﬂuxes arising from the Kirchhoﬀ Law (3.14), which is by deﬁnition the one which minimises the total energy dissipated [33], and thus justiﬁes theansatz made in (3.15) by adding the term √ Dij. 3.5 Methods 3.5.1 Algorithm The algorithm used to simulate our model is described in the following. First, we generate a planar graph, G, embedded in the two-dimensional Euclidean space, which represents the initial geometry of Physarum’s network, or of any other transport network. The edges lengthsLij are obtained based on the node positions. These two remain ﬁxed throughout the simulation, and only the conductivitiesDij are 25the target of adaptation. Some of the nodes are assigned as sources or sinks and have a net ﬂux diﬀerent from zero, such that the constraint 3.13 is veriﬁed. The initial conditions of our dynamical system are the initial edges conductivities,Dij(0). Usually, we consider an initial homogeneous distribution, and if they aren’t speciﬁed, it’s assumedDij(0) = 1 for all edges(i,j) ∈E. The initial conductivities are used to compute the total volume of ﬂuid through (3.11), where we always consider the parameterβ = 1. Giventhedistributionofthenodes’netﬂuxes, q, andtheinitialsetofedgeconductivities, thetemporal evolution of the system starts with the computation of the channel ﬂuxesQij by solving the linear system (3.14). Then, based on those ﬂuxes, the conductivities of all the channels are updated according to (3.34), or more generally, according to (3.19) given a predeﬁned functiong. The new channel conductivities are used to compute the new channel ﬂuxes in the next time step of the algorithm. These steps are repeated over time until a steady state of channel conductivities is eventually reached. From the numerical point of view, we consider that a steady state is reached when the change in the conductivity of all channels from one step to another is lower than10−6. This can be translated into the following stopping condition max (i,j)∈E |Dij(n∆τ) −Dij((n−1)∆τ)| ≤10−6 . (3.36) where n is the ﬁrst integer for which the inequality is veriﬁed, and∆τ is the time increment used to solve numerically the adaptation rule (3.19). The time of convergence of the adaptation algorithm is thus τ∗= n∆τ <∞. The simulations were all done inPython, exploiting in particular the modulesNetworkX for mesh rep- resentation and graph analysis;SciPy and NumPy for numerical computations; andMatplotlib and seaborn for the graphical interface and plotting. More exotic initial meshes were generated usingMathematica. 3.5.2 Computation of the network ﬂows The network can be seen as an edge-weighted graph, where the edge weights are given byCij = Dij Lij , measuring the degree of ease with which a channel can carry ﬂow. The channel ﬂuxes in each time step can be computed by ﬁrst ﬁnding the pressures of the nodes. The linear system (3.14) can be rewritten as ∑ j Qij = ∑ j Cij(pi −pj) =  ∑ j Cij  pi − ∑ j Cijpj = (∑ k Cik )∑ j δijpj − ∑ j Cijpj = ∑ j [(∑ k Cik ) δij −Cij ] pj = ∑ j ℓijpj . (3.37) where δij denotes the Kronecker delta function. Letp be theN-dimensional vector whose ith element is 26the pressurepi of the ith node, andq the N-dimensional vector whose ith element is the net currentqi through the ith node. If we deﬁneLas theN ×N symmetric matrix with entries ℓij = (∑ k Cik ) δij −Cij , (3.38) the system of conservation laws (3.37) can be written in matrix form as Lp = q . (3.39) The matrixLis the generalisation of the Laplacian matrix for a weighted graph with edge weightsCij. For a simple undirected graph, the Laplacian matrix is deﬁned asL= D−A, where D is the degree matrix 1 and A the adjacency matrix of the graph. In Figure 3.2 it’s depicted the calculation of the Laplacian matrix for a small weighted graph. Note that the Laplacian matrix is singular since all the rows sum to zero, meaning it has an eigenvector 1 = (1,1,..., 1) with a zero eigenvalue (L1 = 0). In fact, it can be shown the dimension of the nullspace of the Laplacian and algebraic multiplicity of the zero eigenvalue is equal to the number of connected components of the graph [34]. In the particular case of connected graphs that we are interested in, the former implies thatrank(L) = N−1. This is related to the fact that the linear system (3.14) is invariant to translations in the pressure, which implies that the pressures are deﬁned up to an additive constant. The physical explanation is that one can only measure pressure diﬀerences, as with potential diﬀerences in electrical circuits. However, the ﬂuxes are uniquely determined since the additive term is cancelled. This means that we can arbitrarily choose the pressure of one node as the reference pressure,p= 0. This can be done indirectly by adding a small arbitrary constant to one diagonal element of the Laplacian, which makes the Laplacian invertible (cf. section 2.4 of [25]). In our case, in each time step, we randomly perturb the diagonal entry of (3.38) corresponding to one of the sink nodes. Finally, once the pressures of the nodes are determined, the channel ﬂuxes can be computed by direct substitution of the former in (3.14). Figure 3.2: Example of the calculation of the Laplacian matrix for a small weighted graph. On the left, the circles represent the nodes, labelled by their index, and lines represent the edges labelled by the corresponding weight. 1The degree matrix of a simple undirected graphG= (V, E) with N = |V |nodes is a N ×N diagonal matrix D = diag(deg1, ..., degN ) where degi is the number of neighbour nodes (degree) of the nodei. 273.5.3 Numerical Scheme Some caution must be taken regarding the choice of numerical scheme used to solve the diﬀerential equation (3.34), or more generally the equation (3.19), since it has to guarantee that the volume is conserved from one step to another. In this work, we adopt a simple explicit Euler numerical scheme with time step∆τ, which results in the following discretisation of (3.19), √ Dn+1 ij = √ Dn ij + ∆τ  V β gn ij∑ (k,m)∈E Lkmgn km − √ Dn ij   , (i,j) ∈E . (3.40) where Dn ij denotes the conductivity of the edge(i,j) at the time τ = n∆τ, i.e Dn ij ≡Dij(n∆τ) and similarly gn ij ≡g(|Qij(n∆τ)|). According to the discretisation (3.40), the volume of the network in the time stepn+ 1 is Vn+1 = β ∑ (i,j)∈E Lij √ Dn+1 ij = β ∑ (i,j)∈E Lij √ Dn ij    Vn +∆τ  V− β ∑ (i,j)∈E Lij √ Dn ij      0 = Vn , (3.41) which implies that, for any choice ofg, the Euler method conserves the volume. Most of the simulations were carried out with a relatively large time increment,∆τ = 0.1, due to computational power limitations. It should be noted however that we’ve performed some tests with diﬀerent time increments, while maintaining the remaining parameters ﬁxed, and in some cases the ﬁnal networks reached by the algorithm were diﬀerent. 3.5.4 Mesh Generation The geometry of the initial networks was generated through a Delaunay Triangulation of a set of points representing the nodes. Given a discrete set of points (nodes)V arbitrarily placed in a plane, a two- dimensional Delaunay triangulation ofV, DT(V), is a triangulation such that no point inV is inside the circumscribed circle of any triangle inDT(V). This type of triangulation results in planar graphs where nodes can have an arbitrarily high degree depending on how they are initially distributed. To generate the underlying initial networks we ﬁrst considered anL×L square lattice of side length 1, with a total ofN = L×Lnodes. Then, the position of the interior nodes of the lattice was randomly perturbed with Gaussian noise, with a given standard deviation which controls the distortion of the ﬁnal mesh. Finally, a Delaunay triangulation is performed on perturbed lattice points, resulting in a graph network Gwith edge lengths,Lij, randomly distributed in the interval[1/(N −1) −ϵ,1/(N −1) + ϵ], where ϵ is a small constant. We’ve performed tests on lattices of linear sizes ranging fromL = 2 up to L= 40. The method used to distribute the points over the plane resulted in networks with an average degree 28typically around 5 or 6. This high number of node neighbours increases the set of possible paths that the adaptation process can select from, which makes the results more robust. Furthermore, by using a Delaunay triangulation of a random set of points to represent the underlyingPhysarum network instead of a structured mesh (rectangular, triangular, hexagonal), we also avoid that eventual lattice symmetries introduce some bias on the results. Some tests were also performed with these types of meshes as underlying networks, although they are not presented here. In these cases, some noise was added to the position of the vertices to break the lattice symmetries. However, as the simulations showed, the optimised networks obtained weren’t in general as natural as the ones obtained from an initial Delaunay mesh, which resembled more closely the networks displayed byPhysarum. 3.6 Exploration of the Model We start by performing simple tests on the model. First, a brief insight into the typical network temporal evolution is given. Then it’s investigated how the choice of sources and sinks and the initial distribution of edge conductivities aﬀect the shape of the ﬁnal network. Finally, it’s studied the general adapta- tion dynamics (3.19) for the class of functionsgγ(|Qij|) = |Qij|γ, where we analyse the ﬁnal network dependency on the parameterγ >0. 3.6.1 Network temporal evolution In Figure 3.3, it’s shown snapshots taken at diﬀerent time steps of two simulations of the model (3.34), depicting the typical evolution of the networks over time, considering a ﬁxed set of terminals. The simulations were carried out on a network resulting from a Delaunay triangulation of 900 randomly placed nodes, with uniform initial channel conductivities,Dij(0) = 1. Figure 3.3a represents the case of only one source and one sink, while Figure 3.3b represents the case of the adaptation in the presence of multiple terminals, where we’ve considered two sources and three sinks. In both cases, the sources give the same amount of ﬂuxqsource = 1/Nsources, which is evenly distributed between the sinks,qsink = −1/Nsinks, where Nsources and Nsinks are the number of sources and sinks respectively. In this work we adopt the following conventions: the sources are represented by yellow circles, the sinks are represented by red triangles, and the thickness of the lines representing the edges is proportional to their radius (D1/4 ij ). The initial network is also shown in light red. From the simulations, we observe that the channels farther away from the straight lines connecting each source-sink pair gradually shrink and eventually vanish, while the closest to those paths are thickened due to the volume conservation. The farthest they are the faster they disappear. On the bottom right of each ﬁgure, it’s plotted the volume of the networks over time, which conﬁrms that the volume is conserved throughout the simulations. In Figure 3.4 we’ve simulated the adaptive mechanism on a more realistic setting mimicking the Physarum scenario. We’ve considered an organism with a circular shape in the presence of a central food source which acts as a source of nutrient ﬂux with intensityqsource = 1. The nutrients are equally distributed between all the nodes at the boundary, which behave like sinks, simulating regions where 29t= 0 t= 50 t= 75 t= 125 iterations = 1  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 42 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 iterations = 49  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 42 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 iterations = 74  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 42 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 iterations = 124  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 42 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 t= 150 t= 300 Network’s volume over time iterations = 149  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 42 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 iterations = 299  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 42 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 0 50 100 150 200 250 300 Time steps 0.96 0.98 1.00 1.02 1.04V / V0 (a) Simulation for one source (yellow) and one sink (red) placed at diagonally opposite corners of a square. The intensity of the terminals are related byqsource = −qsink = 1. t= 0 t= 50 t= 75 t= 125 iterations = 1  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 42 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 iterations = 49  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 42 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 iterations = 74  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 42 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 iterations = 124  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 42 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 t= 200 t= 350 Network’s volume over time iterations = 199  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 42 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 iterations = 349  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 42 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 0 50 100 150 200 250 300 350 Time steps 0.96 0.98 1.00 1.02 1.04V / V0 (b) Simulation for multiple terminals aligned in an M-shaped conﬁguration: 2 sources (yellow) and 3 sinks (red). The intensity of the sources isqsource = 1/2 and the intensity of the sinks isqsink = −1/3. Figure 3.3: Simulation examples of the model (3.34), depicting the network optimisation process at diﬀerent time steps, for two diﬀerent choices of terminals. The simulations start from an initial Delaunay triangulationof900nodes, showninred, wheretheedgesstartwithhomogeneousconductivities, Dij(0) = 1. The thickness of the black lines is proportional to the radius of the channels (∼D1/4 ij ). At the bottom right of each group, it’s plotted the networks’ volume throughout the simulation, which shows that the volume is conserved in both cases. 30there is a continuous uptake of nutrients so that the eventual growth of the organism may occur (which is for now neglected). The ﬁnal network shows a preferential radial orientation consistent with the ﬂux direction and resembles to some degree the networks displayed byPhysarum depicted in Figure 2.2. However, the key diﬀerence is thatPhysarum’s networks have some redundancy which is not observed in the simulations. Thinner secondary veins branch from thicker primary veins and connect on the other end to other main veins, and this pattern is repeated for even thinner veins, resulting in the formation of loopy structures. Those loops and redundant paths are important in the sense that they provide robustness and tolerance to damage to the network, and are characteristic of other biological systems as well like the leaf venation in plants. Based on extensive simulations considering diﬀerent conﬁgurations of ﬁxed terminals, we couldn’t reproduce the formation of loops and redundant connections. In all the simulations, the adaptation dynamics (3.34) resulted in steady-state networks which looked liketrees, i.e., acyclic connected graphs where any two vertices are connected by exactly one path. This is partly due to assuming ﬁxed sources and sinks and thus neglecting ﬂux ﬂuctuations that are believed to be important for the formation of those redundant paths [1, 2]. The impact of the ﬂux ﬂuctuations will be explored in the next chapter. t= 0 iterations = 1  Parameters  = 0.67  L = 30 mesh = disk noise = 0.0 G seed = 2 I0 = 1  D0 ij  = (0.5, 1.5) D0 seed = 42 dt = 0.1 fs_seed = 2  t= 60 iterations = 55  Parameters  = 0.67  L = 30 mesh = disk noise = 0.0 G seed = 2 I0 = 1  D0 ij  = (0.5, 1.5) D0 seed = 42 dt = 0.1 fs_seed = 2  t= 120 iterations = 115  Parameters  = 0.67  L = 30 mesh = disk noise = 0.0 G seed = 2 I0 = 1  D0 ij  = (0.5, 1.5) D0 seed = 42 dt = 0.1 fs_seed = 2  t= 300 iterations = 295  Parameters  = 0.67  L = 30 mesh = disk noise = 0.0 G seed = 2 I0 = 1  D0 ij  = (0.5, 1.5) D0 seed = 42 dt = 0.1 fs_seed = 2 Figure 3.4: Simulation of the model (3.34) replicating thePhysarum’s network adaptation, starting from initial homogeneous conductivities,Dij(0) = 1. The labelstdesignate the iteration number in which the snapshots were taken. The ﬂux is driven by a central source (yellow) and the sinks (red) placed at the boundary of the organism, which represent stimulated regions with high metabolic activity. The source gives qsource = 1, which is evenly distributed between the sinks. The adaptation dynamics results in a tree-like steady-state network (cf. Figure 2.2). 3.6.2 Choice of sources and sinks We now investigate how the choice of terminals and their intensity aﬀect the geometry of the optimal networks. For that, we’ve considered a ﬁxed Y-shaped arrangement of 4 terminal nodes, and tested the adaptation mechanism (3.34) for all the diﬀerent combinations of source and sink states of the 4 nodes (Figure 3.5). Since the all-sinks and all-sources aren’t valid states, as they don’t satisfy (3.13), for 4 terminals there are24 −2 = 14 possible states. To make a fair comparison, the same initial mesh and same initial conductivities (Dij(0) = 1 ) were used in the simulations (Figure 3.5a), and the nodes net ﬂuxes were chosen according to 31qi =    I0/Nsources , i ∈sources −I0/Nsinks , i ∈sinks 0 , otherwise , i∈V (3.42) where I0 = 1, which implies that the same amount of ﬂux ﬂows through the network in each case. The resultant optimised networks obtained for each state are depicted in Figure 3.5. The results show that for a given set of terminals the geometry of the steady-state network depends greatly on which of those are sources and which ones are sinks. An interesting observation is that symmetrical conﬁgurations, i.e., conﬁguration with the opposite choice of sources and sinks, lead to the same ﬁnal network. This isn’t due to the particular choice of the Y-shape arrangement, but it’s a general result conﬁrmed by numerous simulations, which is related with the choice of the nodes net ﬂuxes distribution (3.42). To demonstrate this, let a given conﬁguration of sources and sinks be represented by a source vectorq, and let the symmetrical conﬁguration be described by the source vectorq′, both following (3.42). Since the number of sources in one is equal to the number of sinks in the other, i.e.,Nsources = N′ sinks and Nsinks = N′ sources, we have that qsource −→ q′ sink = I0 N′ sinks = − I0 Nsources = −qsource qsink −→ q′ source = I0 N′sources = − I0 Nsinks = −qsink . (3.43) Opposite conﬁgurations can thus be seen as a transformationq′ →−q in the system. Given the linearity of the conservation laws (3.14), this is translated into a global ﬂow reversal, i.e.,Q′ ij →−Qij for (i,j) ∈E. However, since the adaptation dynamics (3.34) depends only on the magnitude of the ﬂux in each vessel, the ﬂow reversal has no eﬀect on the dynamics. Therefore, we conclude that opposite conﬁgurations yield the same ﬁnal network. More generally, following a similar approach, we can demonstrate that the adaptation dynamics 3.19 is invariant for scale transformations of the nodes’ net ﬂuxes,q →αq, with α ∈R, as long as the function g satisﬁes g(|αQij|) = |δ|g(|Qij|) for some non-zero constantsαand δ (e.g., g polynomial). This means that the parameterI0 >0 in (3.42) has no eﬀect on the conductivities of the steady-state optimal networks. These observations are supported by the results of Figure 3.6, where diﬀerent choices of the parameter I0 were tested for the same geometry and initial conditions. Figures 3.5j to 3.5o also reveal that for certain choices of sources and sinks the adaptation mechanism can lead to apparently disconnected networks, where each “disconnected component” adapts almost in- dependently from the others. Note, however, given the nature of adaptation dynamics (3.34), technically the components are not truly disconnected from each other, but are weakly coupled through edges with very small conductivities which asymptotically approach zero.Further simulations showed that, in gen- eral, this may happen for distributions of the nodes’ ﬂuxes,q, which allow for a partition of the graph Gsuch that the sum of the nodes’ ﬂuxes of every subgraph is (non-trivially) zero. For instance, when the number of sources and sinks is equal and for each source there is a sink with a symmetric intensity, allowing the terminals to be grouped in pairs. Although this is a necessary condition, it’s not suﬃcient 32iterations = 0  Parameters L = 30 mesh = d noise = 0.5 G_seed = 42 I0 = 1  D0 ij  = 1 dt = 0.1 (a) iterations = 330 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 1.7825 Lnetwork = 2.4865 L = 0.7039 = 39.4912 %  config 1: 1110 (b) iterations = 330 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 1.7825 Lnetwork = 2.4865 L = 0.7039 = 39.4912 %  config 14: 0001 (c) iterations = 358 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 1.7825 Lnetwork = 2.4676 L = 0.6850 = 38.4287 %  config 2: 1101 (d) iterations = 358 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 1.7825 Lnetwork = 2.4676 L = 0.6850 = 38.4287 %  config 13: 0010 (e) iterations = 326 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 1.7825 Lnetwork = 1.7825 L = -0.0000 = -0.0000 %  config 4: 1011 (f) iterations = 326 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 1.7825 Lnetwork = 1.7825 L = -0.0000 = -0.0000 %  config 11: 0100 (g) iterations = 319 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 1.7825 Lnetwork = 2.8561 L = 1.0735 = 60.2235 %  config 7: 1000 (h) iterations = 319 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 1.7825 Lnetwork = 2.8561 L = 1.0735 = 60.2235 %  config 8: 0111 (i) iterations = 323 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 1.7825 Lnetwork = 1.5699 L = -0.2127 = -11.9320 %  config 5: 1010 (j) iterations = 323 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 1.7825 Lnetwork = 1.5699 L = -0.2127 = -11.9320 %  config 10: 0101 (k) iterations = 314 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 1.7825 Lnetwork = 1.5444 L = -0.2381 = -13.3572 %  config 6: 1001 (l) iterations = 322 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 1.7825 Lnetwork = 1.5444 L = -0.2381 = -13.3572 %  config 9: 0110 (m) iterations = 320 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 1.7825 Lnetwork = 1.5444 L = -0.2381 = -13.3572 %  config 3: 1100 (n) iterations = 320 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 1.7825 Lnetwork = 1.5444 L = -0.2381 = -13.3572 %  config 12: 0011 (o) Figure 3.5: Dependency of the optimal network geometry on the choice of sources and sinks.(a) Several simulations of the adaptation dynamics (3.34) were carried on an initial network (red mesh) withDij(0) = 1, considering a ﬁxed Y-shaped arrangement of 4 terminals (blue circles).(b-o) steady-state networks reached considering all the diﬀerent combinations of sources (yellow circles) and sinks (red triangles) states of the arrangement of terminals(a). The thickness of the black lines is proportional to the radius of the channels. In each case, the distribution of node ﬂuxes follows (3.42) (I0 = 1 ). The geometry of the steady-state network varies greatly with the choice of sources and sinks. By interchanging the sources with the sinks, the same steady state is obtained. Some conﬁgurations can lead to apparently disconnected solutions(j-o). to guarantee that the steady-state graph is apparently disconnected, otherwise, it would be the case of all the steady-state networks of Figure 3.7. In the context ofPhysarum, these apparently disconnected solutions are unrealistic, since the organism grows and adapts as a single contiguous network. 33It’s not clear what is the most appropriate choice of sources and sinks in the case ofPhysarum. Naturally, food sources are sources of nutrients, but other stimulated regions can also be sources of signalling molecules [17]. However, it’s certain that they aren’t static as we are assuming here, since the ﬂow is not steady, and periodically changes direction. Only ﬂuctuations of the sources and sinks may replicate this shuttle streaming behaviour. I0 = 0.1 I0 = 1 I0 = 10 I0 = 100 iterations = 331  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 2 I0 = 0.1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 Steady state reached   max|Dt + 1 ij Dt ij|  1e-06 (a) iterations = 331  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 2 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 Steady state reached   max|Dt + 1 ij Dt ij|  1e-06  (b) iterations = 331  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 2 I0 = 10  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 Steady state reached   max|Dt + 1 ij Dt ij|  1e-06  (c) iterations = 355  Parameters  = 0.67  L = 30 mesh = d noise = 0.5 G seed = 2 I0 = 100  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 Steady state reached   max|Dt + 1 ij Dt ij|  1e-06  (d) Figure 3.6: Dependency of the steady states of the dynamics (3.34) on a global scaling of the nodes’ net ﬂuxes, considering the same conﬁguration of terminals (3 sources and 4 sinks), initial conditions (Dij(0) = 1 ) and initial mesh. The images show the steady states obtained considering the nodes ﬂux distribution (3.42) for diﬀerent choices of the parameterI0. The dynamics converged to the same steady- stateconductivitiesregardlessofthevalueof I0, whichimpliesthatit’sinvariantforscalingtransformation of q. q = -3 q = 3 q = -3 q = 4 q = -1 (a) q = 2 q = 3 q = 1 q = -3 q = -2 q = -1 (b) q = -3 q = 4 q = -3 q = 3 q = -1 (c) q = 2 q = 3 q = 1 q = -3 q = -1 q = -2 (d) Figure 3.7: (a-b) Examples of (apparently) disconnected steady-state networks of the dynamics (3.34) for two diﬀerent choices of the nodes’ net ﬂux distribution. The labels near each terminal represent their net ﬂux, while the remaining nodes haveq= 0. (c-d) Examples of connected steady states for the same conﬁgurations of(a) and (b), although disconnected solutions would be possible in principle. Note that the nodes ﬂuxes are identical in cases(a) and (c), apart from a permutation of the sources, and the same goes for the cases(b) and (d), apart from a permutation of the sinks. All the simulations were done considering initial conductivitiesDij(0) = 1. The thickness of the black lines is proportional to the radius of the channels. 3.6.3 Initial Conditions We now study the uniqueness of the steady-state solutions of the dynamics (3.34). If the steady state is unique, given a distribution of sources and sinks,q, and an initial mesh geometry, the system should converge to the same ﬁnal network regardless of the initial conductivities,Dij(0). Note that until now we have only considered homogeneous initial conditions,Dij(0) = 1 for all(i,j) ∈E. 34To check if that is the case, several simulations were repeated on the same conﬁguration, but consid- ering diﬀerent sets of initial conductivities, which were drawn each time from a uniform distribution in the interval [0,2], i.e.,Dij(0) ∼U(0,2). Figure 3.8 represents the steady states reached for two diﬀerent conﬁgurations: one considering 2 sources and 5 sinks, and the other considering 3 sources and 5 sinks. The results show that for both cases, the geometry of the ﬁnal network is highly sensitive to the initial conditions, and therefore we conclude that for a given setting the dynamics (3.34) may have multiple steady-state solutions, as one could have expected. Further simulations suggested that, in general, the higher the number of terminals, the easier it is for the system to converge to a diﬀerent steady state. The multiplicity of the steady states can be explained through the positive feedback loop between the ﬂux and veins thickness inherent to the model’s dynamics (3.34). Edges that start with higher conductivities in principle will carry more ﬂow, and therefore are thickened at the expense of channels with initial lower conductivities, where the ﬂow rate is likely lower, due to the volume conservation. This idea is repeated in the following time steps, which implies that the initially thinner channels are likely to shrink faster, while initially thicker channels are likely to thrive. Therefore, if the initial discrepancy of conductivities is considerable, some connections are initially favoured, which may aﬀect the ultimate fate of the system. iterations = 294 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = None I0 = 1  D0 ij  = (0, 2) dt = 0.1 D0 =  (0, 2) iterations = 356 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = None I0 = 1  D0 ij  = (0, 2) dt = 0.1 D0 =  (0, 2) iterations = 353 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = None I0 = 1  D0 ij  = (0, 2) dt = 0.1 D0 =  (0, 2) iterations = 290 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = None I0 = 1  D0 ij  = (0, 2) dt = 0.1 D0 =  (0, 2) (a) 2 sources (yellow) with intensitiesqsource = 1/2, and 5 sinks (red) with intensitiesqsink = −1/5. iterations = 315 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = 42 I0 = 1  D0 ij  = (0, 2) dt = 0.1 D0 =  {D0} iterations = 337 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = 42 I0 = 1  D0 ij  = (0, 2) dt = 0.1 D0 =  {D0} iterations = 349 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = 42 I0 = 1  D0 ij  = (0, 2) dt = 0.1 D0 =  {D0} iterations = 311 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = 42 I0 = 1  D0 ij  = (0, 2) dt = 0.1 D0 =  (0, 2) (b) 3 sources (yellow) with intensitiesqsource = 1/3, and 5 sinks (red) with intensitiesqsink = −1/5. Figure 3.8: Dependency of the geometry of the optimal network on the initial conductivities of the channels. Each group of simulations, (a) and (b), contains examples of the steady-state networks of (3.34) for a given conﬁguration of terminals and same initial mesh (red lines). Each steady state was obtainedbyconsideringadiﬀerentsetofinitialrandomconductivitiesuniformlydistributedintheinterval [0,2]. In both cases, the geometry of the ﬁnal networks is highly sensitive to the initial conditions, which implies that the system may have multiple steady states. The thickness of the black lines is proportional to the radius of the channels. Another interesting thing to test is how the system behaves under scaling transformations of the initial 35conductivities. If all the conductivities are scaled by a factorα> 0, i.e.,Dij →D′ ij = αDij, the channel ﬂuxes are scaled by the same amount (Qij →Q′ ij = αQij) due to the linearity of the Hagen-Poiseuille ﬂow (3.1), while by (3.11) the volume of the network scales asV →V′ = √αV. Consequently, the adaptation dynamics (3.34) is transformed as d dτ √ D′ ij = V′ β (Q′ ij)2/3 ∑ (k,m)∈E Lkm(Q′ km)2/3 − √ D′ ij ⇔ ⇔ ZZ √α d dτ √ Dij = ZZ √αV β \b\b\bα2/3Q2/3 ij ∑ (k,m)∈E Lkm\b\b\bα2/3Q2/3 km −ZZ √α √ Dij ⇔ ⇔ d dτ √ Dij = V β Q2/3 ij ∑ (k,m)∈E LkmQ2/3 km − √ Dij , (3.44) implying that it remains invariant under the transformationDij →αDij with α >0. This means that if D(τ) is solution of (3.34), so isD′(τ) ≡αD(τ). As the solution of (3.34) is unique given an initial condition, this further implies that the only eﬀect on the system of scaling the initial conductivities by a global factor (D(0) →αD(0)) is that the conductivities of the steady state are also scaled by the same factor, but the geometry of the network shouldn’t change (lim τ→∞ D(τ) →α lim τ→∞ D(τ)). Note that these observations can be generalised for the generic adaptation dynamics (3.19), as long as the function g satisﬁes g(|αQij|) = δg(|Qij|) for some constants α >0 and δ >0, which is the case of (3.34), as |αQij|2/3 = α2/3|Qij|2/3. The scaling of the initial conditions was tested by performing repeated simulations on the same conﬁguration, starting from diﬀerent sets of initial homogeneous conductivities,Dij(0) = D0, where D0 > 0 is the only parameter which was varied between runs. In Figure 3.9 are depicted the steady states obtained for diﬀerent values ofD0, considering the same conﬁgurations of Figure 3.8. The results conﬁrm the observations, since for both cases, only the thickness of the channels of the steady-state networks changes when the initial conductivities are scaled by a given global factor, but the networks’ geometry remains the same. 36Dij(0) = 0.1 Dij(0) = 1 Dij(0) = 5 Dij(0) = 15 iterations = 297 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = None I0 = 1  D0 ij  = 0.1 dt = 0.1 D0 =  0.1 iterations = 319 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = None I0 = 1  D0 ij  = 1 dt = 0.1 D0 =  1 iterations = 334 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = None I0 = 1  D0 ij  = 5 dt = 0.1 D0 =  5 iterations = 345 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = None I0 = 1  D0 ij  = 15 dt = 0.1 D0 =  15 (a) 2 sources (yellow) with intensitiesqsource = 1/2, and 5 sinks (red) with intensitiesqsink = −1/5. iterations = 340 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = 42 I0 = 1  D0 ij  = 0.1 dt = 0.1 D0 =  {D0} iterations = 362 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = 42 I0 = 1  D0 ij  = 1 dt = 0.1 D0 =  {D0} iterations = 378 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = 42 I0 = 1  D0 ij  = 5 dt = 0.1 D0 =  {D0} iterations = 388 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06  Parameters L = 30 mesh = d noise = 0.5 G_seed = 42 I0 = 1  D0 ij  = 15 dt = 0.1 D0 =  {D0} (b) 3 sources (yellow) with intensitiesqsource = 1/3, and 5 sinks (red) with intensitiesqsink = −1/5. Figure 3.9: Dependency of the optimal network geometry on the scaling of the initial conductivities. The images correspond to the steady states of (3.34) obtained for a diﬀerent set of homogeneous initial conductivities, Dij(0) = D0, withD0 ∈{0.1,1,5,15}, considering the same conﬁgurations of Figure 3.8. A scaling of the initial conductivities is reﬂected in a scaling of the steady-state conductivities by the same factor. In each group of simulations, only the thickness of the channels (∼D1/4 0 ) is increased asD0 increases, but the geometry of the network is left unchanged. 3.6.4 Phase Transition The choice of g(|Qij|) := |Qij|2/3 in (3.19) used until now led us to study the more general class of polynomial functions,gγ(|Qij|) := |Qij|γ, whereγ >0 is a new parameter. In the following, we analyse the evolution of the system subject to the adaptation dynamics d dτ √ Dij = V |Qij|γ ∑ (k,m)∈E Lkm|Qkm|γ − √ Dij (3.45) as a function of the parameterγ, assumingβ = 1 once more. Note that the steady state of (3.45) forγ = 0 is always homogeneous,√D∗ ij = V/∑ ELkm, regardless of the initial conductivities, asg0(|Qij|) = 1 . Furthermore, if the initial conductivities are also homogeneous,Dij(0) = D0, the steady state corresponds to the initial network, sinceV= D0 ∑ ELkm, and therefore,√D∗ ij = D0 = Dij(t= 0). Considering the same Physarum scenario of the simulation in Figure 3.4, we have simulated the dynamics (3.45) for diﬀerent values ofγ ∈[0,2] starting from random initial conductivities,Dij(0) ∼ U(1/2,3/2). Some examples of the steady-state networks reached by the system for a givenγ are shown in Figure 3.10. The drastic change in the topology of the network suggests the existence of a phase transition in the 37system nearγ = 1/2. For γ <1/2, the steady-state networks have a very redundant structure, and are characterised by having a high density of loops and many veins with similar radius. By contrast, for γ >1/2, no loops remain as the conductivities of most vessels converge to zero. The steady states are trees spanned by the central source and the boundary sinks, with a clear hierarchy of vein thickness. Asγ increases, fewer primary veins remain, and it’s more evident the hierarchy of the veins, with thinner veins as we move away from the central source. However, once more we haven’t found steady-state networks with a reticulated hierarchical structure for any value ofγ, as observed in the real networks produced by Physarum (Figure 2.2), or in leaf venation networks. This is related to the choice of ﬁxed sources and sinks previously discussed [2]. The diﬀerences in the topology of the networks can be better quantiﬁed by the histograms of the steady-state conductivitiesDij for 4 diﬀerent values ofγ, presented in Figure 3.11. Each one has a very unique proﬁle. The phase transition was quantiﬁed through the evaluation of four metrics at the steady-state of each γ. The ﬁrst two are the total power dissipation of the network (3.21) and its total length, L= ∑ (i,j)∈E Lij = ∑ (i,j)∈E √ (xi −xj)2 + (yi −yj)2 , (3.46) where (xi,yi) are the coordinates of the nodei. The third is the ﬂux coupling factor of the adaptation dynamics (3.45) Z = ∑ (i,j)∈E Lij|Qij|γ , (3.47) which can be seen as a measure of an eﬀective network length, where the contribution of each channel is weighted by a function of its ﬂux, |Qkm|γ. Finally, we have considered the loop density, LD, as a simple measure of the network’s redundancy, deﬁned as the number of independent loops2 of the steady-state network, normalised by the number of independent loops of the initial network, which in our case, corresponds to the number of triangles of the initial Delaunay triangulation. Since the networks are connected and planar, the number of loops or faces,f, can be determined using Euler’s formula f = 1 +M−N, whereM = |E|is the number of edges, andN = |V|the number of vertices, yielding [2] LD = 1 + Mfinal −Nfinal 1 + Minitial −Ninitial . (3.48) Note that only edges with conductivities above a threshold (Dthr = 5 ×10−4) were considered in the computation of the total length and the number of loops of the ﬁnal networks. The dependency of these quantities onγ is plotted in Figure 3.12. The change in the slope of the total network’s dissipation,P(γ), observed atγ = 1/2, suggests the existence of a discontinuity on its ﬁrst derivative with respect toγ, which may correspond to a ﬁrst-order phase transition, according to the Ehrenfest classiﬁcation. Forγ ≳ 1 the power dissipated at the steady state starts to notably increase, 2By number independent loops it’s meant the number of graph faces. Faces of a planar graph are regions enclosed by a set of edges that don’t contain any other node or edge. 38which can be understood based on the results of Figure 3.10. Asγ increases, the number of channels connecting the sinks to the main branches decreases, and the channels are very thin, meaning they have low conductivities. This implies that they are associated with a large pressure drop∆pij to reach the signiﬁcantly high channel ﬂux (the ﬂux from the source is split between few channels). Therefore the power dissipated by each one,Pij = ∆pijQij is very large. Figure 3.12b represents a closer look at the plot of the power dissipated nearγ = 2/3. Contrary to expectations, the minimum of the power dissipated is not reached forγ = 2/3, as derived in (3.32). This is most likely explained by the error propagation in the computation of the dissipation. By (3.21), the uncertainty of the dissipation,∆Pij is related to the uncertainties of the conductivities,∆Dij and the ﬂuxes ∆Qij by ∆P P = ∑ (i,j)∈E ( 2∆Qij Qij −∆Dij Dij ) . (3.49) Therefore, the sum of all the edge contributions with low conductivities and low ﬂuxes can be con- siderably large. Other conﬁgurations of terminals were also tested, and in most cases, the minimum was indeed reached atγ = 2/3. The transition is even more clear in the graphs of the total length (Figure 3.12c) and loop density (Figure 3.12d). In particular, the loop density starts and stays at its maximum forγ ≲ 0.3, meaning that the adaptation doesn’t change the topology of the network for low values ofγ, and progressively decreases asγ increases, becoming zero forγ >0.5, implying the absence of loops. This conﬁrms that the steady-state networks forγ >0.5 are spanning trees assuming the thresholdDthr = 5×10−4. Conversely, as Figure 3.12e shows, the quantityZ decreases withγ and displays a smooth transition nearγ = 1/2. In Figure 3.12f it’s plotted the volume of the steady-state network normalised to the volume of the initial one as a function ofγ, showing that the volume is always conserved. Further simulations revealed that the phase transition is independent of the initial distribution of conductivities and of the distribution of sources and sinks. 39iterations = 105 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 3.5976 Lnetwork = 87.2377 L = 83.6401 = 2324.8586 % = 0.00 (a) γ = 0.0 iterations = 587 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 3.5976 Lnetwork = 80.5144 L = 76.9168 = 2137.9772 % = 0.39  (b) γ = 0.39 iterations = 2067 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 3.5976 Lnetwork = 54.4237 L = 50.8261 = 1412.7608 % = 0.47  (c) γ = 0.47 iterations = 3358 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 3.5976 Lnetwork = 20.2181 L = 16.6205 = 461.9830 % = 0.51 (d) γ = 0.51 iterations = 699 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 3.5976 Lnetwork = 19.3939 L = 15.7963 = 439.0725 % = 0.59  (e) γ = 0.59 iterations = 356 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 3.5976 Lnetwork = 19.3482 L = 15.7505 = 437.8020 % = 0.67  (f) γ = 0.67 iterations = 289 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 3.5976 Lnetwork = 17.9117 L = 14.3140 = 397.8730 % = 0.78 (g) γ = 0.78 iterations = 240 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 3.5976 Lnetwork = 16.6821 L = 13.0845 = 363.6963 % = 0.98  (h) γ = 0.98 iterations = 208 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 3.5976 Lnetwork = 15.9723 L = 12.3747 = 343.9658 % = 1.18  (i) γ = 1.18 iterations = 217 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 3.5976 Lnetwork = 15.0824 L = 11.4848 = 319.2311 % = 1.41 (j) γ = 1.41 iterations = 228 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 3.5976 Lnetwork = 13.4982 L = 9.9006 = 275.1959 % = 1.76  (k) γ = 1.76 iterations = 246 Steady state reached  Dij : |Dt + 1 ij Dt ij|  1e-06 Lsteiner_approx = 3.5976 Lnetwork = 12.8827 L = 9.2850 = 258.0863 % = 2.00  (l) γ = 2 Figure 3.10: Dependency of the steady-state networks of the dynamics (3.45) onγ, for the same conﬁgu- ration of Figure 3.4. Note the transition in the network topology fromγ = 0.47 to γ = 0.51. Forγ <1/2 the networks have a redundant structure with loops and no hierarchical organisation, while forγ >1/2 the networks have a tree-like structure with a clear hierarchy of tubes. 40(a) γ = 0.1 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 Dij 0.00 0.01 0.02 0.03 0.04Probability γ=  0.10  Histogram of the edges conductivities (b) γ = 0.47 0 10 20 30 40 50 60 70 80 Dij 0.00 0.01 0.02 0.03 0.04Probability γ=  0.47  Histogram of the edges conductivities (c) γ = 0.51 0 25 50 75 100 125 150 175 Dij 0.00 0.01 0.02 0.03 0.04Probability γ=  0.51  Histogram of the edges conductivities (d) γ = 2/3 0 100 200 300 400 Dij 0.00 0.01 0.02 0.03 0.04Probability γ=  0.67  Histogram of the edges conductivities Figure 3.11: Histogram of frequencies of the steady-state conductivities, for diﬀerent values ofγ, consid- ering the same settings as in Figure 3.10. The distributions are diﬀerent from each other. Forγ = 0.1, the distribution is very similar to the initial one. In the remaining cases, most of the conductivities are concentrated near zero, as most of the edges disappear. For clarity, the histograms focus on lower probabilities. 410.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 γ 0.0 0.2 0.4 0.6 0.8 1.0P / P0 Energy of the final network, E, normalized by E0 = E(γ= 0)  (a) Power dissipated by the steady-state networks, P(γ), normalised byP0 ≡P(γ = 0). 0.55 0.60 0.65 0.70 0.75 0.80 γ 0.040 0.041 0.042 0.043 0.044 0.045 0.046 0.047P / P0 γ= 2 3 Energy of the final network, E, normalized by E0 = E(γ= 0)  (b) Detailed view of the normalised power dissipated, P/P0, nearγ = 2/3. 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 γ 0.0 0.2 0.4 0.6 0.8 1.0L / L0 Length of the final network, L, normalized by L0 = L(γ= 0).    (c) Total Length of the steady-state networks, L, normalised byL0 ≡L(γ = 0). 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 γ 0.0 0.2 0.4 0.6 0.8 1.0Loop density Loop density  (d) Loop density of the steady-state networks deﬁned as (3.48). 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 γ 0.0 0.2 0.4 0.6 0.8 1.0Z / Z0  Z of the final network, Z, normalized by Z0 = Z(γ= 0)  (e) Flux coupling factor of the steady-state networks, Z(γ), deﬁnedas(3.47), normalisedby Z0 ≡Z(γ = 0). 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 γ 0.96 0.98 1.00 1.02 1.04V / V0 Volume  (f) Volume of the steady-state networks,V(γ), nor- malised by the volume of the initial network,V0 ≡ V(t= 0). Figure 3.12: Plots of diﬀerent metrics as functions of gammaγ, evaluated at the steady state of the simulations presented in Figure 3.10. The vertical dashed line marks the transition atγ = 1/2. Note the discontinuity of the slopes of the network’s dissipation, total length and loop density. The correspondent graphs for other choices of initial conductivities and distribution of terminals showed the same tendency, thus supporting the universality of the phase transition. 42Chapter 4 Applications In this chapter, we explore some applications of the model, namely, maze solving and optimal network design. In the ﬁrst case, we replicate the maze setting considered in [5], where it was reported for the ﬁrst time thePhysarum maze-solving abilities (Figure 2.3). In the second case, inspired byPhysarum Tokyo experiments [6] (Figure 2.4), we simulate the model in an arena mimicking the Portuguese mainland. In this regard, we study the importance of ﬂux ﬂuctuations to build eﬃcient and resilient networks, by introducing time-dependent distributions of sources and sinks in the model. The performance and topology of the resulting networks are then compared with those of the real Portuguese railway system. 4.1 Maze Solving The famous maze experiments performed by Nakagaki et al. [5] showed thatPhysarum can solve mazes when two food sources are placed at both ends (Figure 2.3). We now demonstrate that our model can reproduce this phenomenon. We have simulated the adaptation dynamics 3.34 over a graph describing approximately the same maze used in [5]. The maze was generated from a15 ×16 square lattice, where some edges were removed and only the edges which outline the possible routes were left out. Like in the experiments, four possible routes connect the maze entrances (Figure 2.3a). The only diﬀerence is that all the possible paths have the same length due to the symmetry of the underlying square lattice, while in [5] the lengths of the α1, α2, β1, β2 segments which made up the paths were slightly diﬀerent. The initial state represents the Physarum plasmodium ﬁlling the entire maze. The food source stimuli which drive the optimisation in the experiments is mimicked by simulating the model considering a source and a sink placed at the entrances of the maze. In Figure 4.1, it’s shown snapshots of two simulations considering homogeneous and non-homogeneous initial conductivities. In both simulations, the paths which lead to dead ends are the ﬁrst to vanish since there is no ﬂux along those channels, and only paths connecting the source and sink survive. For homogeneous initial conditions (Figure 4.1a), the steady state is a compromise between the four possible solutions. The adaptation mechanism can’t select only one of the paths, as they both have the same 43length, and no channel conductivities are initially favoured. In contrast, when considering random initial conductivities(Figure4.1b), onlyonepathremainsattheend, sincethesymmetryofthesystemisbroken. In this case, the simulations showed that after the dead-end cutting, the system seems to converge to the same degenerated steady state, but soon two of the redundant branches starts to disappear, and only one path survives. The path which is selected depends exclusively on the initial distribution of conductivities (Figure 4.2). t= 0 t= 20 t= 130 iterations = 1  Parameters  = 0.67  L = 10 mesh = maze noise = 0.0 G seed = 42 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 iterations = 19  Parameters  = 0.67  L = 10 mesh = maze noise = 0.0 G seed = 42 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 iterations = 129  Parameters  = 0.67  L = 10 mesh = maze noise = 0.0 G seed = 42 I0 = 1  D0 ij  = 1 D0 seed = 42 dt = 0.1 fs_seed = 2 (a) Simulation with initial homogeneous conductivities,Dij(0) = 1. t= 0 t= 20 t= 130 t= 250 iterations = 1  Parameters  = 0.67  L = 10 mesh = maze noise = 0.0 G seed = 42 I0 = 1  D0 ij  = (0.5, 1.5) D0 seed = None dt = 0.1 fs_seed = 2 iterations = 19  Parameters  = 0.67  L = 10 mesh = maze noise = 0.0 G seed = 42 I0 = 1  D0 ij  = (0.5, 1.5) D0 seed = None dt = 0.1 fs_seed = 2 iterations = 129  Parameters  = 0.67  L = 10 mesh = maze noise = 0.0 G seed = 42 I0 = 1  D0 ij  = (0.5, 1.5) D0 seed = None dt = 0.1 fs_seed = 2 iterations = 249  Parameters  = 0.67  L = 10 mesh = maze noise = 0.0 G seed = 42 I0 = 1  D0 ij  = (0.5, 1.5) D0 seed = None dt = 0.1 fs_seed = 2 (b) Simulation with initial random conductivities,Dij(0) ∼U(1/2,3/2). Figure 4.1: Simulation of the model (3.34) in a maze similar to that of Figure 2.3, starting from diﬀerent sets of conductivities. The snapshots were taken at diﬀerent iterationstof the algorithm.(a)All possible solution paths are selected since they have the same total length and the initial conditions are the same. (b) First the dead ends disappear like in(a), but due to the initial uneven distribution of conductivities, only the path which is initially favoured remains. iterations = 282  Parameters  = 0.67  L = 10 mesh = maze noise = 0.0 G seed = 42 I0 = 1  D0 ij  = (0.5, 1.5) D0 seed = None dt = 0.1 fs_seed = 2 Steady state reached   max|Dt + 1 ij Dt ij|  1e-06 (a) iterations = 256  Parameters  = 0.67  L = 10 mesh = maze noise = 0.0 G seed = 42 I0 = 1  D0 ij  = (0.5, 1.5) D0 seed = None dt = 0.1 fs_seed = 2 Steady state reached   max|Dt + 1 ij Dt ij|  1e-06  (b) iterations = 280  Parameters  = 0.67  L = 10 mesh = maze noise = 0.0 G seed = 42 I0 = 1  D0 ij  = (0.5, 1.5) D0 seed = None dt = 0.1 fs_seed = 2 Steady state reached   max|Dt + 1 ij Dt ij|  1e-06  (c) iterations = 346  Parameters  = 0.67  L = 10 mesh = maze noise = 0.0 G seed = 42 I0 = 1  D0 ij  = (0.5, 1.5) D0 seed = None dt = 0.1 fs_seed = 2 Steady state reached   max|Dt + 1 ij Dt ij|  1e-06  (d) Figure 4.2: Maze solving considering non-homogeneous initial conductivities,Dij(0) ∼U(1/2,3/2). The images represent the ﬁnal networks for diﬀerent initial random states. The system converges to the initially favoured path, i.e., the path which has the highest average conductivity. All four possible paths can be reached by changing the random seed. 44Further simulations showed that if the square symmetry is broken by perturbing the position of the nodes, only one path remains regardless of the initial conditions since the path lengths are diﬀerent. The dynamics of the model can thus account for thePhysarum maze solving ability. Another interesting thing to test would be to simulate the model on a 2D discretisation of the maze, by not ignoring the width of the maze segments. This would introduce more variability in the possible solutions and could accommodate the natural meandering of thePhysarum networks observed in Figure 2.3. 4.2 Finding the Shortest Path Miyaji and Ohnishi [35] proved mathematically that for any initial planar graph embedded in a two- dimensional surface, the Physarum Solver model (3.10) with the particular choice off(|Qij|) = |Qij| and µ = 1 always converges to the shortest-path connecting the two terminals regardless of the initial conductivities. This result was later generalised by Bonifaci et al. [36] for any graph topology, considering the same choice of parameters. We investigated if this holds for our model. For that, we have considered the adaptation dynamics with the choice ofgγ(|Qij|) = |Qij|γ, i.e., the class of models (3.45), and tested if, in the case of only two terminals, the system converges always to the shortest path for a certain value of γ. Extensive simulations were carried in planar graphs with 300 nodes resulting from a triangulation of a square region, for 20 diﬀerent values ofγ in the range[0.55,2]. No values belowγ = 1/2 were considered since the previous analysis of the phase transition showed that for those values the steady states have redundant connections, and therefore can never correspond to the shortest path solution. In each case, the source and the sink were placed in the diagonally opposite corners of the square bounding region, to maximise the variability of paths, the dynamics could choose from. To prevent that any path is initially favoured, the simulations started always from an initial state of homogeneous conductivities, Dij(0) = 1. In Figure 4.3a, it’s plotted the probability of the system to converge for the shortest path as a function of γ, based on 150 realisations for eachγ, each corresponding to a diﬀerent initial mesh. The results show that the probability tends to decrease asγ increases. In particular, forγ = 2/3 the probability is near 85%. However, there isn’t a single value ofγ which guarantees that the ﬁnal solution is always the shortest path. Nevertheless, for all values ofγ the deviations of the total length of the steady state from the length of the shortest path solution are on average very small. This is shown by the results of Figure 4.3b, where it’s plotted the relative error of the total length as a function ofγ, averaged between the simulations where the system didn’t converge to the shortest path. One can observe that the deviations tend to increase withγ, but are always below1%, showing the solution is on average very close to the shortest path. More tests should be performed to validate the high values of probabilities and small deviations obtained, by considering larger graphs, smaller time steps and a higher number of realisations. In particular, it would be interesting to test values nearγ = 0.55, where the probability is above90%. 450.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 γ 65 70 75 80 85 90Shortest Path (%) (a) 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 γ 0.0 0.2 0.4 0.6 0.8Average εSP (%)  (b) Figure 4.3: (a) Probability of the system converging to the shortest path that connects two terminals as a function of the parameterγ, considering the dynamics (3.45).(b) Average percentage error of the steady-state total path length,L, relatively to the length of the shortest path solution,LSP, as a function of γ, i.e., εSP = (L−LSP)/LSP. The computation of the average error only includes the ﬁnal states diﬀerent from the shortest path. The blue shaded region corresponds to the95% conﬁdence interval of the mean error. The results are based on 150 realisations for eachγ value, each realisation corresponding to a diﬀerent initial mesh with 300 nodes. The source and the sink were always placed in diagonally opposite corners of the square. In all the cases, the system starts from homogeneous distribution of conductivities, Dij(0) = 1. 4.3 Approximating Portugal’s Rail System The topology of a biological network has a great impact on its performance and vulnerability, and thus on the chances of survival of the organism. The network structure should provide an eﬀective distribution of the resources (transport eﬃciency) by ensuring a small average path length between any two points. However, the total cost to build the network, which is proportional to its overall length, should be minimised due to the limited amount of available resources. Thus, the transport eﬃciency should be maximised under the constraint of low cost. On the other hand, the network must be robust enough, by providing secondary paths that ensure its normal functioning in case of damage or random failure of some links. Once again, the number of redundant pathways have to be balanced with the additional cost of producing them. Therefore, the design of an optimal network requires a complex trade-oﬀ between the production cost, transport eﬃciency and fault tolerance. As seen in the section 2.3.2, a famous experiment carried by Tero et al. in 2010 showed thatPhysarum builds networks with a good compromise between these three metrics and whose values are comparable to those of real-world infrastructure networks, in particular, to the Tokyo railway system. Motivated by this experiment, we have simulated the adaptation dynamics (3.45) considering a mesh with the shape of mainland Portugal, for diﬀerent values ofγ and other model parameters,and compared the results with the Portuguese railway system. The boundary of Portugal was approximated by a polygon, followed by a triangulation of the enclosed region which resulted in an initial network with 1005 nodes and 2817 edges. We have considered a conﬁguration of 25 terminals, representing the geographical locations of the 18 Portuguese district capitals and 7 additional major cities, except for the Viseu district which was represented by the city Mangualde 46for convenience. The optimised networks resulting from the dynamics (3.45) were compared with the section of the Portuguese rail network which connects those cities [37] as depicted in Figure 4.4. Note that the rail lines connecting Bragança to Tua (Linha do Tua, 1887–2008) and Vila Real to Régua (Linha do Corgo, 1906-2009) are no longer active, but were included for comparison purposes. Valença Viana do  Castelo Braga Bragança Vila Real Mangualde (Viseu) Vilar  Formoso Guarda Castelo  Branco Porto Aveiro Coimbra Leiria Portalegre Elvas Santarém Évora Beja Lisboa Setúbal Sines Lagos Faro Vila Real de  Santo António Pocinho Figure 4.4: Approximation of the part of the real Portuguese rail network used in this study, which connects all the 18 district capitals (green nodes) and 5 additional terminal cities (red nodes). Note that Mangualde was used to represent the capital district Viseu. Some rail lines are no longer active but were included for consistency. The initial graph is assumed to be embedded in the Earth’s surface, meaning that the position of the node iis deﬁned by its geographical coordinates: latitudeΦi and longitudeλi. Consequently, the length of the edge(i,j), Lij, is given by the geodesic distance between the two end-node positions, which can be approximated using the Haversine formula, yielding Lij = 2REarcsin (√ sin2 (Φi −Φj 2 ) + cos(Φi) cos(Φj) sin2 (λi −λj 2 )) , (4.1) where RE = 6371 km is the Earth’s radius. The performance of the optimised networks was evaluated in terms of cost, transport eﬃciency and fault tolerance. The total cost of producing the network is measured by its total length (TL) TL = ∑ (i,j)∈E Lij , (4.2) where the lengths of the edges,Lij, are given by (4.1). Note that in the case of biological networks the cost of producing the networks should be deﬁned more accurately by the total section area of the channels. However, here we are interested in applications to road and rail transportation, where the costs for the construction company are proportional to the total network length. On the other hand, from the perspective of a traveller, an optimal network should ensure a fast travel between two destinations by minimising the distance between them. Thus, the transport eﬃciency (TE) 47is deﬁned as the inverse of the average minimum distance (MD) between all the distinct pairs of terminal cities. IfdSP(i,j) denotes the length of the shortest path in the graph connecting the nodesiand j, and T is the set of the|T|terminals, then TE = MD−1 =   2 |T|(|T|−1) ∑ i<j i,j∈T dSP(i,j)   −1 . (4.3) Lastly, the robustness of the networks is measured by its fault tolerance (FT), which is deﬁned as the probability of the network remaining connected after a single edge is removed. The probability of disconnecting the network equals its fraction of edges which arebridges1. Thus, FT is given by FT = 1 − b |E|, (4.4) where b is the number ofbridges of the ﬁnal network. Note that, as before, only the edges with conduc- tivities above the threshold,Dthr = 5 ×10−4 are considered in the computation of the three metrics of the steady-state networks. Besides the actual railway system (Figure 4.5a), the performance of the ﬁnal networks was also compared with those of the minimal spanning tree (MST) and the complete graph (CG) spanned by the city nodes. The MST2 (Figure 4.5b) is by deﬁnition the graph that connects all the city positions with minimal possible total cost (4.2), while the CG (Figure 4.5c) is the graph that connects every pair of cities by a distinct edge, maximising, therefore, the transport eﬃciency (4.3) and the fault tolerance (FT= 1) at the expense of a tremendous cost. The cost (TL), transport eﬃciency (TE) and fault tolerance (FT) of the ﬁnal networks were normalised to the corresponding values for the CG, yielding TLCG, TECG, FTCG. A comparison between the metrics of these graphs is presented in Table 4.1. To compare the overall performance, the trade-oﬀ between the transport eﬃciency, fault tolerance and the cost was captured by two beneﬁt-cost measures, deﬁned as the ratios BCRTE= TECG/TLCG and BCRFT= FTCG/TLCG. Graph TL (km) TL CG TLMST TLrailway TE (km−1) TE CG TEMST TErailway FT mesh 31482 0.46 22.75 17.04 0.0042 0.95 1.75 1.31 1.00 railway 1848 0.03 1.34 1.00 0.0032 0.73 1.33 1.00 0.45 MST 1384 0.02 1.00 0.75 0.0024 0.55 1.00 0.75 0.00 CG 68009 1.00 49.14 36.80 0.0044 1.00 1.83 1.38 1.00 Table 4.1: Comparison between the initial mesh (mesh), real railway (railway), minimum spanning tree (MST) and complete graph (CG) in terms of the total length (TL), transport eﬃciency (TE) and fault tolerance (FT). The columnsXG denotes the metricX of the graph of each row normalised by the one of the graphG. Until now we have been always considering the adaptation under a ﬁxed set of sources and sinks.The previousanalysisrevealedthatthiseitherresultedintree-likenetworks( γ >1/2)withzerofaulttolerance, 1In graph theory, abridge is any edge whose removal increases the number of connected components of the graph. 2The (geometric) minimum spanning tree (MST) spanned by a set of points (terminals) embedded in a manifold is the graph that connects all the terminals together by geodesic lines, without any cycles, such that the total length of the lines is minimised. Note, however, if other additional nodes are allowed (Steiner points), the graph with the minimum total length which connects all the terminals, either directly or via the Steiner points, is in general diﬀerent, and it’s known as the (geometric) minimum Steiner Tree. 48(a) Railway (1384, 313, 0.45) (b) MST (1848, 417, 0.00) (c) CG (68009, 227, 1.00) Figure 4.5: (a) Graph of Portugal’s rail system (Figure 4.4) with all the city nodes (terminals) marked in blue. (b) The minimum spanning tree (MST) connecting the same set of cities, i.e., thetree graph spanned by the city nodes with the minimal possible total length (4.2).(c) The complete graph (CG) connecting every pair of cities by a distinct edge. The legend of each graph refers to the metrics(TL, MD, FT), where TL and MD are given in kilometres. or poorly-optimised networks similar to the initial mesh, (γ <1/2) which aren’t cost-eﬃcient. In both cases, thismeansthatnetworkshaveanoveralllowperformance, converselytotheonesbuiltby Physarum. To tackle this issue, we now introduce ﬂuctuations in the ﬂux by considering time-dependent sources and sinks, similar to the original model [6]. At each step of the algorithm, two nodes are randomly selected from the set of terminals to drive the ﬂow: one acts as a source with intensityqsource = I0 (I0 >0) and the other as a sinkqsink = −I0, while the remaining terminals haveq = 0. This emulates more closely the shuttle streaming characteristic ofPhysarum networks, by changing the ﬂux direction in each vessel over time, although not in an exactly periodic way. Due to the high ﬂuctuations of the channel ﬂuxes induced by the stochastic choice of the terminals, the stopping condition considered in the case of ﬁxed terminals (3.36) is hardly ever met. Simulations showed that although the system converges to a stable network topology, the channel conductivities display unceasing oscillations which are larger the greater is the time step used,∆τ, being hard to establish their bounds beforehand. Therefore for the stochastic case, we have considered a diﬀerent stopping criterion. As we are mostly concerned with the topology of the steady state, we consider that the algorithm converges when the topology of the network remains unchanged in a period of 500 iterations, meaning that the set of edges with conductivities above a given threshold (Dthr = 5 ×10−4) doesn’t change over that period. To regularise the eﬀect of the ﬂuctuations on the adaptation, a smaller time step was used,∆τ = 0.02, corresponding therefore to a period of 10 time units without any changes in topology. This stopping criterion has some limitations since it depends on the choice of the number of iterations, which was carefully chosen according to the∆τ used and the overall the time scale of the adaptation mechanism. 494.3.1 Dependence of the performance onγ It was ﬁrst analysed the dependency of the topology and performance of the ﬁnal networks on the parameter γ of the adaptation dynamics (3.45). Numerous simulations were carried out for diﬀerent values ofγ, considering the same mesh, initial conditions (Dij(0) = 1) and seed. Some examples of the diﬀerent networks reached by the system are given in Figure 4.6. For low values of γ, the ﬂux ﬂuctuations result in the formation of stable redundant paths that improve the network’s robustness. In particular, forγ <1/2, similarly to the case of ﬁxed terminals, most of the initial mesh remains and very few preferential pathways are formed, resulting in networks with a huge cost. A drastic change in the topology is observed nearγ = 1/2, characterised by a substantial reduction in the network’s total length yet some alternative routes remain, leading to a better compromise between cost and fault tolerance. A good trade-oﬀ between the cost and the transport eﬃciency is also reached for low values of γ due to the formation of additional bifurcation points, which resemble Steiner tree type of connections. As γ is increased the redundant paths progressively disappear, and the system slowly converges towards the MST solution (Figure 4.5b). The minimisation of the cost is achieved with the inevitable complete loss of the network’s robustness. Simulations also showed that for a givenγ, the structure of the ﬁnal network depended slightly on the random seed used, however these observations remain always valid. The trade-oﬀ between the network’s cost, transport eﬃciency and fault tolerance can be better quan- tiﬁed by the plots of Figure 4.7. As the Figures 4.7a and 4.7b conﬁrm, the transport eﬃciency and fault tolerance tends to decrease asγ increases. Interestingly, most of the simulation results of the ﬁrst plot lie in a well-deﬁned curve that resembles the Pareto front [38–40] associated with the compromise between maximising the eﬃciency while minimising the overall cost. By deﬁnition, the networks lying on the Pareto front can’t achieve a better transport eﬃciency without an increase of the cost, neither can have a lower cost without a decrease in the eﬃciency. The real railway is quite far from this fron- tier. In particular, one can observe that forγ ∼0.8, the simulations achieve signiﬁcantly better fault tolerance and a higher transport eﬃciency comparing to the real railway, with a slightly smaller cost, implying a much better beneﬁt-cost trade-oﬀ. The overall performance, captured by the two beneﬁt-cost ratios, BCRTE= TECG/TLCG and BCRFT= FTCG/TLCG, is depicted in Figure 4.7c, as a function of γ. For γ ≤0.6, the excessive cost doesn’t compensate the increase of the network’s robustness and transport eﬃciency, resulting in worse performance than the MST and the real railway. However, in the interval γ ∈[0.7,1[, the situation completely changes, and simulations result in networks with a much better compromise between the three metrics than any other graph. For higher values ofγ, the networks still achieve a slightly better eﬃciency-cost trade-oﬀ than the real railway, although the network’s re- silience is completely lost. The graph with the worst compromise is naturally the complete graph, due to the tremendous cost (Table 4.1) of connecting all the pairs of cities individually. In conclusion, for γ ∈[0.7,1[, the model results in networks with the overall best performance and, in general, higher than the performance of the real railway, MST and CG. 50Parameters  = 0.45  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 2  choose_fs = Tero D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = None fs_seed = None D0 seed = None nodes = 1005 edges = 2817 iterations = 7500 Metrics Steady-State  TL = 30031.277 MD = 236.492 FT = 0.0 TL_mesh = 0.954 TL_railway = 16.244 TL_steiner = 20.359 TL_MST = 21.685 TL_CGpy = 0.423 TL_CGmat = 0.442 MD_mesh = 1.0 MD_railway = 0.75 MD_steiner = 0.538 MD_MST = 0.574 MD_CGpy = 1.0 MD_CGmat = 1.043 avg_deg = 5.453 is_connected = 1 n_loops = 1693 LD = 0.934 New Model (poly  = 0.45) (0.45, 30031, 237, 1.00) Parameters  = 0.55  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 1  choose_fs = Tero D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = 42 fs_seed = 42 D0 seed = 42 nodes = 1005 edges = 2817 iterations = 5000 Metrics Steady-State  TL = 2803.787 MD = 274.655 FT = 0.0 TL_mesh = 0.089 TL_railway = 1.517 TL_steiner = 1.901 TL_MST = 2.025 TL_CGpy = 0.04 TL_CGmat = 0.041 MD_mesh = 1.161 MD_railway = 0.994 MD_steiner = 0.625 MD_MST = 0.667 MD_CGpy = 1.161 MD_CGmat = 1.212 avg_deg = 2.156 is_connected = 1 n_loops = 20 LD = 0.011 New Model (poly  = 0.55)  (0.55, 2803, 275, 1.00) Parameters  = 0.67  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 1  choose_fs = Tero D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = 42 fs_seed = 42 D0 seed = 42 nodes = 1005 edges = 2817 iterations = 3000 Metrics Steady-State  TL = 2124.12 MD = 293.168 FT = 0.025 TL_mesh = 0.067 TL_railway = 1.149 TL_steiner = 1.44 TL_MST = 1.534 TL_CGpy = 0.03 TL_CGmat = 0.031 MD_mesh = 1.24 MD_railway = 1.061 MD_steiner = 0.667 MD_MST = 0.712 MD_CGpy = 1.24 MD_CGmat = 1.293 avg_deg = 2.062 is_connected = 1 n_loops = 7 LD = 0.004 New Model (poly  = 0.67)  (2/3, 2124, 293, 0.98) Parameters  = 0.75  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 1  choose_fs = Tero D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = 42 fs_seed = 42 D0 seed = 42 nodes = 1005 edges = 2817 iterations = 2000 Metrics Steady-State  TL = 1816.822 MD = 305.386 FT = 0.12 TL_mesh = 0.058 TL_railway = 0.983 TL_steiner = 1.232 TL_MST = 1.312 TL_CGpy = 0.026 TL_CGmat = 0.027 MD_mesh = 1.291 MD_railway = 1.106 MD_steiner = 0.695 MD_MST = 0.742 MD_CGpy = 1.291 MD_CGmat = 1.347 avg_deg = 2.024 is_connected = 1 n_loops = 3 LD = 0.002 New Model (poly  = 0.75) (0.75, 1817, 305, 0.88) Parameters  = 0.95  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 1  choose_fs = Tero D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = 42 fs_seed = 42 D0 seed = 42 nodes = 1005 edges = 2817 iterations = 1500 Metrics Steady-State  TL = 1667.228 MD = 323.441 FT = 0.13 TL_mesh = 0.053 TL_railway = 0.902 TL_steiner = 1.13 TL_MST = 1.204 TL_CGpy = 0.023 TL_CGmat = 0.025 MD_mesh = 1.368 MD_railway = 1.171 MD_steiner = 0.736 MD_MST = 0.785 MD_CGpy = 1.368 MD_CGmat = 1.427 avg_deg = 2.013 is_connected = 1 n_loops = 2 LD = 0.001 New Model (poly  = 0.95)  (0.95, 1667, 323, 0.87) Parameters  = 1.05  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 1  choose_fs = Tero D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = 42 fs_seed = 42 D0 seed = 42 nodes = 1005 edges = 2817 iterations = 1000 Metrics Steady-State  TL = 1566.12 MD = 358.95 FT = 0.651 TL_mesh = 0.05 TL_railway = 0.847 TL_steiner = 1.062 TL_MST = 1.131 TL_CGpy = 0.022 TL_CGmat = 0.023 MD_mesh = 1.518 MD_railway = 1.299 MD_steiner = 0.816 MD_MST = 0.872 MD_CGpy = 1.518 MD_CGmat = 1.583 avg_deg = 2.0 is_connected = 1 n_loops = 1 LD = 0.001 New Model (poly  = 1.05)  (1.05, 1566, 359, 0.35) Parameters  = 1.35  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 1  choose_fs = Tero D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = 42 fs_seed = 42 D0 seed = 42 nodes = 1005 edges = 2817 iterations = 1500 Metrics Steady-State  TL = 1499.98 MD = 379.32 FT = 1.0 TL_mesh = 0.048 TL_railway = 0.811 TL_steiner = 1.017 TL_MST = 1.083 TL_CGpy = 0.021 TL_CGmat = 0.022 MD_mesh = 1.604 MD_railway = 1.373 MD_steiner = 0.863 MD_MST = 0.921 MD_CGpy = 1.604 MD_CGmat = 1.673 avg_deg = 1.986 is_connected = 1 n_loops = 0 LD = 0.0 New Model (poly  = 1.35) (1.35, 1500, 379, 0.00) Parameters  = 1.55  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 1  choose_fs = Tero D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = 42 fs_seed = 42 D0 seed = 42 nodes = 1005 edges = 2817 iterations = 1000 Metrics Steady-State  TL = 1499.98 MD = 379.32 FT = 1.0 TL_mesh = 0.048 TL_railway = 0.811 TL_steiner = 1.017 TL_MST = 1.083 TL_CGpy = 0.021 TL_CGmat = 0.022 MD_mesh = 1.604 MD_railway = 1.373 MD_steiner = 0.863 MD_MST = 0.921 MD_CGpy = 1.604 MD_CGmat = 1.673 avg_deg = 1.986 is_connected = 1 n_loops = 0 LD = 0.0 New Model (poly  = 1.55)  (1.55, 1500, 379, 0.00) Parameters  = 1.95  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 1  choose_fs = Tero D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = 42 fs_seed = 42 D0 seed = 42 nodes = 1005 edges = 2817 iterations = 1000 Metrics Steady-State  TL = 1488.565 MD = 384.954 FT = 1.0 TL_mesh = 0.047 TL_railway = 0.805 TL_steiner = 1.009 TL_MST = 1.075 TL_CGpy = 0.021 TL_CGmat = 0.022 MD_mesh = 1.628 MD_railway = 1.394 MD_steiner = 0.876 MD_MST = 0.935 MD_CGpy = 1.628 MD_CGmat = 1.698 avg_deg = 1.986 is_connected = 1 n_loops = 0 LD = 0.0 New Model (poly  = 1.95)  (1.95, 1489, 385, 0.00) Figure 4.6: Topology of the networks resulting from the adaptation dynamics (3.45) as a function of the parameter γ, considering a stochastic choice of the source-sink pair from the set of terminals in each step of the algorithm (I0 = 1 and Dij(0) = 1). Forγ <1/2 the dynamics result in poorly-optimised networks very close to the initial mesh, similar to the case of ﬁxed terminals. Asγ is increased, the networks slowly evolve towards the minimum spanning tree (Figure 4.5b), losing all the redundant paths which provide robustness to the network. The legend of each image refers to the network metrics(γ, TL, MD, FT), where TL and MD are given in kilometres. The top 3 networks with the best overall performance are highlighted in green. 51(a)  (b) (c) Figure 4.7: Network performance of the adaptation dynamics (3.45) as a function of the parameterγ, with the other parameters ﬁxed, including the random seed.(a-b) Transport eﬃciency (4.3) and fault tolerance (4.4) plotted against the total length of the network (4.2) (cost). The metrics are normalised to those of the complete graph (CG) connecting the city nodes, yielding TLCG, TECG, FTCG. The coloured circles represent the simulation results asγ was varied from 0.55 to 2.00, considering the stochastic choice of the source-sink pair withI0 = 1, and initial conditionsDij(0) = 1. The results were compared with the same normalised metrics of the real railway (green triangles) and MST network (red squares).(c) Plots of the beneﬁt-cost ratios, deﬁned as BCRTE= TECG/TLCG and BCRFT= FTCG/TLCG, as the function ofγ, compared with the ones of the real railway, MST and CG. The proposed optimal models (i.e., which result in networks with the best performance trade-oﬀ) are highlighted in green. 524.3.2 Dependence of the performance on the stochastic choice of terminals The introduction of the ﬂux ﬂuctuations in the system opens numerous possibilities for the criterion of selecting the sources and sinks in each step. One can consider a deterministic time-dependent distribution of the nodes ﬂux,q, or a purely stochastic one based on some hypothesis, as we have considered in the previous analysis. The choice of a speciﬁc set of sources and sinks in a given step tends to reinforce preferentially the channelsalongtheshortestpathsconnectingthemattheexpenseoftheremainingones. Consequently, the ﬁnal network results from a compromise of averaging out the selected routes in all time steps. Therefore, diﬀerent methods of choosing the driving terminals in each step lead in principle to distinct network topologies, ultimately aﬀecting their ﬁtness. In the context of Physarum, it’s not straightforward to decide what is the more adequate criterion based on the available experimental data, and due to the limitations of the model in describing through simplistic terms the complex and not-well-understood mechanism underlying the network optimisation. We now compare diﬀerent stochastic methods of choosing the sources and sinks in each step and study their impact on the topology and performance of the optimised networks. Five diﬀerent cases were studied. The ﬁrst one considered was the original method proposed by Tero et al. [6] of randomly choosing in each step one source-sink pair from the set of terminals, such thatqsource = −qsink = I0. This method is referred to as the “Random pair” method. From the perspective of a traveller who wants to move from one city (the source) to any other the fastest way possible, the network should be a good compromise of all the shortest paths between the cities. A similar argument can be applied toPhysarum, which seeks to transport the nutrients throughout the network in a fast and eﬃcient way, by establishing multiple short connections between the available food sources, enabling an eﬀective management of the food consumption and distribution. Therefore, it makes sense to consider that at each time step, one terminal is randomly assigned as the source while all the remaining terminals are sinks, receiving an equal amount of ﬂuid, i.e.,qsink = −qsource/(|T|−1) where qsource = I0. This method is designated by “Random source”, and should in principle maximise the transport eﬃciency. However, biologically speaking, there is no speciﬁc argument that sustains the hypothesis of only one pair of food sources being “activated” at a given moment, or one food acting as a source while all the remaining act like sinks. At a given step, all the food sources can be actively pumping nutrients, so any possible source-sink state should be possible. For this reason, we consider the case where the nodes’ net ﬂuxes of all the terminals (qi with i∈T) are time-dependent random variables subject to the constraint ∑ i∈sources qi = − ∑ i∈sinks qi = I0 . (4.5) In this way, at each step, a random combination of sources and sinks is generated. This method is referred to as the “All random” case. We also compare with the case of ﬁxed terminals, where some cities were assigned as sources and the others as sinks from the beginning (“Fixed Terminals” method). Diﬀerent combinations of sources 53and sinks were tested. As seen before, the choice of ﬁxed terminals can lead to disconnected solutions. Thus, to ensure that the ﬁnal network remained connected, in each case, we have considered a ﬁxed random distribution of node ﬂuxes which satisﬁes (4.5). All the above methods were tested considering the parameterisation of the model which minimises the power dissipated, i.e.,γ = 2/3 in (3.45). Finally, we also studied the networks produced by thePhysarum Solvermodel (3.10), with the choice of a sigmoidal response typically used in literature,f(|Qij|) = |Qij|γ/(1 + |Qij|γ), and considering the stochastic choice of the source-sink pair as in the “Random pair” case. This method is designated by “PS - random pair”. In this case, the simulations were performed usingγ = 1.8 and I0 = 2, which according to [6] are the parameters which yielded networks mimicking the Tokyo rail system with the best trade-oﬀ between cost, eﬃciency and fault tolerance. To establish an even comparison, all the remaining methods were also simulated considering the same total inlet ﬂux,I0. Examples of the typical networks produced by the diﬀerent methods are represented in Figure 4.8. As the images show, the diﬀerent choices lead to steady states with distinct topological features. It’s also interesting to note how thinner are the selected channels by thePhysarum Solver model comparing to any other case, which is explained by the adaptation mechanism not conserving the network’s volume in this case. To establish a comparison between the methods, the overall performance of the networks was again quantiﬁed in terms of cost, transport eﬃciency and fault tolerance. The average performance of the diﬀerent methods, based on 10 realisations for each case, is summarised in Table 4.2. A more visual quantiﬁcation is given by the plots of Figure 4.10, depicting the trade-oﬀ between the diﬀerent metrics of all simulations. The choice of ﬁxed terminals is deﬁnitely the method that leads to networks with the worst perfor- mance by far in every respect. In the plots of Figure 4.10, the points corresponding to this method are all scattered, reﬂecting the great variety oftree topologies depending on the speciﬁc arrangement of sources and sinks (Figure 4.9), but in every case, the performance metrics are consistently low. The characteris- tic tree-like topology of the steady states entails a great cost without any beneﬁt in terms of transport eﬃciency, as the terminals are on average very distant from each other, and in terms of tolerance to damage, as no redundant paths are formed (FT= 0). As a result, the average values of the metrics are the lowest ones from all the methods by a large gap, in particular, the cost-beneﬁt ratios. This justiﬁes the importance of the ﬂux ﬂuctuations to build eﬃcient and resilient networks. One could have expected that the “All random” method would lead to the highest diversity of topolo- gies, due to being the most stochastic one. However, the proximity of the points in the plots of Figure 4.10suggests quite the opposite. Actually, this method imposes more restrictions on the topology of the ﬁnal network, as all the states of sources and sinks are possible, meaning that the connections between the terminals must accommodate all these possibilities. However, there is still the possibility that the method used to generate the random distribution of node ﬂuxes in each step might have introduced some bias in the states actually generated. As expected, the “Random source” method is the one that results in the maximum transport eﬃciency on average. However, the “All random” method reaches a slightly better trade-oﬀ between the eﬃciency 54Parameters  = 0.67  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 2  choose_fs = Tero D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = None fs_seed = None D0 seed = None nodes = 1005 edges = 2817 iterations = 3000 Metrics Steady-State  TL = 2017.088 MD = 291.87 FT = 0.081 TL_mesh = 0.064 TL_railway = 1.091 TL_steiner = 1.367 TL_MST = 1.457 TL_CGpy = 0.028 TL_CGmat = 0.03 MD_mesh = 1.234 MD_railway = 0.925 MD_steiner = 0.664 MD_MST = 0.709 MD_CGpy = 1.234 MD_CGmat = 1.287 avg_deg = 2.067 is_connected = 1 n_loops = 7 LD = 0.004 New Model (poly  = 0.67) (a) “Random pair” (2017, 292, 0.92) Parameters  = 0.67  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 2  choose_fs = 1_rand_source D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = None fs_seed = None D0 seed = None nodes = 1005 edges = 2817 iterations = 3000 Metrics Steady-State  TL = 1790.029 MD = 289.792 FT = 0.399 TL_mesh = 0.057 TL_railway = 0.968 TL_steiner = 1.213 TL_MST = 1.293 TL_CGpy = 0.025 TL_CGmat = 0.026 MD_mesh = 1.225 MD_railway = 0.919 MD_steiner = 0.659 MD_MST = 0.704 MD_CGpy = 1.225 MD_CGmat = 1.278 avg_deg = 2.025 is_connected = 1 n_loops = 3 LD = 0.002 New Model (poly  = 0.67) (b) “Random source” (1790, 290, 0.60) Parameters  = 0.67  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 2  choose_fs = all_random D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = None fs_seed = None D0 seed = None nodes = 1005 edges = 2817 iterations = 2500 Metrics Steady-State  TL = 1713.726 MD = 304.026 FT = 0.087 TL_mesh = 0.054 TL_railway = 0.927 TL_steiner = 1.162 TL_MST = 1.237 TL_CGpy = 0.024 TL_CGmat = 0.025 MD_mesh = 1.286 MD_railway = 0.964 MD_steiner = 0.691 MD_MST = 0.738 MD_CGpy = 1.286 MD_CGmat = 1.341 avg_deg = 2.025 is_connected = 1 n_loops = 3 LD = 0.002 New Model (poly  = 0.67) (c) “All random” (1714, 304, 0.91) Parameters  = 0.67  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 2  choose_fs = fixed I0 D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = None fs_seed = None D0 seed = None nodes = 1005 edges = 2817 iterations = 2500 Metrics Steady-State  TL = 3166.413 MD = 390.659 FT = 1.0 TL_mesh = 0.101 TL_railway = 1.713 TL_steiner = 2.147 TL_MST = 2.286 TL_CGpy = 0.045 TL_CGmat = 0.047 MD_mesh = 1.652 MD_railway = 1.239 MD_steiner = 0.889 MD_MST = 0.949 MD_CGpy = 1.652 MD_CGmat = 1.723 avg_deg = 1.993 is_connected = 1 n_loops = 0 LD = 0.0 New Model (poly  = 0.67) (d) “Fixed terminals” (3166, 391, 0.00) Parameters  = 1.80  model = Tero func = sigmoid stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 2  choose_fs = Tero D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = None fs_seed = None D0 seed = None nodes = 1005 edges = 2817 iterations = 2000 Metrics Steady-State  TL = 1643.617 MD = 325.635 FT = 0.084 TL_mesh = 0.052 TL_railway = 0.889 TL_steiner = 1.114 TL_MST = 1.187 TL_CGpy = 0.023 TL_CGmat = 0.024 MD_mesh = 1.377 MD_railway = 1.032 MD_steiner = 0.741 MD_MST = 0.791 MD_CGpy = 1.377 MD_CGmat = 1.436 avg_deg = 2.013 is_connected = 1 n_loops = 2 LD = 0.001 Tero Model (sigmoid  = 1.80) (e) “PS - random pair” (1644, 326, 0.92) Figure 4.8: Topology of the optimised networks considering diﬀerent methods of choosing the sources and sinks in each step. Note that the examples given are not fully representative, since the topology may change signiﬁcantly due the stochastic nature of the algorithm. (a) “Random pair” -Model (3.45) withγ = 2/3, considering the choice of random source-sink pair from the set of terminals in each step. (b) “Random source” -Model (3.45) withγ = 2/3, where in each step a random terminal is chosen as the source, and the remaining terminals are sinks receiving the same amount of ﬂux.(c) “All random” -Model (3.45) withγ = 2/3, where in each step all the terminals are randomly chosen either as sources or sinks. The terminals net ﬂux are random variables subject to the constraint (4.5).(d) “Fixed Terminals” -Model (3.45) withγ = 2/3, considering ﬁxed sources and sinks from the beginning. In this case, the capital Lisbon acts like a source (blue circle) of ﬂux, while all the remaining cities are sinks (blue triangles).(e) “PS - random pair” -Physarum Solvermodel (3.10) with the choice of a sigmoid update function f(|Qij|) = |Qij|1.8/(|Qij|1.8 + 1) typically used in literature andµ = 1. The choice of terminals in each step is the same as “Random pair” method. Note how much thinner are the channels of the ﬁnal network comparing to the remaining cases, due to the total volume not being conserved in this case. In all the cases the total inlet ﬂux isI0 = 2, and the same initial conditions were used,Dij(0) = 1. The legend of each image refers to the network metrics(TL, MD, FT), where TL and MD are given in kilometres. and the cost, achieving the highest BCRTE. By contrast, from all the stochastic methods, “Random source” is the one with the lowest fault tolerance and BCRFT, which is related with the characteristic 55Parameters  = 0.67  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 2  choose_fs = fixed I0 rand D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = None fs_seed = None D0 seed = None nodes = 1005 edges = 2817 iterations = 2500 Metrics Steady-State  TL = 2670.105 MD = 523.165 FT = 1.0 TL_mesh = 0.085 TL_railway = 1.444 TL_steiner = 1.81 TL_MST = 1.928 TL_CGpy = 0.038 TL_CGmat = 0.039 MD_mesh = 2.212 MD_railway = 1.659 MD_steiner = 1.19 MD_MST = 1.27 MD_CGpy = 2.212 MD_CGmat = 2.308 avg_deg = 1.991 is_connected = 1 n_loops = 0 LD = 0.0 New Model (poly  = 0.67) (2670, 523, 0.00) Parameters  = 0.67  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 2  choose_fs = fixed I0 rand D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = None fs_seed = None D0 seed = None nodes = 1005 edges = 2817 iterations = 2000 Metrics Steady-State  TL = 2747.657 MD = 547.704 FT = 1.0 TL_mesh = 0.087 TL_railway = 1.486 TL_steiner = 1.863 TL_MST = 1.984 TL_CGpy = 0.039 TL_CGmat = 0.04 MD_mesh = 2.316 MD_railway = 1.737 MD_steiner = 1.246 MD_MST = 1.33 MD_CGpy = 2.316 MD_CGmat = 2.416 avg_deg = 1.992 is_connected = 1 n_loops = 0 LD = 0.0 New Model (poly  = 0.67)  (2748, 548, 0.00) Parameters  = 0.67  model = New func = poly stop = (  method:on_edges  N:500  Dmin:0.0005  max_frames:7000) max_i = 7000 I0 = 2  choose_fs = fixed I0 rand D0 ij  = 1 dt = 0.02 L_attr = L_hav seed = None fs_seed = None D0 seed = None nodes = 1005 edges = 2817 iterations = 2000 Metrics Steady-State  TL = 2167.263 MD = 458.592 FT = 1.0 TL_mesh = 0.069 TL_railway = 1.172 TL_steiner = 1.469 TL_MST = 1.565 TL_CGpy = 0.031 TL_CGmat = 0.032 MD_mesh = 1.939 MD_railway = 1.454 MD_steiner = 1.043 MD_MST = 1.114 MD_CGpy = 1.939 MD_CGmat = 2.023 avg_deg = 1.99 is_connected = 1 n_loops = 0 LD = 0.0 New Model (poly  = 0.67)  (2167, 459, 0.00) Figure 4.9: Typical networks obtained when considering a static set of sources and sinks, forγ = 2/3. Diﬀerent choices of sources and sinks lead to completely diﬀerent networks, which are alwaystrees. Clearly, these topologies are far from being optimal due to excessive cost of connecting all the terminals and the zero fault tolerance, resulting in low transport eﬃciency. Only the adaptation subject to time- dependent sources and sinks can result in eﬃcient and resilient networks. The legend of each image refers to the network metrics(TL, MD, FT), where TL and MD are given in kilometres. topology of the resulting networks, as seen in Figure 4.8b – the cities closer to the border are connected by tree-like branches to a robust core connecting the interior cities. Although the Physarum Solver has the worst transport eﬃciency from the stochastic methods, it achieves a similar BCRTE comparing to the “All random” and “Random source” cases, only because it produces networks with the lowest total length of all the methods. In compensation, it attains the highest fault tolerance, which together with this low-cost results in the best cost-robustness trade-oﬀ (highest BCRTE). The “All Random”, “Random source” and “PS - random pair” methods produced networks with better metrics than the railway graph (Figure 4.5a). This is almost true for “Random pair” except for the total length, which is considerably higher than that of the railway graph, leading to a slightly lower BCRTE. All the stochastic methods also achieved a better overall performance than the MST graph (Figure 4.5b), although the BCRTE of the latter is slightly higher than the “Random pair” case, only because it has the lowest cost by deﬁnition. The discrepancy is even higher when comparing with the CG (Figure 4.5c). Although the CG reaches the maximum eﬃciency and fault tolerance, it has the highest cost by a very large margin, resulting in the worst cost-beneﬁt ratios. In comparison, the stochastic methods achieve more than70% of the eﬃciency of the CG, for only less than3% of the cost, resulting in at least 25 times better cost-beneﬁt relationship. Note that, from the previous analysis, we can concludeγ = 2/3 is not the value ofγ which results in networks with the best overall performance. The plot of Figure 4.7c suggests thatγ = 0.7 could have been a better choice, since, from all the values ofγ tested, it’s the one which achieves the highest BCRTE and the second-highest BCRFT, reaching the best compromise between the three metrics. This would allow a more reliable and fair comparison with thePhysarum Solver method, whose parameters were cherry- 56picked to achieve the highest performance by extrapolating the results of the Tokyo experiments[6]. In conclusion, the results clearly show the importance of ﬂux ﬂuctuations in the design of low-cost, eﬃcient and robust ﬂow networks [1, 2, 6, 40]. Due to the Pareto nature of the optimisation, the criteria of choosing the best stochastic method to generate the driving terminals depends on the relevance of these network features in a given context. Assuming we want the best overall trade-oﬀ between the three metrics (TL, TE, FT), the results suggest that the “All Random” method is the best one, given that is the method which achieved the highest BCRTE and highest BCRFT from all the tests of our model. (a)  (b) (c)  (d) Figure 4.10: Performance of the networks for diﬀerent methods of choosing the sources and sinks over time, as described in Figure 4.8. (a-c) Plots showing the trade-oﬀ between the cost (total length), transport eﬃciency and fault tolerance of the networks. The metrics are normalised to those of the CG graph. Each type of marker corresponds to a diﬀerent method. The results were compared with the same normalised metrics of the real railway (green triangles) and MST network (red squares)(d) The beneﬁt-cost ratios, deﬁned as BCRTE= TECG/TLCG and BCRFT= FTCG/TLCG, plotted against each other, measuring the overall compromise between the three metrics for the diﬀerent methods of choosing the terminals. The simulations which achieved the best trade-oﬀ between each pair of metrics are highlighted in green. Overall, the stochastic networks are more robust and eﬃcient than any other network, especially comparing to the case of ﬁxed terminals. 57Method TL CG (×10−5) TE CG (×10−3) FT ( ×10−3) BCR TE (×10−1) BCR FT (×10−1) All random 2523 ±3 751 ±3 896 ±5 298 ±1 355 ±2 Random source 2648 ±20 777 ±4 620 ±27 294 ±3 234 ±10 PS - random pair 2471 ±22 718 ±8 909 ±4 291 ±4 368 ±4 Random pair 3086 ±44 768 ±3 960 ±7 249 ±4 312 ±5 Fixed terminals 3645 ±178 449 ±27 0 ±0 125 ±10 0 ±0 MST 2036 551 0 270 0 railway 2718 719 455 264 167 CG 100000 1000 1000 10 10 Table 4.2: The mean and corresponding standard error of the performance metrics for diﬀerent methods of choosing the sources and sinks in each step of the algorithm. The methods are described in Figure 4.8. The metrics are normalised to the corresponding values of the CG (Figure 4.5c). The results are based on 10 runs for each method and are sorted by the beneﬁt-cost ratio of the network’s transport eﬃciency, BCRTE. At the bottom is presented the same metrics of the MST, railway graph and CG for comparison. The stochastic methods produce networks with much better performance than the case of ﬁxed terminals. Overall, the stochastic networks also have a better beneﬁt-cost relationship than the MST, CG and the real railway. 58Chapter 5 Modelling Physarum’s Growth Physarum grows and progressively rearranges its network structure as it forages. So far we have only considered the network optimisation of static organisms, and neglected the growth mechanism. We have developed a generic model describing the adaptation dynamics of a static ﬂow network of distensible channels ﬁlled with an incompressible ﬂuid, and applied to the particular case ofPhysarum. In this chapter, we extend the previous model to accommodate the formation of new channels, connecting the growth to the network optimisation, as an attempt to describe the foraging behaviour ofPhysarum, neglecting its body mobility. A simple stochastic model of cellular growth was proposed by Eden in 1961 [41], which is described as follows. The cells are represented by square lattice sites and growth can only occur at the boundary i.e., from one occupied cell to one adjacent free cell. Initially, only one cell is occupied. At each time step, a random boundary cell is selected to reproduce to a neighbour empty cell with a given probability that may depend on diﬀerent factors, leading to diﬀerent types of cluster formations. Diﬀerent variations of the Eden model applied to the growth ofPhysarum were recently studied by Ferreira and Dilão [26]. In this case, the network development is simulated by tracing the linage that connects the starting cell to a given cell at the boundary. Each edge of the graph connects the father to the daughter cell. In particular, it was considered a ﬂow-based growth version which shares some similarities with the one proposed here. The ﬂow was driven by the starting cell (food source) and the outside perimeter cells which behave like sinks with ﬁxed pressures,p= 0. After computing the channel ﬂuxes, the probability that a sink is occupied in the current step is proportional to the amount of ﬂux received, q. However no coupling between the growth and the adaptation mechanisms was explicitly considered, and the formation of channels is merely probabilistic. Here we propose a simple growth model which solves these two issues. 5.1 Growth Model Toincorporatethegrowthintothepreviousadaptationmodel, Physarum isnowrepresentedbyadynamic graph. The growth is regarded as the formation of new channels at the boundary of the organism when 59there are enough nutrients in the neighbourhood to build them. The nutrients are supplied by active food sources, initially placed at certain nodes, and transported to the nodes at the boundary where they are stored until they are used in the veins’ development. In this way, the boundary nodes behave like sinks of the nutrients ﬂux, mimicking simulated regions where the growth occurs (growth fronts). For simplicity, it’s assumed that the dynamics take place on top of a pre-existing mesh resulting from a Delaunay triangulation, which means that thePhysarum’s network at a given time is a subgraph of the underlying mesh. This implies that all the channels have already a pre-determined orientation and ﬁxed length, Lij, but ensures that the organism grows as a planar graph. The formation of the new channels is simulated by the progressive activation of the edges of the underlying mesh when the cost of producing it is overcome. Each edge of the mesh is thus associated with a nutrient cost of activation, which in principle should depend on its length, and can have two possible states: • Active: the edge represents a vein of thePhysarum’snetwork. • Inactive: the edge is not currently part of the network. Similarly to the previous model, each active channel is considered as a cylindrical elastic tube whose diameter can change in response to the ﬂux ﬂowing through it. They are characterised by a conductivity, Dij, which is subject to the same adaptation dynamics (3.19). If the channel(i,j) is inactive,Dij = 0, otherwise Dij >0. A newly formed channel is initialised with an arbitrary conductivity valueDij(0) = D0 >0. On the other hand, each nodei is characterised by an amount of nutrients,mi, and can have three possible states: • Empty state:inactive nodes that are not yet part of thePhysarum network. • Growing state: nodes located at the growing margins ofPhysarum (boundary) that store tem- porarily nutrients from the sources and participate in channels formation. These nodes are referred to as “boundary nodes”. Active nodes are in the growing state as long as they contain at least one inactive neighbour node. • Transport state:nodes that can’t give rise to new channels anymore, and serve only as interme- diaries to transport nutrients to the boundary, where the formation of new channels can occur. In addition, any given node can have a food source, which is “activated” from the moment that the node is activated i.e., it’s changed to the growing state. 5.1.1 Algorithm The algorithm compromises three main steps which are described as follows. Initially, thePhysarum is represented by a single node containing a food source. 60Computation of the ﬂuxes In each time step, the nutrients ﬂow from the active food sources to the current boundary sinks via the active channels, where they are accumulated. In the simplest case, there is only one food source with magnitude qsource = I0, which supplies an equal amount of nutrients to all the boundary sinks, although other distributions of node ﬂuxes can be considered as well. In any case, the conservation of the mass (3.13) requires that the input ﬂux from the active food sources balances the ﬂux of the boundary sinks. For simplicity, it’s assumed that the food sources never run out. Given the current sets of boundary nodes and of active food sources, the ﬂuxes of the active channels are computed through the conservation laws (3.14), as described in section 3.5.2. Then, the nutrient reserves of the boundary nodes are increased according to the ﬂux that each receives i.e.,|qi|with i ∈boundary. Assuming the ﬂuid has a unitary density, the amount of nutrients that a given node accumulates per time step is dmi = |qi|∆τ ,i ∈boundary (5.1) where ∆τ is the duration of the time step. In the beginning, whilePhysarum is a single node covering a food source and no channels are yet formed, only the last step is applied. Growth stage The next step is the network’s growth. For each boundary node, it’s computed the set of the incident edges which are currently inactive, i.e., edges where new channels can be formed. The nutrient reserves of the node are equally distributed among those neighbour inactive edges where they are accumulated, mimicking the formation of the channel tips. In practice, the production cost of each inactive edge is reduced by the amount of nutrients given. If the nutrients transferred to an edge exceeds its production cost, the excess is kept stored on the boundary node. When the cost of producing the channel reaches zero, the edge is activated, and a new channel is created with a conductivityDij(0) = D0. If the other end node of the edge is inactive (empty state), the node is activated on the “growing state”, and the boundary is extended with the new sink node. Note that every time a new channel with lengthLij is formed, the total network’s volume,V, is increased by√D0Lij. It’s also assumed that the cost of producing a channel is proportional to its initial volume. Finally, for every boundary node, it’s checked if they contain any inactive neighbour node. In case they don’t, the node state is changed to the “transport state”, and growth can no longer occur starting from that node. The eventual nutrient reserves of the node are evenly distributed between the neighbours on the “growing state” which give continuity to the network development. Network adaptation Finally, the conductivities of the active channels are adapted according to the dynamics (3.19) for a given choice of the function g. In the following we consider the polynomial choice used previously, gγ(|Qij|) = |Qij|γ. Note that, although the total volume of the network,V, in (3.19) increases over time 61due to the continuous network growth, it’s still conserved in the adaptation process. 5.2 Results 5.2.1 One food source We start by analysing the simplest case of the individual growing from a food source, without any more food sources available in the surroundings. In each step, the food source with intensityqsource = I0 supplies an equal amount of nutrients to all the current boundary sinks. In Figure 5.1 it’s shown snapshots of a simulation for the choice of parametersI0 = 0.1 and γ = 2/3, and considering that channels are formed with a conductivityD0 = 0.1. t= 300 iterations = 290  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1  t= 900 iterations = 890  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1  t= 1500 iterations = 1490  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1 t= 2100 iterations = 2090  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1  t= 2700 iterations = 2690  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1  t= 3900 iterations = 3890  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1 Figure 5.1: Physarum growing from a food source considering the adaptation dynamics (3.45) with γ = 2/3. The growth is driven by a central food source (yellow) which pumps nutrients to the moving boundary (red) where there is a constant uptake of nutrients to create new channels (with an initial conductivity D0). In each time step, an amount ofI0∆τ nutrients given by the source is distributed evenly between the boundary sinks. AsPhysarum grows, the channels adapt their thickness according to the ﬂux of nutrients ﬂowing through. Simulation carried out withD0 = 0.1, ∆τ = 0.02, I0 = 0.1. The dynamics result in a tree-like network that resembles the networks produced by the real organism (Figure 2.2), although no stable cross-links between the main veins are formed. The labelst designate the time step in which the snapshots were taken. As the images show, the growth occurs in a more or less isotropic fashion and the growth fronts are circular. Furthermore, the growth slows down as time passes, since the boundary is progressively extended, and thus less food is supplied per time step to each boundary sink. These two features of the growth mechanism are biologically consistent. Note that the former is related to the underlying irregular mesh used, resulting from a Delaunay triangulation. Other types of meshes were tested as well, namely 62square, triangular and hexagonal lattice graphs. However, even when adding a small noise to the node positions to break the lattice symmetries, simulations have shown that the growth fronts weren’t circular due to the regularity of the underlying graph, meaning the lattice symmetries were still quite noticeable. The dynamics resulted in tree-like networks which share some similarities with the networks produced by the real organisms (Figure 2.2). In particular, as the network grows the inner channels are highly optimised, leading to the formation of distinct fan-shaped growth fronts with a more dense branching as we move away from the source. This is especially noticeable as time increases. However, an important feature is still missing, the interconnections between the main veins which result in a loopy architecture that provides robustness to the network. In the simulations forγ = 2/3, these redundant connections are not stable and ultimately disappear. By contrast, similar to the static cases, forγ <1/2 the networks end up being highly redundant, as Figure 5.2 shows. However, they are poorly optimised and there isn’t a clear hierarchy of veins as observed in the realPhysarum networks, where the thicker main veins branch into progressively thinner ones. t= 300 iterations = 290  Parameters       = 0.45  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1  t= 900 iterations = 890  Parameters       = 0.45  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1  t= 1500 iterations = 1490  Parameters       = 0.45  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1 t= 2100 iterations = 2090  Parameters       = 0.45  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1  t= 2700 iterations = 2690  Parameters       = 0.45  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1  t= 3900 iterations = 3890  Parameters       = 0.45  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1 Figure 5.2: Physarum growing from a food source considering the adaptation dynamics (3.45) with γ = 0.45. Same settings as in Figure 5.1. Forγ <1/2 the dynamics result in highly redundant networks very close to the underlying mesh. Despite the redundancy, due to the lack of optimisation and a clear hierarchy of vein thickness, the networks don’t resemble the ones produced by the real organisms. The labels t designate the time step in which the snapshots were taken. Previous results of the static optimisation revealed that ﬂuctuations of the nodes ﬂuxes have a great impact on the topology of the ﬁnal networks, and thus could be at the origin of the stable redundant paths. Until now, we assumed a radial growth driven by a central source and uniform boundary sinks. As discussed previously, the adaptation tends to reinforce the channels along the shortest paths connecting 63the sources and sinks, which in this case correspond to the connections along the radial direction, as the ﬂux develops preferentially in that direction. Therefore the angular connections end up being the ﬁrst ones to disappear. As an attempt to prevent this behaviour, we considered the hypothesis of each boundary sink receiving a random fraction of the source ﬂux in each time step. In principle, this would create perturbations in the direction of the ﬂow, which could lead to the formation of stable angular connections between the main branches. However, as the Figure 5.3 shows, the network evolution is very similar to the case of the sinks receiving uniformly (Figure 5.1), and redundant paths are still not formed. We conclude that the hypothesis of random sinks is still not suﬃcient to explain the loops observed in the realPhysarum networks. t= 300 iterations = 290  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one rand tips e cost = V cost f = 1  t= 900 iterations = 890  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one rand tips e cost = V cost f = 1  t= 1500 iterations = 1490  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one rand tips e cost = V cost f = 1 t= 2100 iterations = 2090  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one rand tips e cost = V cost f = 1  t= 2700 iterations = 2690  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one rand tips e cost = V cost f = 1  t= 3900 iterations = 3890  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one rand tips e cost = V cost f = 1 Figure 5.3: Physarum growing from a food source considering the adaptation dynamics (3.45) with γ = 2 /3 and the sinks receiving a random amount of ﬂux over time. Same settings as in Figure 5.1, except that in each time step, the amount of nutrients given by the source is distributed randomly between the boundary sinks. The network evolution is nearly identical to the case of the nodes receiving uniformly, and therefore a random distribution of nutrients can’t explain the formation of stable loops observed inPhysarum networks. 5.2.2 Multiple food sources Finally, we simulated the case of thePhysarum accommodating new food sources as it grows, which is a better representation of its foraging behaviour. However, it’s not clear how real organisms manage food consumption when they acquire multiple food sources. We studied two diﬀerent methods considering the growth in the presence of two food sources. In both cases, the nutrients supplied by the active food sources are evenly shared among the boundary sinks, and the simulations were performed withγ = 2/3, I0 = 0.1, D0 = 0.1 and ∆τ = 0.02. 64In the ﬁrst case, we considered that as soon as the second food source was reached, both food sources were constantly operational and injected the same amount of nutrients into the network. This means that from that moment, the amount of ﬂux received by each boundary node was doubled. Snapshots of the simulation are depicted in Figure 5.4. As the images show, after the second food source is accommodated, the short connections between the two are weakened and ultimately collapse. We conclude that the synchronous and continuous operation of both food sources leads to their repulsion. This is the opposite ofthetrue Physarum’sbehaviour, whichtendstoconnectthefoodsourcesthroughshortpathstooptimise the transport of nutrients and minimise the costs (Figure 5.5). t= 300 iterations = 290  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = all e cost = V cost f = 1  t= 600 iterations = 590  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = all e cost = V cost f = 1  t= 900 iterations = 890  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = all e cost = V cost f = 1 t= 1200 iterations = 1190  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = all e cost = V cost f = 1  t= 1500 iterations = 1490  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = all e cost = V cost f = 1  t= 1800 iterations = 1790  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = all e cost = V cost f = 1 Figure 5.4: Physarum growing in the presence of two food sources with constant input ﬂux. Same parameters as in Figure 5.1. Starting from the left food source,Physarum accommodates a second food source (blue circle) as it forages. From that moment, both sources are always active, each injecting a constantI0∆τ amount of nutrients per time step, which is evenly distributed between the boundary sinks (red circles). Yellow circles represent active food sources in a given moment. The synchronous activation of both food sources leads to their “repulsion”, meaning that no direct connection is formed between them. This is unrealistic in the context ofPhysarum. In the second case, we considered that after the second food source was activated, only one of them was operational at a time. In each time step, one of the food sources was randomly selected to supply the nutrients to the boundary nodes. The results can be found in Figure 5.6. The asynchronous operation of the food sources generates ﬂow reversals which are a better approximation of the shuttle streaming behaviour of Physarum. Conversely to the previous case and similar to what is observed in the real organism, this mechanism results in the formation of short connections between the food sources (Figure 5.5). 65Figure 5.5: RealPhysarum growing in a presence of two food sources (agar blocks). A direct connection is established between them. Adapted from [42]. t= 300 iterations = 290  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1  t= 600 iterations = 590  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1  t= 1200 iterations = 1190  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1 t= 1500 iterations = 1490  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1  t= 2100 iterations = 2090  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1  t= 2700 iterations = 2690  Parameters       = 0.67  L = 35 mesh = d noise = 0.5 seed = 42 I0 = 0.1  D0 ij  = 0.1 dt = 0.02 thick = 3 choose_fs   = one e cost = V cost f = 1 Figure 5.6: Physarum growing in the presence of two food sources supplying nutrients alternately. Same settings as in Figure 5.4, but once the second food source is covered, at a given time step only one of them is randomly selected to supply the nutrients to the boundary sinks (red circles). The yellow circles represent the activated food source in a given instant, while the blue circles represent the inactive one. This asynchronous operation of the food sources allows the formation of a short direct connection between them, similarly to what is observed inPhysarum (Figure 5.5). 5.2.3 Discussion The results for one food source showed that the simple growth-optimisation model introduced cannot fully reproduce the observed network topology ofPhysarum. Besides the distribution of node ﬂuxes, this could be also related to the relative timescales between the growth and optimisation mechanisms, which are controlled by the channels’ production cost, the choice of the update function,g, and the time increment ∆τ. The model should be carefully parametrised so that an eventual equilibrium between the processes could be achieved that would allow the persistence of the redundant connections. 66Regarding the simulations of the growth in the presence of two food sources, the results suggest that the asynchronous operation of the food sources is far more biologically reasonable than the synchronous pumping of both. Further improvements to the model can be also made. One can consider a more realistic distribution of the nodes ﬂuxes, instead of an even distribution or a purely stochastic as considered here. For instance, the sinks can receive a fraction of nutrients proportional to the distance they are from the sources, as the farther away a region is from the source, the longer the nutrient will take to reach it. Alternatively, a more realistic approximation to the shuttle streaming might be achieved if the sinks (sources) receive (give) unequal amounts of nutrients which vary periodically in time. In addition, some interior nodes could be also treated as sinks, as those regions also require nutrients. The cost of producing a channel could also include a second term describing the metabolic costs inherent to the channels formation. Finally, it would be also interesting to test the dynamics with ﬁnite food sources which eventually run out. 6768Chapter 6 Concluding remarks 6.1 Achievements In this work, we studied the network development and adaptation ofPhysarum polycephalum. It was ﬁrst analysed the adaptation of static organisms, i.e., neglecting the network growth. We have derived a general class of equations describing the adaptation dynamics of a network comprised of elastic channels ﬁlled with an incompressible ﬂuid subject to a Hagen-Poiseuille ﬂow. The adaptation mechanism models the evolution of the channels’ radii as a response to the ﬂow ﬂowing through them at a given moment, which is described by an arbitrary functiong. This adaptation dynamics was inspired by the previous model proposed by Teroet al. [6], but is more physically realistic, as it provides a correct description of a Hagen-Poiseuille ﬂow by ensuring the conservation of the volume of the ﬂuid over time. Following a Lagrangian approach, we proved that the particular choice ofg(|Qij|) = |Qij|2/3 minimises the total power dissipated by the network at the steady-state assuming a ﬁxed set of sources and sinks. Considering these choices, it was shown that the ﬁnal networks weretreesconnecting the set of terminals, whose geometry is highly dependent on the speciﬁc distribution of the nodes’ ﬂux. In particular, some conﬁgurations led to apparently disconnected solutions which are unrealistic in the context ofPhysarum. It was also observed that the same conﬁguration of terminals could result in diﬀerent steady-states depending on the initial conductivities. The previous choice of the functiong led us to study the adaptation dynamics of the general class of polynomial functions,gγ(|Qij|) = |Qij|γ, as a function of the parameterγ. This analysis revealed the existence of a ﬁrst-order phase transition in the system nearγ = 1/2, marked by the discontinuity of the derivative of dissipated power at the steady state with respect toγ and a drastic change in the network topology. Some applications of the model were also tested. For graphs describing mazes, with a source and a sink placed at both entrances, it was shown that the adaptation dynamics converged to a single path connecting them, and therefore can reproduce the maze solving abilities ofPhysarum [5]. However, for a general planar graph, the system doesn’t always converge to the shortest path connecting the two terminals. 69Simulations have shown that typicaltree topologies resulting from the adaptation under ﬁxed sources and sinks are not optimal, as they entail high costs to connect all the terminals and no redundancy, leading to a low transport eﬃciency. By contrast, the networks displayed byPhysarum show a good compromise between these metrics, and thus can’t be mimicked by constant distribution of nodes ﬂuxes, which naturally can’t account for the periodical ﬂow reversals (shuttle streaming). This justiﬁed the need of considering the adaptation subject to time-dependent sources and sinks to produce eﬃcient and resilient networks, and as a better approximation toPhysarum’s shuttle streaming behaviour. The inﬂuence of ﬂux ﬂuctuations on the network ﬁtness was studied using a conﬁguration of terminals that mimicked the geographical locations of major Portuguese cities. Since it’s not clear what is the most appropriate choice, diﬀerent stochastic methods of selecting the driving terminals in each time step were considered. The introduction of ﬂux ﬂuctuations enabled the formation of redundant connections which provided robustness to the network. As expected, compared to the case of ﬁxed terminals, the stochastic methods achieved a far better performance in every respect. In general, all the stochastic cases also achieved a better overall performance than the MST and the real Portuguese railway. The results also suggest that the choice of a completely random distribution (“All random”) might be a better alternative to the original method proposed by Tero et al. [6] of choosing a source-sink pair (“Random pair”), as it achieved the best overall cost-beneﬁt relationship. Regarding the dependence onγ, it was observed that the network resolves towards the MST solution and progressively loses all its redundancy asγ is increased. For values ofγ ∈[0.7,1[, the simulations resulted in networks with better overall performance than the MST and the real railway. A more rigorous analysis should be performed to support these results. In particular other values ofgamma and stochastic distributions can be considered. Lastly, we extended the adaptation model to incorporate the network growth, coupling both processes and providing a better description ofPhysarum’s foraging behaviour. The growth is driven by a nutrient ﬂux from the food sources to the boundary of the organism where the nutrients are accumulated and later used in the formation of new channels. Considering only one source, we observed that this simple mechanism could mimic to some degree the natural growth ofPhysarum, in particular the formation of the fan-shaped fronts. We have also considered the accommodation of new food sources as the organism grows. Since it’s not clear how food consumption and distribution is managed in this case, we tested two hypotheses. We concluded that the synchronous operation of the food sources couldn’t explain the observed behaviour, as the network optimisation leads to their repulsion, and no short connection between them remains. Conversely, when the food sources operate alternately, a strong direct connection between the two is established, similarly to what is observed inPhysarum. This suggests that the second method is a closer approximation to the real mechanism. The general form of the adaptation dynamics enables the introduction of diﬀerent choices for the function g, depending on the speciﬁc application, allowing the simulation of other physical and biological network systems such as fungi networks. It should be noted that the polynomialgadopted here may lead to high conductivity values which are only constrained by the network’s ﬂuid volume. In this regard, it would be interesting to test other adaptation functions that saturate for large ﬂux values [6, 43]. In the context of graph theory, the model can be applied to several network optimisation problems [8, 9] and 70as a guide network construction in diﬀerent domains. Some benchmarks should be carried against the Physarum Solver to assess which is the fastest algorithm. 6.2 Future work The model proposed here couldn’t fully mimic the characteristic loopy structure ofPhysarum networks, although diﬀerent hypotheses of its origin were tested. In particular, when growth is considered, the dynamics doesn’t result in the formation of stable traversal interconnections of the main veins. Never- theless, the resultant networks share some similarities with the real ones and with other network systems found in nature, such as tumour vascularisation and leaf venation. As an attempt to replicate the loopy patterns, more realistic time- and space-dependent distributions of terminals and other choices of adap- tation functions can be considered. The functional form of the latter should be derived from theoretical expectations of the ﬂow dynamics and the model parameters should be calibrated based on available experimental data. The contractile activity should also play a prominent role on the channels selection, which isn’t explicitly accounted for in the model. The incorporation of the contractions would provide a more realistic description of the shuttle streaming and ultimately explain the self-organisation of the ﬂows [3], without relying on an arbitrary distribution of terminals as the driving mechanism which the model assumes. This self-organised pulsating behaviour may be the key for the observed network structure. Hypothetically, this could be achieved by introducing the transport of signalling molecules released in stimulated regions that would drive the ﬂow and the channels’ adaptation based on a similar feedback dynamics [17]. As an improvement to the growth mechanism, one can also consider a reaction-diﬀusion process at the boundary of the organism to describe the vessel development. The interplay between the nutrients sup- plied by the food sources and chemical regulators released at the boundary would provide a more realistic growth mechanism from where the fan-shaped fronts could naturally emerge. Finally, the introduction of chemical agents into the model would also open doors to model the migration of the plasmodium which was not here addressed. The mobility should be mediated and directed by chemical stimuli [44] in order to explain the chemotatic behaviour ofPhysarum. Fully modelling Physarum remains a challenging and exciting task, due to the complexity of its behaviour and lack of understanding of the underlying mechanism. The search for a uniﬁed model that can reproduce all the network features and explain the intelligent behaviours of this fascinating brainless organism is still ongoing. 7172Bibliography [1] Eleni Katifori, Gergely J. Szöllősi, and Marcelo O. Magnasco. Damage and ﬂuctuations induce loops in optimal transport networks.Phys. Rev. Lett., 104:048704, Jan 2010. [2] Francis Corson. Fluctuations and redundancy in optimal transport networks. Phys. Rev. Lett., 104:048703, Jan 2010. [3] K. Alim, G. Amselem, F. Peaudecerf, M. P. Brenner, and A. Pringle. Random network peristalsis in Physarum polycephalum organizes ﬂuid ﬂows across an individual.Proceedings of the National Academy of Sciences, 110(33):13306–13311, 2013. [4] Vladimir A Teplov. Role of mechanics in the appearance of oscillatory instability and standing waves of the mechanochemical activity in the Physarum polycephalum plasmodium.Journal of Physics D: Applied Physics, 50(21):213002, May 2017. [5] Toshiyuki Nakagaki, Hiroyasu Yamada, and Ágota Tóth. Maze-solving by an amoeboid organism. Nature, 407(6803):470–470, 2000. [6] Atsushi Tero, Seiji Takagi, Tetsu Saigusa, Kentaro Ito, Dan P Bebber, Mark D Fricker, Kenji Yumiki, Ryo Kobayashi, and Toshiyuki Nakagaki. Rules for Biologically Inspired Adaptive Network Design. Science, 327(5964):439–442, 2010. [7] AtsushiTero, RyoKobayashi, andToshiyukiNakagaki. Amathematicalmodelforadaptivetransport network in path ﬁnding by true slime mold.Journal of Theoretical Biology, 244(4):553–564, 2007. [8] Yahui Sun. Physarum-inspired network optimization: A review.ArXiv, 2017. [9] Chao Gao, Chen Liu, Daniel Schenz, Xuelong Li, and Zili Zhang. Does being multi-headed make you better at solving problems ? A survey of Physarum -based models and computations.Physics of Life Reviews, 29:1–26, 2019. [10] Andrew Adamatzky.Advances in Physarum Machines, volume 21. Springer, Cham, 2016. [11] Jeﬀ Jones. Characteristics of Pattern Formation and Evolution in Approximations of Physarum Transport Networks.Artiﬁcial Life, 16:127–153, 2010. [12] Christina Oettmeier, Klaudia Brix, and Hans-Günther Döbereiner. Physarum polycephalum – a new take on a classic model system.Journal of Physics D: Applied Physics, 50(41):413001, Sep 2017. 73[13] OpenStax College Microbiology.The Eukaryotes of Microbiology, chapter 5, pages 195–240. Open- Stax CNX, 2014. [14] Marc Durand. Marc Durand’s Lab, Disordered and Biological Soft Matter, Department of Physics, University Paris Diderot.https://www.marcdurand.net/, 2021. Accessed in July of 2021. [15] T. Ueda, M. Muratsugu, K. Kurihara, and Y. Kobatake. Chemotaxis in Physarum polycephalum. Ef- fects of chemicals on isometric tension of the plasmodial strand in relation to chemotactic movement. Experimental Cell Research, 100(2):337–344, 1976. [16] G F Oster and G M Odell. Mechanics of Cytogels I : Oscillations in Physarum. 503:469–503, 1984. [17] Karen Alim, Natalie Andrew, Anne Pringle, and Michael P. Brenner. Mechanism of signal propa- gation in Physarum polycephalum.Proceedings of the National Academy of Sciences, 114(20):5136– 5141, 2017. [18] Andrew Adamatzky and Jeﬀ Jones. Road planning with slime mould: If physarum built motorways it would route m6/m74 through newcastle.International Journal of Bifurcation and Chaos, 20, 12 2009. [19] Andrew Adamatzky. Route 20, autobahn 7, and slime mold: Approximating the longest roads in usa and germany with slime mold on 3-d terrains.Cybernetics, IEEE Transactions on, 44:126–136, 01 2014. [20] Yukio-Pegio Gunji, Tomohiro Shirakawa, Takayuki Niizato, and Taichi Haruna. Minimal model of a cell connecting amoebic motion and adaptive transport networks.Journal of Theoretical Biology, 253(4):659–667, 2008. [21] Michael Haupt and Marcus J B Hauser. Eﬀective mixing due to oscillatory laminar ﬂow in tubular networks of plasmodial slime moulds.New Journal of Physics, 22(5):53007, May 2020. [22] Nobur&ocirc; Kamiya. The Rate of the Protoplasmic Flow in the Myxomycete Plasmodium. I. CYTOLOGIA, 15(3-4):183–193, 1950. [23] Alexander V Bykov, Alexander V Priezzhev, Janne Lauri, and Risto Myllylä. Doppler OCT imaging of cytoplasm shuttle ﬂow in Physarum polycephalum.Journal of Biophotonics, 2(8-9):540–547, 2009. [24] Derivation of Hagen-Poiseuille equation for pipe ﬂows with fric- tion. https://www.tec-science.com/mechanics/gases-and-liquids/ hagen-poiseuille-equation-for-pipe-flows-with-friction/#mjx-eqn-dv . Accessed: June of 2021. [25] Eleni Katifori. Biological ﬂow networks: The absolute basics.https://cpb-us-w2.wpmucdn.com/ web.sas.upenn.edu/dist/6/217/files/2019/07/BoulderFlowsPart1v1-1.pdf, 2019. Accessed: June of 2021. [26] João Ferreira. Pattern formation during the growth of physarum. Master thesis, IST, 2017. 74[27] Atsuko Takamatsu, Takuma Gomi, Tatsuya Endo, Tomo Hirai, and Takato Sasaki. Energy-saving with low dimensional network in physarum plasmodium.Journal of Physics D: Applied Physics, 50, 02 2017. [28] Junjie Jiang, Xingang Wang, and Ying-Cheng Lai. Optimizing biologically inspired transport net- works by control.Phys. Rev. E, 100:032309, Sep 2019. [29] Steﬀen Bohn and Marcelo O. Magnasco. Structure, scaling, and phase transition in the optimal transport network.Phys. Rev. Lett., 98:088702, Feb 2007. [30] Jayanth R Banavar, Francesca Colaiori, Alessandro Flammini, Amos Maritan, and Andrea Rinaldo. Scaling, Optimality, and Landscape Evolution.Journal of Statistical Physics, 104(1):1–48, 2001. [31] Dan Hu and David Cai. Adaptation and optimization of biological transport networks.Phys. Rev. Lett., 111:138701, Sep 2013. [32] Jan Haskovec, Lisa Maria Kreusser, and Peter Markowich. Ode-and pde-based modeling of biological transportation networks.Communications in Mathematical Sciences, 17(5):1235–1256, Dec 2019. [33] R. P. Feynman, R. B. Leighton, and M. Sands.The Feynman lectures on physics, volume 2, chap- ter 19. 1975. \"Note added after the lecture\". [34] Chris Godsil and Gordon Royle. The Laplacian of a Graph, pages 279–306. Springer New York, New York, NY, 2001. [35] Tomoyuki Miyaji and Isamu Ohnishi. Physarum can solve the shortest path problem on riemannian surface mathematically rigorously.International Journal of Pure and Applied Mathematics, 47, 01 2008. [36] Vincenzo Bonifaci. Physarum can compute shortest paths: A short proof.Information Processing Letters, 113(1):4–7, 2013. [37] Portugal’s railway system map.https://www.infraestruturasdeportugal.pt/pt-pt/rede. Ac- cessed: June of 2021. [38] Rui Dilão, Daniele Muraro, Miguel Nicolau, and Marc Schoenauer. Validation of a Morphogenesis Model of Drosophila Early Development by a Multi-objective Evolutionary Optimization Algorithm. In Clara Pizzuti, Marylyn D Ritchie, and Mario Giacobini, editors, Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics, pages 176–190, Berlin, Heidelberg, 2009. Springer Berlin Heidelberg. [39] Daniele Muraro and Rui Dilao. A parallel multi-objective optimization algorithm for the calibration of mathematical models.Swarm and Evolutionary Computation, 8, 2013. [40] Henrik Ronellenﬁtsch and Eleni Katifori. Phenotypes of vascular ﬂow networks.Phys. Rev. Lett., 123:248101, Dec 2019. 75[41] Murray Eden. A two-dimensional growth process. InProceedings of Fourth Berkeley Symposium on Mathematics, Statistics, and Probability, pages 223–239. University of California Press, 1960. [42] Subash K Ray, Gabriele Valentini, Purva Shah, Abid Haque, Chris R Reid, Gregory F Weber, and Si- mon Garnier. Information Transfer During Food Choice in the Slime Mold Physarum polycephalum. Frontiers in Ecology and Evolution, 7:67, 2019. [43] S I Rubinow and Joseph B Keller. Flow of a viscous ﬂuid through an elastic tube with applications to blood ﬂow.Journal of Theoretical Biology, 35(2):299–313, 1972. [44] Soﬁa Almeida and Rui Dilão. Directional sensing and streaming in dictyostelium aggregation.Phys- ical Review E, 93:052402, May 2016. 76",
      "references": [
        "Damage and fluctuations induce loops in optimal transport networks.",
        "Fluctuations and redundancy in optimal transport networks.",
        "Random network peristalsis in Physarum polycephalum organizes fluid flows across an individual.",
        "Role of mechanics in the appearance of oscillatory instability and standing waves of the Physarum polycephalum plasmodium.",
        "Maze-solving by an amoeboid organism.",
        "Rules for Biologically Inspired Adaptive Network Design.",
        "A mathematical model for adaptive transport network in path finding by true slime mold.",
        "Physarum-inspired network optimization: A review.",
        "Does being multi-headed make you better at solving problems ? A survey of Physarum -based models and computations.",
        "Advances in Physarum Machines, volume 21.",
        "Characteristics of Pattern Formation and Evolution in Approximations of Physarum Transport Networks.",
        "Physarum polycephalum – a new take on a classic model system.",
        "The Eukaryotes of Microbiology, chapter 5, pages 195–240.",
        "Chemotaxis in Physarum polycephalum. Effects of chemicals on isometric tension of the plasmodial strand in relation to chemotactic movement.",
        "Mechanics of Cytogels I : Oscillations in Physarum.",
        "Mechanism of signal propagation in Physarum polycephalum.",
        "Road planning with slime mould: If physarum built motorways it would route m6/m74 through newcastle.",
        "Route 20, autobahn 7, and slime mold: Approximating the longest roads in usa and germany with slime mold on 3-d terrains.",
        "Minimal model of a cell connecting amoebic motion and adaptive transport networks.",
        "Effective mixing due to oscillatory laminar flow in tubular networks of plasmodial slime moulds.",
        "The Rate of the Protoplasmic Flow in the Myxomycete Plasmodium. I.",
        "Doppler OCT imaging of cytoplasm shuttle flow in Physarum polycephalum.",
        "Pattern formation during the growth of physarum.",
        "Energy-saving with low dimensional network in physarum plasmodium.",
        "Optimizing biologically inspired transport networks by control.",
        "Structure, scaling, and phase transition in the optimal transport network.",
        "Scaling, Optimality, and Landscape Evolution.",
        "Adaptation and optimization of biological transport networks.",
        "Ode-and pde-based modeling of biological transportation networks.",
        "The Feynman lectures on physics, volume 2, chapter 19.",
        "The Laplacian of a Graph, pages 279–306.",
        "Physarum can solve the shortest path problem on riemannian surface mathematically rigorously.",
        "Physarum can compute shortest paths: A short proof.",
        "Validation of a Morphogenesis Model of Drosophila Early Development by a Multi-objective Evolutionary Optimization Algorithm.",
        "A parallel multi-objective optimization algorithm for the calibration of mathematical models.",
        "Phenotypes of vascular flow networks.",
        "A two-dimensional growth process.",
        "Information Transfer During Food Choice in the Slime Mold Physarum polycephalum.",
        "Flow of a viscous fluid through an elastic tube with applications to blood flow.",
        "Directional sensing and streaming in dictyostelium aggregation."
      ],
      "meta_data": {
        "arxiv_id": "2305.12244v1",
        "authors": [
          "Rodrigo Almeida",
          "Rui Dilão"
        ],
        "published_date": "2023-05-20T17:29:32Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Developed a flow-based model for adaptive network formation in Physarum polycephalum that addresses inconsistencies of previous models, particularly ensuring the conservation of fluid volume under Hagen-Poiseuille flow. Derived a general class of adaptation equations and identified a specific form by minimizing total power dissipation, revealing a first-order phase transition in network topology near a parameter value of gamma = 1/2. Demonstrated the model's ability to solve mazes and design efficient and resilient transport networks (e.g., approximating the Portuguese railway system) when considering flux fluctuations. Extended the model to incorporate network growth, showing it can replicate fan-shaped growth fronts and the formation of direct connections between alternately operating food sources, similar to observations in real Physarum.",
        "methodology": "The model represents Physarum as an undirected, planar, connected graph of elastic channels with fixed lengths and variable radii. Fluid flow is assumed to be incompressible and follow Hagen-Poiseuille law, with channel conductivities (D_ij) adapting based on a derived evolution law that intrinsically conserves the total network volume. A specific form of the adaptation function, g(|Q_ij|) = |Q_ij|^(2/3), was derived by minimizing the total power dissipated by the network. The system's temporal evolution is simulated using an explicit Euler numerical scheme. Network growth is incorporated by progressively activating inactive edges of an underlying mesh at the organism's boundary, with new channels initialized with a specific conductivity D0. Fluxes are driven by fixed or time-dependent (stochastic) sources and sinks, mimicking nutrient supply and uptake.",
        "experimental_setup": "Initial network geometries were generated using Delaunay triangulations of randomly placed nodes in a square lattice or a circular disk, and also a mesh approximating mainland Portugal. For maze-solving, a graph similar to Nakagaki et al.'s maze was used. For shortest path finding, planar graphs with 300 nodes were tested with sources/sinks at diagonally opposite corners over 150 realisations. For the Portuguese rail system approximation, 25 terminals represented major cities, with edge lengths based on geodesic distances (Haversine formula). Network performance was evaluated using Total Length (TL), Transport Efficiency (TE), and Fault Tolerance (FT), normalized against Minimum Spanning Tree (MST) and Complete Graph (CG) benchmarks. Stochastic methods for selecting sources and sinks included 'Random pair', 'Random source', and 'All random'. Growth simulations involved one or two food sources with synchronous or alternate nutrient supply. Simulations were implemented in Python using NetworkX, SciPy, NumPy, Matplotlib, and Seaborn.",
        "limitations": "The model, particularly for gamma > 1/2, typically results in tree-like networks, lacking the stable loopy and reticulated hierarchical structure observed in real Physarum, which is partly attributed to the assumption of fixed sources and sinks. The multi-agent model (Jones) and previous flow-based models (Tero et al.) were noted for biological unrealisticness (autonomous agents, biased stimuli) or physical inconsistency (violating volume conservation). The model’s steady-state geometry can be highly sensitive to initial channel conductivities, indicating multiple possible steady states. Synchronous operation of multiple food sources in the growth model led to their 'repulsion', which is unrealistic. The stopping criterion for stochastic simulations can be sensitive to parameters, and the model does not explicitly account for Physarum's contractile activity or body mobility.",
        "future_research_directions": "Future work includes exploring more realistic time- and space-dependent distributions of terminals and alternative adaptation functions to replicate loopy network patterns. The functional form of the adaptation function (g) should be derived from theoretical flow dynamics and calibrated with experimental data. Incorporating contractile activity is suggested to provide a more realistic description of shuttle streaming and self-organization of flows, possibly through the introduction of signaling molecules. Improvements to the growth mechanism could involve reaction-diffusion processes at the organism's boundary. Modeling the migration of the plasmodium by introducing chemical agents is also proposed. Testing adaptation functions that saturate for large flux values, applying the model to other biological systems (e.g., fungi networks), and benchmarking its speed against other algorithms like Physarum Solver are also suggested avenues. More realistic nutrient distribution, metabolic costs for channel formation, and finite food sources could also be explored.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Deductive Verification of Chain-of-Thought Reasoning",
      "full_text": "arXiv:2404.18512v2  [cond-mat.mes-hall]  8 Jun 2025 Floquet Amorphous Topological Orders in a One-dimensional Rydberg Glass Peng He, 1, 2, ∗ Jing-Xin Liu, 3 Hong Wu,4, † and Z. D. Wang 1, 2, ‡ 1Department of Physics and HK Institute of Quantum Science & Technology, The University of Hong Kong, Pokfulam Road, Hong Kong, China 2Hong Kong Branch for Quantum Science Center of Guangdong-Hong Kong-Macau Great Bay Area, Shenzhen, China 3National Laboratory of Solid State Microstructures, School of Physics, and Collaborative Innovation Center of Advanced Microstructures, Nanjing University, Nanjing 210093, China 4School of Science, and Institute for Advanced Sciences, Chongqing University of Posts and Telecommunications, Chongqing 400065, China (Dated: June 10, 2025) Abstract The topological orders in amorphous systems that lack crystalline symmetry have gained considerable attention recently. Here we propose the Floquet amorphous topological matter, among which the topological orders are explored in experimentally accessible one-dimensional array of randomly pointed Rydberg atoms with periodic driving. The topological properties are comprehen- sively characterized, considering both the single-particle and many-body perspectives. It is found that the periodic driving leads to rich topological phases of matter. At the single-particle level, we evaluate the real space winding numbers and polarization, revealing robust amorphous topological phases with 0-type and π-type edge modes. We show a structural disorder induced topological phase transition associated with localization transition in the nonequilibrium system. Remarkably, in the many-body case it is discovered that the amorphous topological order exists in the chain of hardcore bosons, captured by the topological entanglement entropy and the string order. Moreover, feasible experimental probe protocols are also elaborated. Introduction Symmetry-protected topological (SPT) orders have been widely explored in systems with underlying spatial order, such as topological insulators and superconductors [1–4], Dirac and Weyl semimetals [5, 6]. Nevertheless, it has been noted that the inclusion of the spatiotemporal engineering leads to richer topological phases. Even in systems without local crystalline symmetries, including topological quasicystals [7–10], amorphous solids and ar- tificial materials with completely random sites [11–19], the SPT order can exist. Such topological glassy matter does not rely on the microscopic details of the spatial configuration, manifesting its robustness and facilitating the material fabrications. On another note, with an ad- ditional degree of freedom in the time domain, period- ically driven nonequilibrium systems have extended the SPT phases to a new classification scheme, supporting anomalous bulk-boundary relations without equilibrium counterpart [20–28]. Aforementioned findings prompt a search for the new class of SPT phases with the synergy of Floquet topology and amorphous order. Meanwhile a feasible proposal on available quantum systems is in great need. Although the studies of SPT orders usually focus on the fermionic systems, they can also naturally appear in bosonic systems when strong interactions are considered and especially when the hardcore condition is fulfilled [29–32]. The characterization and classification of the bosonic SPT phases are based on the ground state of the many-body systems. The concept of SPT orders has been extended to the nonequilibrium setup in terms of the Floquet quantum matter [33–39]. For the experimental aspect, the Rydberg atoms combined with Floquet engi- neering provide a versatile platform for the quantum sim- ulation of many-body physics and the topological theory, due to its high tunability [4, 40–46]. The Rydberg atoms are individually controlled by the optical tweezer [47–50], and the coupling is tunable via dipole-dipole and van der Waals interactions [51–53], which allows rather flexible and local design of the lattice configuration. Among var- ious experimental progress [54–59], a bosonic SPT phase has been observed with a one-dimensional arrays of 87Rb atoms [54]. A later theoretical work has shown the ex- istence of the bosonic SPT order in amorphous systems [60]. In this paper, we study the SPT phases in a one- dimensional (1D) Floquet amorphous bosonic Rydberg atomic array, based on a experimentally feasible setup. We show the existence of rich SPT phases in both the single-particle level and many-body level. In the single- particle level, we map out the phase diagram according to the real space winding number and the polarization. Topological 0 phase and π/T phase can exist in amor- phous lattices with suitable design of the periodic driv- ings. Specifically, we further identify the structural disor- der driven phase transitions in our model. We find clear numerical signatures of the Anderson localization in the topological regimes, implying a special gapless Floquet phase. In the many-body level, we consider the hard core conditions at half-filling. We characterize the topology in terms of the topological entanglement entropy (TEE) and the string order, using exact diagonalization (ED) and density matrix renormalization group (DMRG). Fur- thermore, we provide methods to detect the SPT phases in experiments, with either microwave spectroscopy or the edge fidelity, which extracts the nontrivial boundary2 physics related with the topology. … … … sub-chain A sub-chain B 1 3 2 4 𝑻𝟏 𝑻𝟐 𝑻𝟏 𝑻𝟐 𝑻𝟏 𝑻𝟐 … a b c 𝑩 𝟏,𝟏 𝟎,𝟏 𝟎,𝟎 𝟎,𝟏 𝟎,𝟎 𝟎,−𝟏 FIG. 1: Floquet array of Rydberg atoms and the topo- logical phase diagrams. aSchematics of the driving pro- tocol on the array of Rydberg atoms with atoms at site 2 i−1 and 2i forming a unit cell. The two atoms in a unit cell are ini- tially sperated by a distanceR = (Rx, Ry, Rz), and subject to a periodic modulation. The dipolar exchange interaction be- tween two atoms provides hopping terms. The magnetic field B = (− √ 2, 0, 1)B is applied in the x-z plane with the polar angle θm = arccos(1/ √ 3) to cancel the hoppings within each subchain. The system is probed by a microwave field with the Rabi frequency Ω and detuning ∆. b Phase diagram char- acterized by W0 for the disordered system, with driving time duration T1 = T2 = 0.4, driving parameter f = (0.9, 1, 1), ini- tial distance Rz = 0.72, structural disorder strength d = 0.8, and system length 2 L = 200. Wπ = 0 for this driving con- dition and disorder strength. c Phase diagram characterized by Wπ for the disordered system, with driving time duration T1 = T2 = 0.4, driving parameter f = (1, 1, 0.55), initial dis- tance Rz = 0 .8, structural disorder strength d = 0 .8, and system length 2 L = 200. W0 = 0 for this driving condition and disorder strength. The red dashed lines show the phase boundaries determined by the polarization. The white solid lines show the phase boundaries of a regular system (without the structural disorder) for comparison. The numbers label the values of the winding numbersW0, Wπ for regular systems in different phases separated by the white lines for a regular system. The color bar shows the values of the winding num- bers for disordered systems. All the quantities are averaged over 30 random configurations. Results Model Hamiltonian. We consider an array of 2 L in- dividually trapped 87Rb atoms in a dimerized configu- ration, as shown in Fig. 1a. For each atom, only two Rydberg states from the 60 S1/2 and the 60 P1/2 mani- folds involve, which naturally serve as two distinct hard- core bosonic degrees of freedom. An s-level|60S1/2, mJ = 1/2⟩ corresponds to the “vacuum” of the many-body sys- tem, and a p-level |60P1/2, mJ = −1/2⟩ corresponds to one occupied boson. Because of the excitation transfer between two Rydberg atoms induced by the dipole-dipole interaction, the system may be described by the following Hamiltonian, ˆH(t) = 2LX i<j Vij(t)(ˆb† iˆbj + ˆb† jˆbi), (1) where ˆb† i (ˆbi) creates (annihilates) a hard-core boson at site i, and Vij = ˜d2 0(1 − 3 cos2 θij)/R3 ij is the dipolar cou- pling strength. Here ˜d0 is the the dipole moment be- tween the two subelevels, Rij is the separation between the atoms at site i and site j, and its angle with re- spect to the magnetic field B determines θij. Specif- ically, the two atoms in each unit cell is separated by a vector R = ( Rx, Ry, Rz)a0, and subejct to an adi- tional random dispalcement R2i−1 z = (i − 1 + δzi)a0 and (R2i z = i−1+ Rz +δzi)a0, where δzi is the structural dis- order uniformly sampled in the range [ −d/2, d/2], amor- phously shaped the lattice geometry, and a0 is the lat- tice unit. Furthermore, the atoms in two subchains are aligned along the so-called “magic angle” θii+2 = θm ≡ arccos(1/ √ 3), so that the coupling within each subchain is vanishing and the chiral (sublattice) symmetry is guar- anteed. In following discussions, we set a0 = 1 and ˜d2 0/a3 0 as the units of length and energy, respectively, and set ℏ = 1. To study the Floquet phases in this system, we con- sider the stroboscopic modulation of the atom displace- ment by periodically shift one of the subchain as, R(t) = \u001a R, t ∈ [mT, mT+ T1) f · R, t ∈ [mT + T1,(m + 1)T) , m∈ Z, (2) where f = (fx, fy, fz) is a set of real coefficients. The implementation of the Floquet protocol requires dynam- ically toggling of the reconfigurable Rydberg atom arrays. This approach is scalable, with long-time coherence, and has been realized in recent experiments [44, 61]. Com- pared to many previous proposals of Floquet engineer- ing usually demonstrated with a time-dependent detun- ing or applying an external field, our protocol preserves the system symmetry as described below. The dynam- ics under driving is described by the unitary evolution UT = e−i ˆH2T2 e−i ˆH1T1 , where we denote the Hamiltonian in the respective time duration T1 and T2 as ˆH1 and ˆH2 (T2 ≡ T − T1). Then we have an effective Hamilto- nian ˆHT = i T ln UT . The spectra εn of ˆHT are known as quasienergies and we take εn ∈ [−π/T, π/T]. We remark that the Hamiltonian ˆH(t) respects a di- hedral Z2 × Z symmetry, represented by an anti-unitary operator SB = Q2L i=1[bi+b† i ]K and discrete translations in time (K denotes the complex conjugation).The symme- try is exact since [ ˆH(t), SB] = 0 holds for every individ- ual sample configuration. However, ˆHF does not inherit the symmetry due to [ ˆH1, ˆH2] ̸= 0. After noting that UT has a Floquet guage degree of freedom, we can ap- ply two similarity transformations ˆGj = ei(−1)j ˆHjTj/2,3 converting UT into ˜UT,1 = e−i ˆH1T1/2e−i ˆH2T2 e−i ˆH1T1/2 and ˜UT,2 = e−i ˆH2T2/2e−i ˆH1T1 e−i ˆH2T2/2 respectively, from which we define ˆHF,i = i T ln ˜UT,i (i = 1, 2). ˆHF,i share the same quasienergies as ˆHT , but recover the symmetry of ˆH(t). Our model also preserves a U(1) symmetry [ˆH(t), ˆN] = 0 with ˆN = P i ˆb† iˆbi being the total particle number op- erator thus ˆN is conserved. In the experiment, a weak global microwave field is applied to create a state with certain excitation numbers if the detuning matches the system energy [54]. Therefore, we can study this system at both single-particle level and many-body level. a c e b d f FIG. 2: The topological phase transitions driven by structural disorder.The winding numbera, b, quasienergy gap c, d, and the mean inverse participation ratio and level- spacing ratio e, f versus structrual disorder d. We use initial distance R = (−0.5, 0, 0.8), driving parameter f = (0.9, 1, 1), driving time duration T1 = T2 = 0 .4 for a, c and e; and initial distance R = ( −0.5, 0, 0.8), driving parameter f = (1, 1, 0.55), driving time durationT1 = T2 = 0.4 for b, d and f. All the quantities are averaged over 50 random configurations. Single-particle case. We first study the topological properties of the single-excitation state manifold of the atom chain. We obtain a single-particle Hamiltonian ˆHS(t) with [ HS]ij = Vij(t)(1 − δij) (1 ≤ i, j≤ 2L), under a basis ˆb ≡ {ˆb† 1,ˆb† 2, . . . ,ˆb† 2L}. ˆHS(t) is chiral sym- metric SHSS−1 = −HS with S = diag {(−1)j−1}2L 1 , thus is equivalent to a Floquet amorphous Su-Schrieffer- Heeger (SSH) model. The nontrivial Floquet topology manifests in the existence of zero-energy edge modes and π/T -energy edge modes, in which the latter has no equi- librium counterpart. In the presence of the structural disorder, the quasi- momentum is no longer a good quantum number. To characterize the topology of single particle bands, we cal- culate the real-space winding numbers [62, 63], Wj=1,2 = 1 2L′ Tr′(SQj[Qj, X]) , (3) where X is the coordinate operator, Qj = P n(|nj⟩⟨nj|− S|nj⟩⟨nj|S†) with ˆHF,j|nj⟩ = εj,n|nj⟩, and Tr ′ denotes the trace over the bulk sites with length L′ = L − 2ℓ. The number of 0- and π/T -mode edge states relates to Wj as [64–67] W0 = (W1 + W2)/2, Wπ/T = (W1 − W2)/2. (4) The topological phases also carries a nontrivial polariza- tion which is quantized by the chiral symmetry. The polarization is given by, P = [ 1 2π Im ln detU − X l,l′,s,s′ Xls,l′s′ 2L ] mod 1 , (5) where the elements of U read Umn ≡ ⟨m|ei2πX/L|n⟩, and Xls,l′s′ = rlδll′ δss′ , with rl being the position of the l- th cell and s, s′ = A, B being the sublattice indices [68]. Due to its Z2 nature, the polarization can only distinguishes between even or odd number of pairs of edge states. Before proceeding, we briefly review the static case, which can be found in Ref.[60]. The phase diagram is dominated by the interplay between the nearest-neighbor intracell hopping and intercell hopping, whereas the long- range hopping enlarges the regime of topological phases. Similar to the SSH model, a topological limit can be found where the intracell hopping vanishes, given by R2 y−R2 x+2 √ 2RxRz (R2x+R2y+R2z)5/2 = 0. A robust topological phase is ob- served around the topological limit line. The Floquet driving results in richer phase structures. We map out two typical phase diagrams in the Rx-Ry plane according to the winding number and polariza- tion, see Fig. 1b and 1c. Numerical results clearly show the existence of the 0-type ( π/T -type) amorphous SPT order with W0 ≈ 1 (Wπ/T ≈ 1) according to dif- ferent drving conditions. Typically, we choose the pe- riodic driving as f = (0 .9, 1, 1) to induce the 0-phase, while f = (1, 1, 0.55) to induce the π/T -phase, although in general the type of the Floquet SPT phase does not rely on the driving of certain components of R (for more discussions, see Supplementary Note 1). Intuitively, as the driving period is increased such that the driving fre- quency is comparable to the band width, the π/T -gap would close and then reopen, which makes it possible to find the nontrivial π/T edge modes. In both nontrivial phases, the polarization is quantized to ≈ 0.5, and show- ing the same phase boundaries with that predicted by the winding numbers. We note that the Floquet system can be topological even when both ˆH1 and ˆH2 are trivial (see Fig. S1(b), Supplementary Note 1). These results reveal the unique features of our model as a nonequilib- rium system. Furthermore, we observe a clear deviation of the phase boundary compared to that in the regular limit. Part of the phase is trivial for regular systems while becomes topological under structural disorder, which im- plies an amorphousness-driven phase transition in the4 Floquet system. We note that the conclusions are not limited for the stroboscopic driving. We also show that the SPT phase exists under hamornic driving in Supple- mentary Note 2. Figure 2 illustrates the typical 0-phase and π/T -phase transition event driven by the disorder. The phase tran- sition is accompanied by gap closure. As shown in Fig. 2c and 2d, the gap quickly drops as the disorder in- creases. We see that the energy gap decreases as the system size increases, indicating the gap closes in the thermodynamical limit and system enters an ungapped localization phase in the topological regime. The topol- ogy is protected by the mobility subgap rather than the spectral gap [69–72]. In such phase all states are lo- calized. The localization properties are identified by both the level-spacing statistics and the mean inverse participation ratio (MIPR). For level statistics, we use the adjacent level-spacing ratio (LSR): r(ε) = [(1/(Nε − 2)) P i min(δi, δi+1)/ max(δi, δi+1)], where δi = εi − εi−1 with eigen-quasienergies εi’s sorted in an ascending order, and Nε is the number of the energy levels counted. As a general rule of thumb, for localized states, r ≈ 0.386, fulfilling the Poisson statistics, whereas for extended states, r ≈ 0.6 associated with the Gaussian orthogo- nal ensemble. On another note, the MIPR is defined by I = (1/NE) P n,x |Ψn,x|4 with |Ψn,x|2 being the local density of the n-th state on site x and NE being the en- ergy levels counted. A state is extended when I ∼ 1/L. As comfirmed in Fig. 2e and 2f, our model supports ro- bust ungapped 0-type and π-type localized SPT phase in a wide range of parameters, which is reminiscent of the Floquet Anderson insulator reported in Refs. [73, 74]. However, the fully localized topological Floquet band in Ref. [74] relies on the anomalous topology with zero bulk invariant. In contrast, our model supports fully localized topological Floquet band in both normal and anomalous phases, guaranteed that the 1D chiral system is Wannier localizable. We also find that the system could be topo- logical even when the atoms are completely randomly positioned as shown in Fig. 2a. In Fig 2b, the system un- dergoes a transition from the topological phase to trivial Anderson phase under strong disorder. The two phases are separated by a delocalized critical point, showing that they can not be continuously connected (for more details, see Supplementary Note 3). Next, we propose to detect the Floquet topological phases with microwave spectroscopy. To this end, a weak global microwave field with the Rabi frequency Ω and de- tuning ∆ is shined on. Then the system is described by a Hamiltonian ˆHΩ = ˆH + Ω/2 P i(ˆb† i + ˆbi) − ∆ Pˆb† iˆbi. The atoms are initially prepared in a vacuum state in the s-level. After applying the microwave probe for some periods of time tm, an excitation can be created only if an eigenstate energy matches the detuning ∆. In Fig. 3a-3d, we display the site-resolved probability on the p-level at the final time for different configurations (trivial regular, Occupancy Site Site Site Site ai aii bi ci di bii cii dii FIG. 3: Microwave detection of the edge states.Occu- pancy of each site excited by the microwave field with detun- ing ∆ for ai-di and the corresponding selected Occupancy for aii-dii at ∆ = 0 for aii, bii and dii, or ∆ = π for cii, respec- tively. We use R = (−0.7, 0, 0.8), f = (0.1, 1, 1), d = 0, Ω = 0.05, tm = 10T in a; R = (−0.5, 0, 0.8), f = (0.7, 1, 1), d = 0, Ω = 0.05, tm = 30T in b; R = (−0.5, 0, 0.8), f = (0.35, 1, 1), d = 0, Ω = 0 .05, tm = 12 T in c; R = ( −0.9, 1.6, 0.8), f = (0 .5, 1, 1), d = 0 .3, Ω = 0 .05, tm = 60 T in d. Here R is the initial distance, f is the driving parameter, d is the structural disorder strength, Ω is the Rabi frequency of the microwave, and tm is the total evolution time. topological 0-phase regular, topological π/T -phase regu- lar, topological amorphous, respectively), which can be detected by the fluorescence imaging in the experiment. We can observe a clear signal of the edge mode at either zero or π detuning only in the topological phases, while a uniform distribution in the trivial cases. Many-body case. We now proceed to address the many-body case. The Hamiltonian (1) can be mapped to an XY spin model via Matsubara-Matsuda trans- formation, ˆH(t) = P i<j Vij(t)(σ+ i σ− j + σ+ j σ− i ), where σ± i = (σx i ± iσy i )/2 with σs j (s = x, y, z) being the Pauli matrices at site i. Correspondingly mz = P i σz i /2 is a conserved quantity. The subspace of the spin Hamilto- nian with mz = 0 is equivalent to the hard-core bosonic Hamiltonian at half-filling. We note that the effective Hamiltonian containing longer-range hoppings, despite without an interaction term like σz i σz j , can not be cast to a free fermionic model by the inverse Jordan-Wigner transformation, showing the many-body aspect of our model. We identify the topology of the many-body system by the topological entanglement entropy [75–77], SE = SL/2 + S˜L/2 − SL/2∪˜L/2 − SL/2∩˜L/2, (6) where SL/2 (S˜L/2) is the half-chain entanglement entropy of the reduced density matrix for the left half (second quarter and fourth quarter) part of the chain. The phase diagram according to SE is shown in Fig. 4a. The TEE is calculated for a Floquet eigenstate (analogous to the ground state of a static system) of a second order ef- fective Hamiltonian, which is not degenerate for system with a periodic boundary. For the many-body case, the quasi-energy will increase when more excitations are cre- ated, ε ∼ O(N). Therefore, there exists larger param- eter regimes with max( ε)T < 2π than the single parti-5 a b c d 2  1 0 FIG. 4: Topological phase diagrams for the many-body case. aPhase diagram characterized by topological entangle- ment entropy SE/ln2 (with the color shows the values) for a half-filled chain with 2 L = 8 sites, and driving time duration T1 = T2 = 0.4, driving paramters f = (0.8, 1, 1), initial dis- tance Rz = 0 .55, structural disorder strength d = 0 .8, and averaged over 100 random configurations. b Phase diagram characterized by string order (with the color shows the val- ues), with same parameter setting. c String order for systems with different lattice size. We choose the section ofRx = −0.8 here. d The trace fidelity for the topological case and trivial case for a lattice with 2 L = 12 sites. cle case, where we could apply the Magnus expansion to simplify the numerical calculations (see Supplementary Note 4). The TEE of an SPT states yields a quantized value 2 ln 2, due to that the topology is essentially car- ried on the boundary. In contrast, TEE vanishes in the trivial phase. The numerical results show a robust re- gion of the topological phase under structural disorder for the many body case. In addition, as an measurable signature of the Floquet SPT, we calculate the string or- der Cz string = ( −1)N−1⟨Q2L−1 i=2 σz i ⟩ [78–80], in Fig. 4b. The bulk of the topological states also acquire a finite long-range string order, and nearly vanishes in the triv- ial phase. We find that the TEE and the string order exhibit almost the same behaviors. In these calculations, We use ED for L <10 and DMRG for L >10, and com- pare the results for different system sizes in Fig. 4c. The DMRG calculations are performed using ITensor library [81]. To further diagnose the boundary physics, we calculate the edge fidelity, Fα i = Tr[σα i (t)σα i (0)]/Ndim with Ndim (α = x, y, z) being the dimension of the Hilbert space, following the definition in Ref. [36]. The Floquet eigen- state is fourfold degenerate on open chains, as the edge modes for two sides can be either empty or occupied. As illustrated in Fig. 4d, the edge sites exhibits much longer coherence time than the bulk sites only in the topological regime, but quickly damps in the trivial phase. The long edge coherence of both Fz and Fx reveals the physical consequence of the SPT order in our model, and provides experimentally observable evidence. Discussion The topologically protected π/T quasienergy excita- tions in the Floquet system always comes up in pairs, results in the subharmonic response at the edge [36]. We show that our system exhibits time cystal behaviors at the edge sites, and such feature even exists under struc- tural disorder (for details, see Supplementary Note 5). Although focusing here on the Rydberg atoms, our model may be generalized to other artificial quantum systems, such as trapped ion chain [82–86], and superconducting qubits [88, 89]; and other symmetry class in higher di- mensions. In summary, we have proposed a concept of the Flo- quet amorphous topological matter. We have studied the topological properties in both single-particle and many- body levels. With various numerical calculations and careful comparison of different methods, the existence of rich Floquet SPT phases has been confirmed. We have found that diverse exotic SPT phases can be induced from the trivial static system by the periodic driving, or from the regular system by the disorder, along with lo- calization transition. We have further addressed possible detection methods based on currently feasible technolo- gies in the experiments of Rydberg atoms. Therefore, our work would provide a promising platform for explor- ing the exotic physics of nonequilibrium systems that are elusive in nature. Data availability The data used to create the figures are available at Sup- plementary Data 1-4. Code availability All relevant codes are available from the corresponding authors upon reasonable request. Contributions P.H. and H.W. developed the ideas and designed the research, as well as performed analytical calculations and computational simulations. J.X.L. performed the DMRG simulations. Z.D.W. supervised the project. All authors discussed the results and contributed to the text of the manuscript. Competing interests The author declares no competing interests. This work was supported by the NSFC/RGC JRS grant (Grant No. N HKU 774/21), the CRF (Grant No. C6009-20G), GRF (Grants No. 17310622 and No. 17303023) of Hong Kong, National Natural Science Foun- dation (Grants No. 12405007), and Funds for Young Sci- entists of Chongqing Municipal Education Commission (Grants No.KJQN20240).6 ∗ Electronic address: penghe@hku.hk † Electronic address: wuh@cqupt.edu.cn ‡ Electronic address: zwang@hku.hk [1] M. Z. Hasan and C. L. Kane, Colloquium: topological insulators, Rev. Mod. Phys. 82, 3045 (2010). [2] X.-L. Qi and S.-C. Zhang, Topological insulators and su- perconductors, Rev. Mod. Phys. 83, 1057 (2011). [3] C. K. Chiu, J. C. Teo, A. P. Schnyder, and S. Ryu, Classi- fication of topological quantum matter with symmetries, Rev. Mod. Phys. 88, 035005 (2016). [4] D.-W. Zhang, Y.-Q. Zhu, Y.-X. Zhao, H. Yan, and S.-L. Zhu, Topological quantum matter with cold atoms, Adv. Phys. 67, 253 (2018). [5] N. P. Armitage, E. J. Mele, and A. Vishwanath, Weyl and Dirac semimetals in three-dimensional solids, Rev. Mod. Phys. 90, 015001 (2018). [6] Y. Xu, Topological gapless matters in three-dimensional ultracold atomic gases, Front. Phys. 14, 43402 (2019). [7] S. Aubry and G. Andr´ e, Analyticity breaking and An- derson localization in incommensurate lattices, Ann. Isr. Phys. Soc. 3, 133 (1980). [8] P. G. Harper, Single band motion of conduction electrons in a uniform magnetic field, Proc. Phys. Soc. London Sect. A 68, 874 (1955). [9] S. Das Sarma, S. He, and X. C. Xie, Localization, mo- bility edges, and metal-insulator transition in a class of one-dimensional slowly varying deterministic potentials, Phys. Rev. B 41, 5544 (1990). [10] J. Biddle and S. Das Sarma, Predicted mobility edges in one-dimensional incommensurate optical lattices: An exactly solvable model of Anderson localization, Phys. Rev. Lett. 104, 070601 (2010). [11] P. Corbae, J. D. Hannukainen, Q. Marsal, D. Mu˜ noz- Segovia, and A. G. Grushin, Amorphous topological mat- ter: Theory and experiment, Europhys. Lett. 142(1), 16001 (2023). [12] A. Agarwala and V. B. Shenoy, Topological insulators in amorphous systems, Phys. Rev. Lett. 118, 236402 (2017). [13] Y. B. Yang, T. Qin, D. L. Deng, L.-M. Duan, and Y. Xu, Topological amorphous metals, Phys. Rev. Lett. 123, 076401 (2019). [14] J. H. Wang, Y. B. Yang, N. Dai, and Y. Xu, Structural- disorder-induced second-order topological insulators in three dimensions, Phys. Rev. Lett. 126, 206404 (2021). [15] M. N. Ivaki, I. Sahlberg, and T. Ojanen, Criticality in amorphous topological matter: Beyond the universal scaling paradigm, Phys. Rev. Res. 2, 043301 (2020). [16] I. Sahlberg, A. Weststr¨ om, K. P¨ oyh¨ onen,, and T. Ojanen, Topological phase transitions in glassy quantum matter. Phys. Rev. Res. 2, 013053 (2020). [17] J. D. Hannukainen, M. F. Mart´ ınez, J. H. Bardarson, and T. K. Kvorning, Local topological markers in odd spatial dimensions and their application to amorphous topological matter, Phys. Rev. Lett. 129, 277601 (2022). [18] C. Wang, T. Cheng, Z. Liu, F. Liu, and H. Huang, Struc- tural amorphization-induced topological order, Phys. Rev. Lett. 128, 056401 (2022). [19] X. Cheng, T. Qu, L. Xiao, S. Jia, J. Chen, and L. Zhang, Topological Anderson amorphous insulator, Phys. Rev. B, 108, L081110 (2023). [20] T. Kitagawa, E. Berg, M. Rudner, and E. Demler, Topo- logical characterization of periodically driven quantum systems, Phys. Rev. B 82, 235114 (2010). [21] M. S. Rudner, N. H. Lindner, E. Berg, and M. Levin, Anomalous edge states and the bulk-edge correspondence for periodically driven two dimensional systems, Phys. Rev. X 3, 031005 (2013). [22] N. Goldman and J. Dalibard, Periodically Driven quan- tum systems: effective Hamiltonians and engineered gauge fields, Phys. Rev. X 4, 031027 (2014). [23] A. Eckardt, Colloquium: Atomic quantum gases in pe- riodically driven optical lattices, Rev. Mod. Phys. 89, 011004 (2017). [24] R. Roy and F. Harper, Periodic table for Floquet topo- logical insulators, Phys. Rev. B 96, 155118 (2017). [25] S. Yao, Z. Yan, Z. Wang, Topological invariants of Flo- quet systems: General formulation, special properties, and Floquet topological defects, Phys. Rev. B 96, 195303 (2017). [26] W. W. Ho, T. Mori, D. A. Abanin, and E. G. Dalla Torre,. Quantum and classical Floquet prethermaliza- tion, Ann. Phys. , 169297 (2023). [27] T. Nag, B. Roy, Anomalous and normal dislocation modes in Floquet topological insulators, Commun. Phys. 4, 157 (2021). [28] A. K. Ghosh, T. Nag, and A. Saha, Generation of higher- order topological insulators using periodic driving, J. Phys. : Condens. Matter 36, 093001 (2023). [29] F. D. M. Haldane, Nonlinear field theory of large-spin Heisenberg antiferromagnets: semiclassically quantized solitons of the one-dimensional easy-axis N´ eel state, Phys. Rev. Lett. 50, 1153 (1983). [30] F. Pollmann, A. M. Turner, E. Berg, M. Oshikawa, En- tanglement spectrum of a topological phase in one di- mension, Phys. Rev. B 81, 064439 (2010). [31] X. Chen, Z.-C. Gu, and X.-G. Wen, Classification of gapped symmetric phases in one-dimensional spin sys- tems, Phys. Rev. B 83, 035107 (2011). [32] X. Chen, Z.-C. Gu, Z.-X. Liu, and X.-G. Wen, Symmetry- protected topological orders in interacting bosonic sys- tems, Science 338, 1604–1606 (2012). [33] V. Khemani, A. Lazarides, R. Moessner, and S. L. Sondhi, Phase structure of driven quantum systems. Phys. Rev. Lett. 116, 250401 (2016). [34] A. C. Potter, T. Morimoto, and A. Vishwanath. Classi- fication of interacting topological Floquet phases in one dimension, Phys. Rev. X 6, 041001 (2016). [35] H. C. Po, L. Fidkowski, T. Morimoto, A. C. Potter, and A. Vishwanath,. Chiral floquet phases of many-body lo- calized bosons. Phys. Rev. X 6, 041070 (2016). [36] I. D. Potirniche, A. C. Potter, M. Schleier-Smith, A. Vishwanath, and N. Y. Yao, Floquet symmetry-protected topological phases in cold-atom systems, Phys. Rev. Lett. 119, 123601 (2017). [37] F. Harper, R. Roy, Floquet topological order in inter- acting systems of bosons and fermions, Phys. Rev. Lett. 118, 115301 (2017). [38] F. Mei, Q. Guo, Y.-F. Yu, L. Xiao, S.-L. Zhu, and S. Jia, Digital simulation of topological matter on pro- grammable quantum processors, Phys. Rev. Lett. 125, 160503 (2020). [39] X. Zhang, W. Jiang, J. Deng, et al., Digital quantum simulation of Floquet symmetry-protected topological phases, Nature 607, 468–473 (2022).7 [40] M. Saffman, T. G. Walker, and K. Mølmer, Quantum information with Rydberg atoms, Rev. Mod. Phys. 82, 2313 (2010). [41] A. Browaeys and T. Lahaye, Many-body physics with individually controlled Rydberg atoms, Nat. Phys. 16, 132 (2020). [42] S. Geier, N. Thaicharoen, C. Hainaut, et al., Floquet Hamiltonian engineering of an isolated many-body spin system, Science 374, 1149 (2021). [43] P. Scholl, H. J. Williams, G. Bornet, et al., Microwave Engineering of Programmable XXZ Hamiltonians in Ar- rays of Rydberg Atoms, PRX Quantum 3(2), 020303 (2022). [44] M. Kalinowski, N. Maskara, M. D. Lukin, Non-abelian floquet spin liquids in a digital rydberg simulator, Phys. Rev. X 13, 031008 (2023). [45] T.-F. J. Poon, X.-C. Zhou, B.-Z. Wang, T.-H. Yang, and X.-J. Liu, Fractional quantum anomalous Hall phase for Raman superarray of Rydberg atoms, Adv Quantum Technol., 2300356 (2024). [46] Y. Cheng and H. Zhai, Emergent gauge theory in Ryd- berg atom arrays, arXiv:2401.07708 (2024). [47] M. Endres, H. Bernien, A. Keesling, H. Levine, E. R. Anschuetz, A. Krajenbrink, C. Senko, V. Vuletic, M. Greiner, and M. D. Lukin, Atom-by-atom assembly of defect-free one-dimensional cold atom arrays, Science 354, 1024 (2016). [48] H. Kim, W. Lee, H.-g. Lee, H. Jo, Y. Song, and J. Ahn, In situ single-atom array synthesis using dynamic holo- graphic optical tweezers, Nat. Commun. 7, 13317 (2016). [49] D. Barredo, S. De L´ es´ eleuc, V. Lienhard, T. Lahaye, and A. Browaeys, An atom-by-atom assembler of defect-free arbitrary two-dimensional atomic arrays, Science 354, 1021 (2016). [50] D. Barredo, V. Lienhard, S. De L´ es´ eleuc, T. Lahaye, and A. Browaeys, Synthetic three-dimensional atomic struc- tures assembled atom by atom, Nature 561, 79 (2018). [51] A. Browaeys, D. Barredo, T. Lahaye, Experimental in- vestigations of dipole–dipole interactions between a few Rydberg atoms, J. Phys. B 49, 152001 (2016). [52] A. P. Orioli, A. Signoles, H. Wildhagen, G. G¨ unter, J. Berges, S. Whitlock, and M. Weidem¨ uller, Relaxation of an isolated dipolar-interacting Rydberg quantum spin system, Phys. Rev. Lett. 120, 063601 (2018). [53] A. Signoles, T. Franz, R. Ferracini Alves, M. G¨ arttner, S. Whitlock, G. Z¨ urn, and M. Weidem¨ uller, Glassy Dy- namics in a disordered Heisenberg quantum spin system, Phys. Rev. X 11, 011011 (2021). [54] S. de L´ es´ eleuc, V. Lienhard, P. Scholl, D. Barredo, S. We- ber, N. Lang, H. P. B¨ uchler, T. Lahaye, and A. Browaeys, Observation of a symmetry-protected topological phase of interacting bosons with rydberg atoms, Science 365, 775 (2019). [55] V. Lienhard, P. Scholl, S. Weber, et al., Realization of a density-dependent Peierls phase in a synthetic, spin- orbit coupled Rydberg system, Phys. Rev. X 10, 021031 (2020). [56] D. Bluvstein, A. Omran, H. Levine, et al., Controlling quantum many-body dynamics in driven Rydberg atom arrays, Science 371, 1355-1359 (2021). [57] G. Semeghini, H. Levine, A. Keesling, S. Ebadi, T. T. Wang, D. Bluvstein, R. Verresen, H. Pichler, M. Kali- nowski, R. Samajdar, et al., Probing topological spin liquids on a programmable quantum simulator, Science 374, 1242 (2021). [58] C. Chen, G. Bornet, M. Bintz, G. Emperauger, L. Leclerc, V. S. Liu, P. Scholl, D. Barredo, J. Hauschild, S. Chatterjee, et al., Continuous symmetry breaking in a two-dimensional rydberg array, Nature 616, 691 (2023). [59] S. Juli` a-Farr´ e, J. Vovrosh, A. Dauphin, Amorphous quantum magnets in a two-dimensional Rydberg atom array, arXiv:2402.02852 (2024). [60] K. Li, J. H. Wang, Y. B. Yang, and Y. Xu, Symmetry- protected topological phases in a Rydberg glass, Phys. Rev. Lett. 127, 263004 (2021). [61] D. Bluvstein, H. Levine, G. Semeghini, T.T. Wang, S. Ebadi, M. Kalinowski, A. Keesling, N. Maskara, H. Pich- ler, M. Greiner, V. Vuleti´ c, and M.D. Lukin, A quantum processor based on coherent transport of entangled atom arrays, Nature 604, 451 (2022). [62] A. Kitaev, Anyons in an exactly solved model and be- yond, Ann. Phys. (Amsterdam) 321, 2 (2006). [63] R. Bianco and R. Resta, Mapping topological order in coordinate space, Phys. Rev. B 84, 241106(R) (2011). [64] J. K. Asb´ oth, B. Tarasinski, and P. Delplace. Chiral sym- metry and bulk-boundary correspondence in periodically driven one-dimensional systems, Phys. Rev. B90, 125143 (2014). [65] L. Lin, Y. Ke, and C. Lee, Real-space representation of the winding number for a one-dimensional chiral- symmetric topological insulator, Phys. Rev. B 103, 224208 (2021). [66] L. Zhou and J. Gong, Non-Hermitian Floquet topolog- ical phases with arbitrarily many real-quasienergy edge states, Phys. Rev. B 98, 205417 (2018). [67] H. Wu and J. H. An, Floquet topological phases of non- Hermitian systems, Phys. Rev. B 102, 041119 (2020). [68] R. Resta, Quantum-mechanical position operator in ex- tended systems, Phys. Rev. Lett. 80, 1800 (1998). [69] Y. Y. Zhang, R.L. Chu, F.C. Zhang, and S.Q. Shen, Lo- calization and mobility gap in topological Anderson in- sulator, Phys. Rev. B 85, 035107 (2012). [70] T. A. Loring, K-theory and pseudospectra for topological insulators, Annals of Physics 356, 383–416 (2015). [71] A. Cerjan and T. A. Loring, Local invariants identify topol ogy in metals and gapless systems, Phys. Rev. B 106, 064109 (2022). [72] M. Ren, Y. Yu, B. Wu, et al., Realization of gapped and ungapped photonic topological Anderson insulators, Phys. Rev. Lett. 132, 066602 (2024). [73] P. Titum, N. H. Lindner, M.C. Rechtsman, and G. Refael, Disorder-induced Floquet topological insulators, Phys. Rev. Lett. 114, 056801 (2015). [74] P. Titum, E. Berg, M. S. Rudner, G. Refael, and N. H. Lindner, Anomalous Floquet-Anderson insulator as a nonadiabatic quantized charge pump, Phys. Rev. X 6, 021013 (2016). [75] B. Zeng and X.-G. Wen, Gapped quantum liquids and topological order, stochastic local transformations and emergence of unitarity, Phys. Rev. B 91, 125121 (2015). [76] B. Zeng and D. L. Zhou, Topological and error-correcting properties for symmetry-protected topological order, Eu- rophysics Letters 113, 56001 (2016). [77] H. Ling, P. Richard, S. R. Koshkaki, M. Kolodrubetz, D. Meidan, A. Mitra, T. Pereg-Barnea, Disorder induced topological phase transition in a driven Majorana chain, arXiv:2310.17088 (2023). [78] M. den Nijs and K. Rommelse, Preroughening transitions8 in crystal surfaces and valence-bond phases in quantum spin chains, Phys. Rev. B 40, 4709 (1989). [79] T. Kennedy and H. Tasaki, Hidden Z2 symmetry break- ing in Haldane-gap antiferromagnets, Phys. Rev. B 45, 304 (1992). [80] K. Hida, Ground-state phase diagram of the spin-1/2 ferromagnetic antiferromagnetic alternating Heisenberg chain with anisotropy. Phys. Rev. B 46, 8268 (1992). [81] M. Fishman, S. R. White, and E. M. Stoudenmire, the ITensor software library for tensor network calculations, SciPost Phys. Codebases 4 (2022). [82] R. W. Bomantara, S. Mu, and J. Gong, Topological and dynamical features of periodically driven spin ladders, Phys. Rev. B 103, 235404 (2021). [83] S. L. Zhu, C. Monroe, and L.M. Duan, Trapped ion quan- tum computation with transverse phonon modes, Phys. Rev. Lett. 97, 050505 (2006). [84] R. Blatt and C. F. Roos, Quantum simulations with trapped ions, Nat. Phys. 8, 277 (2012). [85] J. Zhang, et al. Observation of a discrete time crystal. Nature 543, 217 (2017). [86] S. Choi, J. Choi, R. Landig, and et al., Observation of dis- crete time-crystalline order in a disordered dipolar many- body system, Nature 543, 221 (2017). [87] A. Kyprianidis, et al., Observation of a prethermal dis- crete time crystal. Science 372, 1192 (2021). [88] C. Ying, et al., Floquet prethermal phase protected by u(1) symmetry on a superconducting quantum processor, Phys. Rev. A 105, 012418 (2022). [89] X. Mi, M. Ippoliti, C. Quintana, and et al. , Time- crystalline eigenstate order on a quantum processor, Na- ture 601, 531 (2022).",
      "references": [
        "Colloquium: topological insulators",
        "Topological insulators and superconductors",
        "Classification of topological quantum matter with symmetries",
        "Topological quantum matter with cold atoms",
        "Weyl and Dirac semimetals in three-dimensional solids",
        "Topological gapless matters in three-dimensional ultracold atomic gases",
        "Analyticity breaking and Anderson localization in incommensurate lattices",
        "Single band motion of conduction electrons in a uniform magnetic field",
        "Localization, mobility edges, and metal-insulator transition in a class of one-dimensional slowly varying deterministic potentials",
        "Predicted mobility edges in one-dimensional incommensurate optical lattices: An exactly solvable model of Anderson localization",
        "Amorphous topological matter: Theory and experiment",
        "Topological insulators in amorphous systems",
        "Topological amorphous metals",
        "Structural-disorder-induced second-order topological insulators in three dimensions",
        "Criticality in amorphous topological matter: Beyond the universal scaling paradigm",
        "Topological phase transitions in glassy quantum matter.",
        "Local topological markers in odd spatial dimensions and their application to amorphous topological matter",
        "Structural amorphization-induced topological order",
        "Topological Anderson amorphous insulator",
        "Topological characterization of periodically driven quantum systems",
        "Anomalous edge states and the bulk-edge correspondence for periodically driven two dimensional systems",
        "Periodically Driven quantum systems: effective Hamiltonians and engineered gauge fields",
        "Colloquium: Atomic quantum gases in periodically driven optical lattices",
        "Periodic table for Floquet topological insulators",
        "Topological invariants of Floquet systems: General formulation, special properties, and Floquet topological defects",
        "Quantum and classical Floquet prethermalization",
        "Anomalous and normal dislocation modes in Floquet topological insulators",
        "Generation of higher-order topological insulators using periodic driving",
        "Nonlinear field theory of large-spin Heisenberg antiferromagnets: semiclassically quantized solitons of the one-dimensional easy-axis N´ eel state",
        "Entanglement spectrum of a topological phase in one dimension",
        "Classification of gapped symmetric phases in one-dimensional spin systems",
        "Symmetry-protected topological orders in interacting bosonic systems",
        "Phase structure of driven quantum systems",
        "Classification of interacting topological Floquet phases in one dimension",
        "Chiral floquet phases of many-body localized bosons",
        "Floquet symmetry-protected topological phases in cold-atom systems",
        "Floquet topological order in interacting systems of bosons and fermions",
        "Digital simulation of topological matter on programmable quantum processors",
        "Digital quantum simulation of Floquet symmetry-protected topological phases",
        "Quantum information with Rydberg atoms",
        "Many-body physics with individually controlled Rydberg atoms",
        "Floquet Hamiltonian engineering of an isolated many-body spin system",
        "Microwave Engineering of Programmable XXZ Hamiltonians in Arrays of Rydberg Atoms",
        "Non-abelian floquet spin liquids in a digital rydberg simulator",
        "Fractional quantum anomalous Hall phase for Raman superarray of Rydberg atoms",
        "Emergent gauge theory in Rydberg atom arrays",
        "Atom-by-atom assembly of defect-free one-dimensional cold atom arrays",
        "In situ single-atom array synthesis using dynamic holographic optical tweezers",
        "An atom-by-atom assembler of defect-free arbitrary two-dimensional atomic arrays",
        "Synthetic three-dimensional atomic structures assembled atom by atom",
        "Experimental investigations of dipole–dipole interactions between a few Rydberg atoms",
        "Relaxation of an isolated dipolar-interacting Rydberg quantum spin system",
        "Glassy Dynamics in a disordered Heisenberg quantum spin system",
        "Observation of a symmetry-protected topological phase of interacting bosons with rydberg atoms",
        "Realization of a density-dependent Peierls phase in a synthetic, spin-orbit coupled Rydberg system",
        "Controlling quantum many-body dynamics in driven Rydberg atom arrays",
        "Probing topological spin liquids on a programmable quantum simulator",
        "Continuous symmetry breaking in a two-dimensional rydberg array",
        "Amorphous quantum magnets in a two-dimensional Rydberg atom array",
        "Symmetry-protected topological phases in a Rydberg glass",
        "A quantum processor based on coherent transport of entangled atom arrays",
        "Anyons in an exactly solved model and beyond",
        "Mapping topological order in coordinate space",
        "Chiral symmetry and bulk-boundary correspondence in periodically driven one-dimensional systems",
        "Real-space representation of the winding number for a one-dimensional chiral-symmetric topological insulator",
        "Non-Hermitian Floquet topological phases with arbitrarily many real-quasienergy edge states",
        "Floquet topological phases of non-Hermitian systems",
        "Quantum-mechanical position operator in extended systems",
        "Localization and mobility gap in topological Anderson insulator",
        "K-theory and pseudospectra for topological insulators",
        "Local invariants identify topol ogy in metals and gapless systems",
        "Realization of gapped and ungapped photonic topological Anderson insulators",
        "Disorder-induced Floquet topological insulators",
        "Anomalous Floquet-Anderson insulator as a nonadiabatic quantized charge pump",
        "Gapped quantum liquids and topological order, stochastic local transformations and emergence of unitarity",
        "Topological and error-correcting properties for symmetry-protected topological order",
        "Disorder induced topological phase transition in a driven Majorana chain",
        "Preroughening transitions in crystal surfaces and valence-bond phases in quantum spin chains",
        "Hidden Z2 symmetry breaking in Haldane-gap antiferromagnets",
        "Ground-state phase diagram of the spin-1/2 ferromagnetic antiferromagnetic alternating Heisenberg chain with anisotropy",
        "the ITensor software library for tensor network calculations",
        "Topological and dynamical features of periodically driven spin ladders",
        "Trapped ion quantum computation with transverse phonon modes",
        "Quantum simulations with trapped ions",
        "Observation of a discrete time crystal",
        "Observation of discrete time-crystalline order in a disordered dipolar many-body system",
        "Observation of a prethermal discrete time crystal",
        "Floquet prethermal phase protected by u(1) symmetry on a superconducting quantum processor",
        "Time-crystalline eigenstate order on a quantum processor"
      ],
      "meta_data": {
        "arxiv_id": "2404.18512v2",
        "doi": "10.1038/s42005-025-02164-4",
        "authors": [
          "Peng He",
          "Jing-Xin Liu",
          "Hong Wu",
          "Z. D. Wang"
        ],
        "published_date": "2024-04-29T08:57:26Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposed and explored Floquet amorphous topological matter in a one-dimensional array of randomly positioned Rydberg atoms with periodic driving. Discovered rich Floquet topological phases at both single-particle and many-body levels, characterized by 0-type and π-type edge modes. Demonstrated structural disorder-induced topological phase transitions associated with localization, and provided feasible experimental probe protocols for these phases.",
        "methodology": "The study uses a model Hamiltonian describing a 1D array of 87Rb atoms with a stroboscopic modulation of atom displacement (Floquet protocol). For the single-particle case, topological properties are characterized by real-space winding numbers and polarization. For the many-body case with hardcore bosons, topological entanglement entropy (TEE) and string order are calculated using exact diagonalization (ED) for smaller systems and density matrix renormalization group (DMRG) for larger systems. Localization properties are identified by level-spacing statistics and mean inverse participation ratio (MIPR). Experimental detection methods include microwave spectroscopy and edge fidelity.",
        "experimental_setup": "The system consists of an array of 2L individually trapped 87Rb atoms in a dimerized configuration, subject to periodic driving by shifting one subchain. Structural disorder is introduced as a random displacement sampled uniformly. Parameters like driving time duration (T1, T2), driving parameters (f), initial distance (Rz), and structural disorder strength (d) are varied to map out phase diagrams. Numerical calculations are averaged over multiple random configurations (e.g., 30 or 50 for single-particle, 100 for many-body). Microwave spectroscopy with a weak global microwave field (Rabi frequency Ω, detuning ∆) and edge fidelity are proposed as experimental validation methods.",
        "limitations": "The effective Hamiltonian used for Floquet phases does not inherently inherit the symmetry of the time-dependent Hamiltonian due to non-commuting terms, requiring similarity transformations to recover symmetry. In the presence of structural disorder, quasi-momentum is no longer a good quantum number. The energy gap closes in the thermodynamical limit, leading to an ungapped localization phase where topology is protected by the mobility subgap rather than the spectral gap.",
        "future_research_directions": "The proposed model could be generalized to other artificial quantum systems, such as trapped ion chains and superconducting qubits. Future work could also explore other symmetry classes and higher dimensions for Floquet amorphous topological matter.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Generated Knowledge Prompting for Commonsense Reasoning",
      "full_text": "Multi-Agent Reinforcement Learning for Network Load Balancing in Data Center Zhiyuan Yao École Polytechnique & Cisco Systems Paris, France zhiyuan.yao@polytechnique.edu Zihan Ding Princeton University Princeton, New Jersey, USA zihand@princeton.edu Thomas Clausen École Polytechnique Paris, France thomas.clausen@polytechnique.edu ABSTRACT This paper presents the network load balancing problem, a chal- lenging real-world task for multi-agent reinforcement learning (MARL) methods. Conventional heuristic solutions like Weighted- Cost Multi-Path (WCMP) and Local Shortest Queue (LSQ) are less flexible to the changing workload distributions and arrival rates, with a poor balance among multiple load balancers. The coopera- tive network load balancing task is formulated as a Dec-POMDP problem, which naturally induces the MARL methods. To bridge the reality gap for applying learning-based methods, all models are di- rectly trained and evaluated on a real-world system from moderate- to large-scale setups. Experimental evaluations show that the inde- pendent and “selfish” load balancing strategies are not necessarily the globally optimal ones, while the proposed MARL solution has a superior performance over different realistic settings. Addition- ally, the potential difficulties of the application and deployment of MARL methods for network load balancing are analysed, which helps draw the attention of the learning and network communities to such challenges. CCS CONCEPTS • Computing methodologies →Multi-agent reinforcement learning; • Networks →Network resources allocation ; Cloud computing. KEYWORDS MARL, load balancing, distributed systems ACM Reference Format: Zhiyuan Yao, Zihan Ding, and Thomas Clausen. 2022. Multi-Agent Rein- forcement Learning for Network Load Balancing in Data Center. InProceed- ings of the 31st ACM International Conference on Information and Knowledge Management (CIKM ’22), October 17–21, 2022, Atlanta, GA, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3511808.3557133 1 INTRODUCTION In data centers (DCs), network load balancers (LBs) play a signifi- cant role to distribute time-sensitive requests from clients across a cluster of application servers and provide scalable services [ 9]. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM ’22, October 17–21, 2022, Atlanta, GA, USA. © 2022 Association for Computing Machinery. ACM ISBN 978-1-4503-9236-5/22/10. . . $15.00 https://doi.org/10.1145/3511808.3557133 least-loaded over-loaded normal Application Servers Client Load Balancer Observations Server Load Inference Server Load Ranking MARL Load Balancing Figure 1: Network load balancing in DC networks and the scope of study of this paper. 0 5 10 15 actual #threads #flows 0 20 40 60 80 100 Time (s) 0 5 10 15 actual #threads #flows@lb0 #flows@lb1 Figure 2: Comparison of the observed number of on-going flows (#flow) between single- (top) and double-load-balancer (bottom) environment. The network topology of load balanced system in real-world DCs is depicted in Figure 1. With the advancement of virtualisation technology and elastic data centers [8], application servers can be instantiated on heterogeneous architectures [15] and have differ- ent processing capacities. This requires network LBs to make fair workloads distribution decisions based on instant server load states to optimise resource utilisation, so that less application servers can be provisioned to provide better quality of service (QoS)–lower job completion time (JCT)–with reduced operational costs. However, there are 2 challenges for network LBs to make fair workloads distribution decisions in real-world systems. Network LBs have limited observations to make informed decisions. Operating at the Transport Layer, network LBs are agnostic to Application-Layer protocol and do not inspect the Application-Layer headers or payloads in network packets, in order to generalise to, and stay universal for all types of network applica- tions [9]. However, this makes LBs also agnostic to the information of received requests and network flows (jobs)–e.g., expected JCT, computation intensity, required database–when making load bal- ancing decisions, which can lead to overloaded servers dealing with arXiv:2201.11727v4  [cs.DC]  19 Aug 2022CIKM ’22, October 17–21, 2022, Atlanta, GA, USA. Zhiyuan Yao, Zihan Ding, and Thomas Clausen Number of Input Features Processing Time (ms)Processing Time (ms) 0 50 100 150 200 250 Number of Input Features 32 64 128 256 512 1024 2048 Figure 3: With a 3 layer fully connected neural network with 4865 parameters built with Keras/Tensorflow, assuming in- put datapoints have 32 features, it takes more than 50ms to obtain a singal output, when using single CPU core (Intel Xeon CPU E5-2690 v3 at 2.60GHz). multiple heavy network flows [11]. Besides, to avoid single-point- of-failure and improve system reliability, multiple LBs are deployed in modern DCs so that the service stays available when a LB fails. To improve workload distribution fairness, heuristic LBs [11] make informed load balancing decisions based on simple features (e.g., the number of on-going connections) extracted from network packets. However, the presence of multiple LBs makes them have only par- tial observations on the network traffic and workloads distributed among servers. An example of partial observation of the number of on-going flows is depicted in Figure 2. In presence of 2 LBs at the same time, the counted number of flows (#flow) no longer reflect the actual number of busy threads (#threads) on the server. This indicates that more features–especially those that are not affected by partial observations (e.g., latency-related features)–should be taken into account when making load balancing decisions. Network LBs deal with high flow arrival rates (higher than 500 flows/s)–load balancing decisions have to be made within sub-ms or micro-second level [7] . Machine learning (ML) and reinforcement learning (RL) approaches are able to make inferences and informed decisions based on multi-modal features extracted from dynamic environments, and they have shown performance gains in various system and networking problems and help avoid error-prone manual configurations [1, 4, 17, 24, 26]. However, Fig- ure 3 shows that it is computationally intractable to apply ML/RL algorithms on network load balancing problems to make more than 500 load balancing decisions per second even using just a minimal size of neural network. Therefore, state-of-the-art network LBs rely on heuristics for decisions on where to place workloads [2, 7, 10]. This paper formulates the network load balancing problem as a cooperative game to bridge the gap between the networked com- puting system community and MARL community. A new load balancing method RLB is proposed, allowing both (i) taking advan- tage of the learning and inference capacity of RL algorithms given only partial observations, and (ii) making–at micro-second level– data-driven load balancing decisions that bring performance gains. Experimental evaluations are conducted by running real-world net- working traffic to compare and contrast the proposed mechanism with state-of-the-art (SOTA) load balancing algorithms. The contribution of this paper are summarised as follows: (i) This paper formally defines the network load balancing problem as a cooperative game in Dec-POMDP [20] framework and presents a real-world system implementation for realistic performance evalu- ations. (ii) This paper proposes a new mechanism that allows bene- fiting from MARL algorithms and make sub-ms level data-driven load balancing decisions. (iii) This paper implements 3 different learning agents–one based on the MARL algorithm QMIX [21] and two other based on independent agents using RL algorithm soft actor-critic (SAC) [12] for solving load balancing tasks with mul- tiple LBs, and evaluates their performance on different scenarios, and compares with 5 SOTA heuristic load balancing algorithms. (iv) By analysing the experimental results, this paper implies the poten- tial challenges (e.g. scalability, synchronisation among distributed agents) for MARL to solve the network load balancing problem, and suggests future work directions. 2 RELATED WORK Network Load Balancing: Network LBs in modern DCs follow the distributed design as in [10], where multiple LBs randomly as- sign servers to incoming tasks using Equal-Cost Multi-Path (ECMP). In case where servers have different processing capacities, servers are assigned with probabilities proportional to their weights using Weighted-Cost Multi-Path (WCMP). However, as available server processing capacities change with time in DCs [8], these statically configured weight may not correspond to the actual processing capacities of servers, and they fail to follow and adapt to the dy- namic networking environment [ 1, 11]. As a variant of WCMP, active WCMP (AWCMP) periodically updates server weights us- ing regression algorithm [1] or threshold-based algorithm [2], by actively probing resource utilisation information (CPU, memory, or IO usage). However, AWCMP requires modifications on every servers to manage communication channels that allow collecting observations. This incurs additional control messages and man- agement overheads, especially in large and scalable DCs. Local shortest queue (LSQ) counts the number of distributed jobs on each server [11], yet it assumes that all servers share the same processing capacity. The proposed method in this paper requires no modifi- cation in the distributed system, yet it is able to passively extract features from network traffic, which help infer server load states and make adaptive and fair load balancing decisions. MARL for Cooperative Games in Networked Computing Systems: RL has been applied on scheduling problems [4, 17, 25], which is similar yet different from network load balancing prob- lems. Agents in scheduling problems know a priori the information of workloads–including the expected job durations, job dependen- cies, etc. –to be distributed before assigning workloads to different processing queues. However, network LBs have limited observa- tions on only the subset of network flows they distribute–only the number of flows distributed on each server and the elapsed time of on-going network flows. Comparing with the networking problems, the jobs in scheduling problems also arrive at lower rates (at second level) and have longer completion time. The inaccurate observation and highly-frequent decision making process make network load balancing a challenging problem to solve. Previous work [28] has ex- plored single-agent RL for load balacing in a network systems, withMulti-Agent Reinforcement Learning for Network Load Balancing in Data Center CIKM ’22, October 17–21, 2022, Atlanta, GA, USA. a close-to SOTA heuristic performance evaluated in simulations. For the case with multiple LBs working at the same time, this paper ap- plies and studies MARL algorithms on the network load balancing problem in real-world systems to evaluate server load states based only on local observations extracted from network packets, and to dynamically adapt to time-variant environments. Researches on centralised training, decentralised execution (CTDE) frameworks show performance gain in multi-agent setups [13, 21, 27]. [13] as- sumes that agents are homogeneous and therefore interchangable, which does not apply in DC networks where load balancers can be deployed on different hardware infrastructure with topologically different distances from servers. This paper adapts QMIX [ 21]– which achieves similar performance as in [27]–in an asynchronous mechanism to make highly frequent load balancing decisions, and to learn from partial observations and solve the network load bal- ancing problem as a cooperative game. RL-based algorithm has also been applied on other load balancing problems [3, 14, 16, 19]. However, the network load balancing problem studied in this paper is different from link load balancing problems studied in [14, 16], where link utilisation is to be maximised and load balancers have observations on link utilisations. As discussed in [30], in network load balancing problems–more precisely, Layer-4 server load bal- ancing problem is studied in this paper–load balancers have no direct observation on server utilisations. The network load balanc- ing problem studied in this paper is also different from mobility load balancing problem studied in [3, 19], where load balancers operate in celular networks instead of DCs and they do not correspond to all resourcese. The action and optimisation goal (link failure) are entirely different as well. Furthermore, these works [3, 14, 16, 19] conduct evaluations based on simulations while this paper imple- ments and evaluates MARL-based load balancing algorithms in a real-world testbed. 3 MARL NETWORK LOAD BALANCING This section defines the network load balancing problem and for- mulates it into a cooperative game. 3.1 Multi-Agent Load Balancing Problem Network load balancing can be defined as allocating a Poisson sequence of network flows with different workloads 𝑤 ∈ W– whose unit can be, e.g., amount of time to process–on a set of 𝑛 servers, to achieve the maximal utilisation of the computational capacity of all servers. The workload 𝑤𝑗(𝑡)assigned on the 𝑗-th server at time 𝑡 follows an exponential distribution in practical experiments [22]. Multi-agent load balancing problem considers workload distribution through a number of 𝑚LBs, which provide high availability and reliability in modern DCs [10]. Deterministic Case. The load balancing method for each LB 𝑖 ∈[𝑚]can be a pure strategy 𝜋𝑖 ∈Π𝑖: W→[ 𝑛]. Therefore, a deterministic workload assignment function is: W×[ 𝑚]→[ 𝑛]. The ensembled policy for the whole multi-agent load balancing system is thus 𝝅 = [𝜋1,...,𝜋 𝑚] ∈𝚷 = Π1 ×···× Π𝑚. The processing speed for each server is 𝑣𝑗,𝑗 ∈ [𝑛], i.e., the amount of workloads that can be processed per unit time. The remaining workloads on the 𝑗-th server (𝑗 ∈[𝑛]) during a time interval 𝑡 ∈ [𝑡0,𝑡𝑛)is thus: 𝑙𝑗 = Í 𝑖∈[𝑚] Í 𝑡∈[𝑡0,𝑡𝑛)𝑤𝑖,𝑗 (𝑡) 𝑣𝑗 , (1) where 𝑤𝑖,𝑗 (𝑡)indicates the workload at time 𝑡 assigned to the 𝑗-th server via the 𝑖-th LB. 𝑙𝑗 represents the expected time to finish processing all the workloads on the 𝑗-th server. Stochastic Case. Since modern DCs have fan-out topology and 𝑚 < 𝑛, using deterministic strategy during a time interval will flood 𝑛 servers under heavy traffic rate ( e.g., higher than 500 flows/s), therefore the stochastic load balancing strategies are more often used in practice [10, 18]. The stochastic workload assignment func- tion 𝛼 is defined as: W×[ 𝑚]×[ 𝑛] → [0,1], representing the probability of the event that the workload is assigned by a specific LB to a specific server. The expected time to finish all workloads on 𝑗-th server during the time interval 𝑡 ∈[𝑡0,𝑡𝑛)is, ∀𝑗 ∈[𝑛]: 𝑙𝑗 = Í 𝑖∈[𝑚] Í 𝑡∈[𝑡0,𝑡𝑛)𝑤𝑖(𝑡)𝛼𝑖,𝑗 (𝑡) 𝑣𝑗 , 𝑛∑︁ 𝑗=1 𝛼𝑖,𝑗 (𝑡)= 1, (2) 𝛼𝑖,𝑗 (𝑡)denoting the probability that LB 𝑖chooses server 𝑗 for load 𝑤 at time 𝑡. Objective. The objective for the whole load balancing system can be defined as finding the optimal ensemble policy: 𝝅∗= min 𝝅 ∈𝚷 𝑐(𝒍),𝒍 = {𝑙𝑗},𝑗 ∈[𝑛] (3) where 𝑐 is a cost function depending on the expected task finish- ing time for all servers 𝑗 ∈ [𝑛]. The definition of makespan is 𝑐(𝑙1,...,𝑙 𝑗)= max𝑗∈[𝑛]{𝑙𝑗}. However, in the practical network load balancing problem, LB agents have no observation over the theoretical makespan for the following reasons: (1) operating at the Transport Layer, LB agents are agnostic to application-level information, thus they cannot estimate the remaining workload on each server but counting only the amount of ongoing jobs; (2) the expected JCT of networking requests follow long-tail dis- tribution [22], which makes it difficult to estimate remaining workload only based on the number of ongoing jobs; (3) in multi-agent setups for the sake of reliability, LB agents only observe partial networking traffic, which makes their counted number of ongoing jobs partial and inaccurate; (4) the estimation of makespan uses the max operator, which may produce large variances when facing dynamic traffic. This paper thus proposes a different cost function–fairness index– which is proved to be equivalent of the makespan as objective. Definition 3.1. (Fairness) For a vector of task completion time 𝒍 = [𝑙1,...,𝑙 𝑛]on each server 𝑗 ∈[𝑛], the linear product-based fairness for workload distribution is defined as: 𝐹(𝒍)= 𝐹([𝑙1,...,𝑙 𝑛])= Ö 𝑗∈[𝑛] 𝑙𝑗 max(𝒍) (4) Proposition 3.2. Maximising the linear product-based fairness is sufficient for minimising the makespan: max 𝐹(𝒍)⇒ min max(𝒍) (5)CIKM ’22, October 17–21, 2022, Atlanta, GA, USA. Zhiyuan Yao, Zihan Ding, and Thomas Clausen Proof. For a vector of task completion time 𝒍 = [𝑙1,...,𝑙 𝑛]on each server 𝑗 ∈[𝑛], by the definition of fairness, max 𝐹(𝒍)= max Î 𝑗∈[𝑛]𝑙𝑗 max𝑘′∈[𝑛]𝑙𝑘′ (6) WLOG, let 𝑙𝑘 = max𝑘′∈[𝑛]𝑙𝑘′, then, max 𝐹(𝒍)= max Ö 𝑗∈[𝑛],𝑗≠𝑘 𝑙𝑗 (7) By means inequality, ©­ « Ö 𝑗∈[𝑛],𝑗≠𝑘 𝑙𝑗 ª® ¬ 1 𝑛−1 ≤ Í 𝑗∈[𝑛],𝑗≠𝑘𝑙𝑗 𝑛−1 = 𝐶−𝑙𝑘 𝑛−1 ,𝐶 = ∑︁ 𝑗∈[𝑛] 𝑙𝑗. (8) with the equivalence achieved when 𝑙𝑖 = 𝑙𝑗,∀𝑖,𝑗 ≠ 𝑘,𝑖,𝑗 ∈ [𝑛] holds. Therefore, max 𝐹(𝒍)⇒ max 𝐶−𝑙𝑘 𝑛−1 (9) ⇔min𝑙𝑘 (10) ⇔min max 𝑗∈[𝑛] 𝑙𝑗 (11) The inverse may not hold sincemax 𝐶−𝑙𝑘 𝑛−1 does not indicatemax 𝐹(𝒍), so maximising the linear product-based fairness is sufficient but not necessary for minimising the makespan. This finishes the proof. □ To sum up, the network load balancing can be formulated as a constrained optimisation problem: 𝑚𝑎𝑥𝑖𝑚𝑖𝑠𝑒 Ö 𝑗∈[𝑛] 𝑙𝑗 max(𝒍) (12) 𝑠𝑢𝑏𝑗𝑒𝑐𝑡𝑡𝑜 𝑙 𝑗 = Í 𝑖∈[𝑚] Í 𝑡∈[𝑡0,𝑡𝑛)𝑤𝑖(𝑡)𝛼𝑖,𝑗 (𝑡) 𝑣𝑗 (13) 𝑛∑︁ 𝑗=1 𝛼𝑖,𝑗 (𝑡)= 1 (14) 𝑚∑︁ 𝑖=1 𝑤𝑖 ≤ 𝑛∑︁ 𝑗=1 𝑣𝑗 (15) 𝛼𝑖,𝑗 ∈[0,1],𝑤𝑖,𝑣𝑗 ∈(0,+∞). (16) The optimisation cost 𝑐 in Eq. (3) is transformed to be the product- based fairness 𝐹(𝒍)due to the Proposition 3.2. Constraints (13) and (14) are from Eq. (2) for stochastic network load balancing. Constraint (15) is a necessary condition to have bounded queue backlog (stability). 3.2 MARL Methods The multi-agent load balancing problem defined in Eq.(12)-(16) can be viewed as a multi-agent cooperative game, where each agent needs to coordinate their behaviour to maximise the common payoff. Specifically, each agent acts independently according to their local observations, the common payoff is improved as long as each agent improves their local policies. Dec-POMDP: MARL for cooperative games can be formulated as decentralised partially observable Markov decision process (Dec- POMDP) [20], which can be represented as (I,S,A,𝑅, O,T,𝛾). I is the agent set, Sis the state set and A= ×𝑖A𝑖,𝑖 ∈I is the joint action set, O= ×𝑖O𝑖,𝑖 ∈I is the joint observation set, and 𝑅is the global reward function 𝑅(𝑠,𝑎): S×A→ R for current state 𝑠 ∈S and action 𝒂 ∈A. The state-transition probability from current state and action to a next state 𝑠′ ∈ Sis defined by T(𝑠′|𝑠,𝒂): S×A×S→[ 0,1]. 𝛾 ∈(0,1)is a reward discount factor. The goal of the RL algorithm is optimising the joint policy𝝅 ∈𝚷 to maximise their expected cumulative rewards: max𝝅 ∈𝚷 E𝝅 [Í 𝑡𝛾𝑡𝑟𝑡]. To solve the above Dec-POMDP problem, this paper implements and compares 3 different RL schemes: (i) CTDE, (ii) centralised training and execution (single agent), and (iii) independent agents. QMIX: QMIX [21] algorithm is implemented in the proposed method, in a CTDE manner. Specifically, QMIX estimates a total 𝑄- value function 𝑄𝑡𝑜𝑡 as a nonlinear combination of the 𝑄𝑖-value for each agent 𝑖 ∈I, as long as the monotonic dependence relationship is satisfied: 𝜕𝑄𝑡𝑜𝑡 𝜕𝑄𝑖 ,∀𝑖 ∈[I]. 𝑄𝑡𝑜𝑡(𝝉,𝒂,𝑠)is a function of joint action- observation history 𝝉, joint action 𝒂 and the state 𝑠, while 𝑄𝑖(𝜏𝑖,𝑎𝑖) for each agent is a function of agent observed history 𝜏𝑖 and its own action 𝑎𝑖. The update rule of QMIX follows: min 𝐿= min ∑︁ [𝑄𝑡𝑜𝑡(𝝉,𝒂,𝑠)−( 𝑟 +𝛾max 𝒂′ 𝑄𝑡𝑜𝑡(𝝉′,𝒂′,𝑠′))]2. Each LB agent using QMIX algorithm has a stochastic policy. SAC: For single-agent game, soft actor-critic (SAC) [ 12] fol- lows the maximum entropy reinforcement learning framework, which optimises the objective E[Í 𝑡𝛾𝑡𝑟𝑡 +𝛼H(𝜋𝜃)]to encour- age the entropy H(·) of the policy 𝜋𝜃 during the learning pro- cess. Specifically, the critic 𝑄 network is updated using the gradi- ents ∇𝜙E𝑠,𝑎 \u0014\u0012 𝑄𝜙(𝑠,𝑎)−𝑅(𝑠,𝑎)−𝛾E𝑠′[𝑉˜𝜙(𝑠′)] \u00132\u0015 , where 𝑉˜𝜙(𝑠′)= E𝑎′[𝑄˜𝜙(𝑠′,𝑎′)− 𝛼log 𝜋𝜃(𝑎′|𝑠′)]and 𝑄˜𝜙 is the target 𝑄 network; the actor policy 𝜋𝜃 is updated using ∇𝜃E𝑠[E𝑎∼𝜋𝜃 [𝛼log 𝜋𝜃(𝑎|𝑠)− 𝑄𝜙(𝑠,𝑎)]]. Single-agent SAC (S-SAC) method is implemented as the second RL scheme. Independent Learning. Apart from QMIX, independent learn- ing agents treat the objective in Eq. (3) from an independent view, where the optimal ensemble policy is factorised as the optimization over each individual policy for each LB agent: 𝜋∗ 𝑖 = min 𝜋𝑖 ∈Π𝑖,𝑖∈[𝑚] 𝑐(𝑙𝑖),𝑖 ∈[𝑚] (17) where 𝑙𝑖 = {𝑙𝑖,𝑗},𝑙𝑖,𝑗 = Í 𝑡∈[𝑡0,𝑡𝑛 )𝑤𝑖 (𝑡)𝛼𝑖,𝑗 (𝑡) 𝑣𝑗 ,𝑗 ∈ [𝑛]for the sto- chastic case. The objective achieved with Eq. (17) is different from Eq. (3) unless the workloads going to each LB are the same at all time, which is impossible in practice. SAC is used for each indepen- dent LB agent, which gives the independent-SAC (I-SAC) method. 3.3 MARL for Multi-Agent Load Balancing The network load balancing problem belongs to multi-commodity flow problems and is NP-hard, which makes it hard to solve with trivial heuristic solution at a micro-second level speed [23]. In real- world systems, limited observations on system states and chang- ing environments require LB agents to continuously approximate server load states. This section describes the network load balancing problem mathematically as a cooperative Dec-POMDP under real- istic constraints. The overview of the MARL framework is depicted in Figure 4. The pseudo-code of MARL framework for network load balancing is shown in Algorithm. 1.Multi-Agent Reinforcement Learning for Network Load Balancing in Data Center CIKM ’22, October 17–21, 2022, Atlanta, GA, USA. Observations Server State Estimation Approximation Function Time SYN FIN ACK ACK ACK tt+1 SYN Data Plane <latexit sha1_base64=\"WojlqXM8lbbo14WHG1s0Hiv9V1c=\">AAACAHicbVDLSsNAFJ34rPUVdeHCzWARXJWkILosqOCyon1AG8pkctMOnUzCzEQMoRt/xY0LRdz6Ge78G6dtFtp6YOBwzr3cOcdPOFPacb6tpeWV1bX10kZ5c2t7Z9fe22+pOJUUmjTmsez4RAFnApqaaQ6dRAKJfA5tf3Q58dsPIBWLxb3OEvAiMhAsZJRoI/Xtw7thLDUoja8fE6AaAnwFnGR9u+JUnSnwInELUkEFGn37qxfENI1AaMqJUl3XSbSXE6kZ5TAu91IFCaEjMoCuoYJEoLx8GmCMT4wS4DCW5gmNp+rvjZxESmWRbyYjoodq3puI/3ndVIcXXs5EkmoQdHYoTDnWMZ60gQMmTWieGUKoZOavmA6JJKYIqcqmBHc+8iJp1aruWdW5rVXq9aKOEjpCx+gUuegc1dENaqAmomiMntErerOerBfr3fqYjS5Zxc4B+gPr8wd3jZZO</latexit> Shortest Expected Delay <latexit sha1_base64=\"z+fBBIBzjwTBD3MJB1PyQCDe/V4=\">AAACF3icbVDLSsNAFJ34rPUVdelmsBUqQkm6UJcFNy4r2AckIUymk3baySTOTIQS8hdu/BU3LhRxqzv/xmmbhbYeuHA4517uvSdIGJXKsr6NldW19Y3N0lZ5e2d3b988OOzIOBWYtHHMYtELkCSMctJWVDHSSwRBUcBINxhfT/3uAxGSxvxOTRLiRWjAaUgxUlryzXrVRWIA3YhyPxu5lDvcy6EbCoSzez+jOTyHdp4hf1RTZ3nVNytW3ZoBLhO7IBVQoOWbX24/xmlEuMIMSenYVqK8DAlFMSN52U0lSRAeowFxNOUoItLLZn/l8FQrfRjGQhdXcKb+nshQJOUkCnRnhNRQLnpT8T/PSVV45WWUJ6kiHM8XhSmDKobTkGCfCoIVm2iCsKD6VoiHSGeidJRlHYK9+PIy6TTq9kXdvm1Ums0ijhI4BiegBmxwCZrgBrRAG2DwCJ7BK3gznowX4934mLeuGMXMEfgD4/MHqgSe9Q==</latexit> arg min j 2 [ n ] q i +1 a j ( t ) <latexit sha1_base64=\"2kDNkaL5YPQjkqFNxw0geKZ1FBc=\">AAAB+nicbVBNS8NAEN3Ur1q/Uj16WSyCp5L0oB4rXrwIFewHtKFstpN06WYTdjdKif0pXjwo4tVf4s1/4zbNQVsfDDzem2Fmnp9wprTjfFultfWNza3ydmVnd2//wK4edlScSgptGvNY9nyigDMBbc00h14igUQ+h64/uZ773QeQisXiXk8T8CISChYwSrSRhnb1NpaAr8JQQphLamjXnLqTA68StyA1VKA1tL8Go5imEQhNOVGq7zqJ9jIiNaMcZpVBqiAhdEJC6BsqSATKy/LTZ/jUKCMcxNKU0DhXf09kJFJqGvmmMyJ6rJa9ufif1091cOllTCSpBkEXi4KUYx3jeQ54xCRQzaeGECqZuRXTMZGEapNWxYTgLr+8SjqNunted+8atWaziKOMjtEJOkMuukBNdINaqI0oekTP6BW9WU/Wi/VufSxaS1Yxc4T+wPr8ARnMk+U=</latexit> More Aggregations <latexit sha1_base64=\"VnKK48EGPRRHqBxBcCNYgiJr7FQ=\">AAACB3icbVDLSsNAFJ3UV62vqktBBovgooSkCx+7ii5cVrSt0IYymdy0QycPZiaFELpz46+4caGIW3/BnX/jpO1CWw9cOHPOvcy9x405k8qyvo3C0vLK6lpxvbSxubW9U97da8koERSaNOKReHCJBM5CaCqmODzEAkjgcmi7w6vcb49ASBaF9yqNwQlIP2Q+o0RpqVc+vBz1zSq+U941jKr4woqr2ANKUvDyR69csUxrArxI7BmpoBkavfJX14toEkCoKCdSdmwrVk5GhGKUw7jUTSTEhA5JHzqahiQA6WSTO8b4WCse9iOhK1R4ov6eyEggZRq4ujMgaiDnvVz8z+skyj93MhbGiYKQTj/yE45VhPNQsMcEUMVTTQgVTO+K6YAIQpWOrqRDsOdPXiStmmmfmvZtrVKvz+IoogN0hE6Qjc5QHd2gBmoiih7RM3pFb8aT8WK8Gx/T1oIxm9lHf2B8/gAsFJb0</latexit> Avg., StdDev, 90p, decayed 90p <latexit sha1_base64=\"j2oFul/gA5UkN9AY/kXhq6MGFdQ=\">AAAB+3icbVC7TsMwFHV4lvIKZWSxqJCYqqQDMBbBwFgk+pDaqHKcm9aqY0e2g6ii/goLAwix8iNs/A1umwFajmTp6Jxz7esTppxp43nfztr6xubWdmmnvLu3f3DoHlXaWmaKQotKLlU3JBo4E9AyzHDopgpIEnLohOObmd95BKWZFA9mkkKQkKFgMaPEWGngVm6ZpjITBiJ8bYNkCAO36tW8OfAq8QtSRQWaA/erH0maJSAM5UTrnu+lJsiJMoxymJb7mYaU0LG9u2epIAnoIJ/vPsVnVolwLJU9wuC5+nsiJ4nWkyS0yYSYkV72ZuJ/Xi8z8VWQM5FmBgRdPBRnHBuJZ0XgiCmghk8sIVQxuyumI6IINbausi3BX/7yKmnXa/5Fzb+vVxuNoo4SOkGn6Bz56BI10B1qohai6Ak9o1f05kydF+fd+VhE15xi5hj9gfP5A9hklE4=</latexit> Discounted Average <latexit sha1_base64=\"VHLANBWBmirZ1ICsAQQyFFTNvXE=\">AAACPHicbVBNaxsxFNQ6bZO6TeM0x1xE7UILjdn1IR+HgiGXQi4uqT/Aay9aWRsr1moX6W3ACP2wXPojeuuplx5SSq89R2u7kNgdEIxm3iC9iXPBNfj+d6+y9eTps+2d59UXL3df7dX2X/d0VijKujQTmRrERDPBJesCB8EGuWIkjQXrx7Pz0u/fMKV5Jr/APGejlFxJnnBKwElR7bIRZs4v4yYEUtjIXFv8EYeJItQE1lzYUBdpZGY45BIP/Q/44r31m2djA0dQDo/NzNoy+u/SiGp1v+kvgDdJsCJ1tEInqn0LJxktUiaBCqL1MPBzGBmigFPBbDUsNMsJnZErNnRUkpTpkVksb/Fbp0xwkil3JOCF+jBhSKr1PI3dZEpgqte9UvyfNywgOR0ZLvMCmKTLh5JCYMhw2SSecMUoiLkjhCru/orplLjawPVddSUE6ytvkl6rGRw3g8+teru9qmMHHaI36B0K0Alqo0+og7qIolv0A92hX95X76f32/uzHK14q8wBegTv7z3MQa8Q</latexit> ⌧ j = 1 K P k 2 [0 ,K ) 0 . 9 t \u0000 t k j ⌧ k j <latexit sha1_base64=\"hHfNEirhywyD+/uLKcrcdHXHTu0=\">AAAB+3icbVDLSgNBEJyNrxhfazx6GQyCp7AbED0GvHiMjzwgWcLspDcZMjO7zMwGw5Jf8eJBEa/+iDf/xkmyB00saCiquunuChPOtPG8b6ewsbm1vVPcLe3tHxweucfllo5TRaFJYx6rTkg0cCahaZjh0EkUEBFyaIfjm7nfnoDSLJaPZppAIMhQsohRYqzUd8v3oEFNYqbwAxF2oxz23YpX9RbA68TPSQXlaPTdr94gpqkAaSgnWnd9LzFBRpRhlMOs1Es1JISOyRC6lkoiQAfZ4vYZPrfKAEexsiUNXqi/JzIitJ6K0HYKYkZ61ZuL/3nd1ETXQcZkkhqQdLkoSjk2MZ4HgQdMATV8agmhitlbMR0RRaixcZVsCP7qy+ukVav6l1Xvrlap1/M4iugUnaEL5KMrVEe3qIGaiKIn9Ixe0Zszc16cd+dj2Vpw8pkT9AfO5w8gIZR7</latexit> Reservoir Sampling <latexit sha1_base64=\"QkSczIGoV/OpUxQZMJV9cOgd9yw=\">AAACBHicbVC7SgNBFJ2Nrxhfq5ZpBhMhgoTdFGoZsLGMYB6QrMvsZJKMmX0wc1cIyxY2/oqNhSK2foSdf+MkWUETD1w4c869zL3HiwRXYFlfRm5ldW19I79Z2Nre2d0z9w9aKowlZU0ailB2PKKY4AFrAgfBOpFkxPcEa3vjy6nfvmdS8TC4gUnEHJ8MAz7glICWXLNYroCb3KW3yTg9xT0g8c/rpOyaJatqzYCXiZ2REsrQcM3PXj+ksc8CoIIo1bWtCJyESOBUsLTQixWLCB2TIetqGhCfKSeZHZHiY6308SCUugLAM/X3REJ8pSa+pzt9AiO16E3F/7xuDIMLJ+FBFAML6PyjQSwwhHiaCO5zySiIiSaESq53xXREJKGgcyvoEOzFk5dJq1a1z6r2da1Ur2dx5FERHaEKstE5qqMr1EBNRNEDekIv6NV4NJ6NN+N93pozsplD9AfGxzcajJe+</latexit> ( t k j , ⌧ k j ) <latexit sha1_base64=\"98EkL9D3hbeZ6jmSbJl1OTC5uLI=\">AAAB9HicbVA9SwNBEJ3zM8avqKXNYhCswl0KtQxoYRkhX5AcYW6zlyzZ2zt39wLhyO+wsVDE1h9j579xk1yhiQ8G3r43w868IBFcG9f9djY2t7Z3dgt7xf2Dw6Pj0slpS8epoqxJYxGrToCaCS5Z03AjWCdRDKNAsHYwvpv77QlTmseyYaYJ8yMcSh5yisZKfgP1mNynavHql8puxV2ArBMvJ2XIUe+XvnqDmKYRk4YK1LrruYnxM1SGU8FmxV6qWYJ0jEPWtVRixLSfLZaekUurDEgYK1vSkIX6eyLDSOtpFNjOCM1Ir3pz8T+vm5rw1s+4TFLDJF1+FKaCmJjMEyADrhg1YmoJUsXtroSOUCE1NqeiDcFbPXmdtKoV77riPVbLNTePowDncAFX4MEN1OAB6tAECk/wDK/w5kycF+fd+Vi2bjj5zBn8gfP5A423kec=</latexit> Task Duration <latexit sha1_base64=\"e0yw7j26K96zUKKXt5NbYgIenJo=\">AAACA3icbVA9SwNBEN2LXzF+ndpps5gIVuEuhVoGbBQsEjAfkISwt5kkq3t75+6cEELAxr9iY6GIrX/Czn/j5qPQxAcDj/dmmJkXxFIY9LxvJ7W0vLK6ll7PbGxube+4u3tVEyWaQ4VHMtL1gBmQQkEFBUqoxxpYGEioBXcXY7/2ANqISN3gIIZWyHpKdAVnaKW2e3ClDDKFtJxAAvQaVA/7NHffvs213ayX9yagi8SfkSyZodR2v5qdiCchKOSSGdPwvRhbQ6ZRcAmjTDMxEDN+x3rQsFSxEExrOPlhRI+t0qHdSNuy50zU3xNDFhozCAPbGTLsm3lvLP7nNRLsnreGQsUJguLTRd1EUozoOBDaERo4yoEljGthb6W8zzTjaGPL2BD8+ZcXSbWQ90/zfrmQLXqzONLkkByRE+KTM1Ikl6REKoSTR/JMXsmb8+S8OO/Ox7Q15cxm9skfOJ8/nwCWyw==</latexit> Instant Queue Length q j Multi-Buffering I-SAC / QMix / S-SAC … <latexit sha1_base64=\"GQWSAzpBu2I8HmKUyGSFNHcaz5c=\">AAAB+HicbZC7TsMwFIZPuJZyaYCRxaJFKkuVdADGIhbGItGL1EaR4zqtqeNEtoNUqj4JCwMIsfIobLwNTpsBWn7J0qf/nKNz/AcJZ0o7zre1tr6xubVd2Cnu7u0flOzDo7aKU0loi8Q8lt0AK8qZoC3NNKfdRFIcBZx2gvFNVu88UqlYLO71JKFehIeChYxgbSzfLl2TDFAF+w9VfV7x7bJTc+ZCq+DmUIZcTd/+6g9ikkZUaMKxUj3XSbQ3xVIzwums2E8VTTAZ4yHtGRQ4osqbzg+foTPjDFAYS/OERnP398QUR0pNosB0RliP1HItM/+r9VIdXnlTJpJUU0EWi8KUIx2jLAU0YJISzScGMJHM3IrICEtMtMmqaEJwl7+8Cu16zb2ouXf1cqORx1GAEziFKrhwCQ24hSa0gEAKz/AKb9aT9WK9Wx+L1jUrnzmGP7I+fwD035H4</latexit> Action a j ( t ) Experience Replay Generated Transitions Mini-Batch Transitions Load Balancer Per-Flow Decision <latexit sha1_base64=\"7CHR91vPfw7e8qq2jz1HuY4Oa8Q=\">AAACCHicbZDLSgMxFIYzXmu9jbp0YbAVKpQy04W6LLhxWcFeoB1LJj3ThiYzQ5IRytClG1/FjQtF3PoI7nwb03YW2vpD4Mt/ziE5vx9zprTjfFsrq2vrG5u5rfz2zu7evn1w2FRRIik0aMQj2faJAs5CaGimObRjCUT4HFr+6Hpabz2AVCwK7/Q4Bk+QQcgCRok2Vs8+KZZUGZMylmWs7ruxZALMPaPzYs8uOBVnJrwMbgYFlKnes7+6/YgmAkJNOVGq4zqx9lIiNaMcJvluoiAmdEQG0DEYEgHKS2eLTPCZcfo4iKQ5ocYz9/dESoRSY+GbTkH0UC3WpuZ/tU6igysvZWGcaAjp/KEg4VhHeJoK7jMJVPOxAUIlM3/FdEgkodpklzchuIsrL0OzWnEvKu5ttVBzsjhy6BidohJy0SWqoRtURw1E0SN6Rq/ozXqyXqx362PeumJlM0foj6zPHwI+l2Q=</latexit> ( s, a, r, s 0 ,a 0 ) Figure 4: Overview of the proposed MARL framework for network LB: A distributed learning framework with multiple LB agents is implemented to interact with network LB devices and allocate tasks on different servers. Each LB agent contains a replay buffer and can learn using 1 of the 3 different RL algorithms–independent SAC (I-SAC), QMix, or single-agent SAC (S-SAC). The 3 RL algorithms consume the network features (on-going flows and flow duration statistics on each server) as well as the actions from last time step, and they generate server load state estimations as the next time-step action for making fair per-flow-level decision based on the shortest expected delay algorithm. Algorithm 1 MARLLB 1: Initialise: 2: replay buffer B 3: learning agents parameterised by 𝜽 = {𝜃𝑖},∀𝑖 ∈[𝑚] 4: reinforcement learning algorithm P 5: server processing speed function 𝑣𝑗,∀𝑗 ∈[𝑛] 6: initial observed instant queue length on server 𝑘 by the 𝑖-th LB: 𝑞𝑖,𝑘 = 0,∀𝑖 ∈[𝑚],𝑘 ∈[𝑛] 7: while not converge do 8: Reset: 9: server load state 𝑋𝑗(1)← 0,∀𝑗 ∈[𝑛] 10: observations 𝒐(1)for LB agents 11: for 𝑡 = 1,...,𝑁 do 12: for LB agent 𝑖do 13: 𝑤𝑖,𝑗 (𝑡)← 0,∀𝑗 ∈[𝑛] 14: 𝑎𝑖(𝑡)←{ 𝑎𝑖,𝑗 (𝑡)}𝑛 𝑗=1 = 𝜋𝜃𝑖 (𝒐𝑖(𝑡)) 15: for job ˜𝑤 arrived at LB 𝑖between timestep [𝑡, 𝑡+1) do 16: LB 𝑖 assigns ˜𝑤 to server 𝑗 = arg max𝑘∈[𝑛] 𝑞𝑖,𝑘 +1 𝑎𝑖,𝑘 (𝑡) 17: 𝑤𝑖,𝑗 (𝑡)← 𝑤𝑖,𝑗 (𝑡)+ ˜𝑤 18: for each server 𝑗 do 19: 𝑋𝑗(𝑡+1)← 𝑚𝑎𝑥{𝑋𝑗(𝑡)+Í𝑚 𝑖=1 𝑤𝑖,𝑗 (𝑡)−𝑣𝑗,0} ⊲ update workload 20: Receive reward 𝑟(𝑡) 21: Collect observation 𝒐(𝑡+1) 22: B= BÐ(𝒐(𝑡),{𝑎𝑖(𝑡)},𝑟(𝑡),𝒐(𝑡+1)) ⊲ Update replay buffer 23: 𝜽 ←P(B) ⊲ Update agents with RL/MARL algorithms return 𝜽 Agent Set. There is a set of homogeneous LB agentsI(|I|= 𝑚) distributing workloads among the same set of𝑛application servers. Each agent only distributes and observes over a subset of workloads that arrive at the system. State and Observation Set. The state set is defined as S = W×V , where W = 𝒘 : 𝒘 ∈(0,∞)𝑚 is a set of incoming net- work traffic (workloads) to be distributed among servers, and V= 𝒗 : 𝒗 ∈(0,∞)𝑛 is a set of server processing speeds. The observation set O= (𝒒,𝝉): 𝒒,𝝉 ∈(0,∞)𝑛, where 𝒒 is a vector of counting num- bers, each represents the number of on-going task on each server, and 𝝉 is a vector of statistical evaluations (mean, standard devi- ation, 90th-percentile, and discounted mean and 90th-percentile over time) of task elapsed time on each server. Action Set. A= ×𝑖A𝑖 is the action set containing the individ- ual action set A𝑖 for each agent 𝑖 ∈I. In the discrete action set A𝑖 ⊂R+𝑛, an action 𝑎𝑖 is a vector representing the weights for the current workload to be allocated to 𝑛application servers by LB agent 𝑖. To make hundreds or thousands of load balancing decisions per second while incorporating RL intelligence, this paper adopts the form of the shortest expected delay (SED) 1 to assign server arg min𝑗∈[𝑛] 𝑞𝑖,𝑗 +1 𝑎𝑖,𝑗 (𝑡) to the newly arrived flow, where 𝑞𝑖,𝑗 is the number of on-going flows on server 𝑗 observed by LB 𝑖, and 𝑎𝑖,𝑗 (𝑡) is the weight assigned to server 𝑗 by LB 𝑖 at timestep 𝑡. Conventionally, LB agents make actions on receipt of each net- working requests, assigning a server to new-coming requests. How- ever, it is not possible for RL models to make such decisions under extremely high traffic rates in practice–sub-ms interval between two consecutive decisions. Using the form of the SED, this paper converts the action of LB agents from assigning servers for each request to periodically (250ms) and dynamically estimating server processing speed 𝑎𝑖,𝑗 (𝑡). By tracking the number of on-going tasks on receipt of every network flow, this allows the proposed LB agents to make load balancing decisions at the pace of task arrival rates and guarantees high-throughput, while adapting to the ever-changing and dynamic server load states and networking environments. State and Observation Transition. The state transition prob- ability function is defined as T : S×A×S → [0,1], follow- ing Markov decision process. More specifically, T(𝒔𝑡+1|𝒔𝑡,𝒂𝑡)= 𝑃𝑟(𝒔𝑡+1|𝜌(𝒔𝑡,𝒂𝑡)), where 𝒂𝑡 ∈A, 𝒔𝑡,𝒔𝑡+1 ∈S, 𝜌(𝒔𝑡,𝒂𝑡)↦→ 𝒔𝑡+𝛿𝑡 represents the response of servers given the updated workloads distribution, and 𝑃𝑟(𝒔𝑡+1|𝒔𝑡+𝛿𝑡)represents the change of incom- ing traffic rates and server processing speeds. The time interval between two actions is denoted as Δ𝑡 = 250ms and 𝛿𝑡 ≪Δ𝑡. 1http://www.linuxvirtualserver.orgCIKM ’22, October 17–21, 2022, Atlanta, GA, USA. Zhiyuan Yao, Zihan Ding, and Thomas Clausen Algorithm 2 Reservoir sampling 1: 𝐾 ←reservoir buffer size 2: 𝑝 ←probability of gathering samples 3: 𝑏𝑢𝑓 ←[(0,0),..., (0,0)] ⊲ Size of 𝐾 4: 𝑀 ←1 𝑝 5: for each observed sample 𝑣 arriving at 𝑡 do 6: 𝑟𝑎𝑛𝑑𝑜𝑚𝐼𝑑 ←𝑟𝑎𝑛𝑑() 7: if 𝑟𝑎𝑛𝑑𝑜𝑚𝐼𝑑%𝑀 == 0 then 8: 𝑖𝑑𝑥 ←𝑟𝑎𝑛𝑑𝑜𝑚𝐼𝑑%𝑁 ⊲ randomly select one index 9: 𝑏𝑢𝑓 [𝑖𝑑𝑥]←( 𝑡,𝑣) ⊲ register sample in buffer 0 1000 2000 3000 4000 5000 Sample's Timestamps (ms) 0 2 4# Buffered Samples Stored Samples Theoretical Distribution Figure 5: An example of reservoir samples’ timestamp dis- tribution with 𝜆= 80, 𝑝 = 0.05, 𝐾 = 10000, 𝑛 ≤50000. Observation Probability Function and Feature Collection. The observation probability function Ω : S×A×O→[ 0,1]de- scribes the measurement errors that can occur when extracting and collecting features and statistics from network packets on LB agents. The counters of on-going network flows 𝒒 are tracked based on connection states (e.g., identified by TCP SYN, FIN packets). These counters are subject to partial observations in presence of multiple LB agents. To reduce the impact of partial observation, this paper proposes to use the flow duration (elapsed time since the connec- tion establishment) to indicate the server load state. The intuition in behind is that, for the same service provided by the server cluster, heavy-loaded or less powerful servers yield longer flow duration than less-loaded or more powerful servers. Using reservoir sam- pling (Algorithm 2), an exponentially-distributed number of flow duration samples are collected over time. For a Poisson stream of events with an arrival rate 𝜆, the expectation of the amount of sam- ples that are preserved in buffer after 𝑇 steps is 𝐸 = 𝜆𝑝 \u0010𝐾−𝑝 𝐾 \u0011𝜆𝑇 , where 𝑝 is the probability of gathering sample and 𝑘 is the size of reservoir buffer. An example reservoir samples distribution over time is shown in Figure 5. Flow duration samples can be gainfully used to infer server load state and reduce the impact of partial observations in presence of multiple LB agents. Reward Function. Since LB agents have limited observations over the actual server load states𝒔, the paper uses the flow duration to approximate the sum of queuing delay and task workload over the underlying processing speed of a server. To give more credits to the latest observations, given the set of samples {(𝑡𝑘 𝑗,𝜏𝑘 𝑗 )|𝑘 ∈[𝐾]}of server 𝑗, the discounted average of flow elapsed time (an estimatiion of 𝑙𝑗 in Eq. (2)) at time𝑡is computed as𝜏𝑗(𝑡)= 1 𝐾 Í 𝑘∈[0,𝐾)𝛾𝑡−𝑡𝑘 𝑗 𝜏𝑘 𝑖 , where 𝛾 = 0.9 in this paper. Then, based on the Proposition 3.2, the reward function is defined as the fairness index of the exponentially weighted average of 𝝉 for all servers: 𝑟𝑡+1 = ( 𝐹(𝝉𝑡) if 𝑡 = 0 𝐹((1 −𝛾)𝝉𝑡 +𝛾𝝉𝑡+1) otherwise. (18) Clients Edge Router 4-CPU  Servers Load Balancers 2-CPU  Servers Figure 6: The moderate-scale testbed topology consisting of 1 traffic generator representing clients, an edge router, 2 LBs and 7 application servers with different processing capaci- ties. Objective Function. The objective is to maximise their ex- pected cumulative rewards: max𝝅 ∈𝚷 E𝝅 [Í 𝑡𝛾𝑡𝑟𝑡], through opti- mising over the parameterised joint policy 𝝅 ∈𝚷,𝝅 = ×𝑖∈I𝜋𝑖,𝜋𝑖 : ˜O𝑖 ×A𝑖 →[0,1]is the stochastic policy for agent 𝑖. ˜O𝑖 is a con- catenation of historical observations and actions for agent 𝑖. This paper uses gated recurrent units (GRU) [6] for both QMIX and SAC agents to handle the sequential history information. 4 IMPLEMENTATION To evaluate the performance of MARL LB algorithms in different realistic setups, experiments are conducted in a real-world system with real network traces deployed on physical servers. The experi- mental platform consists of client nodes, an edge router nodes, LB agents, and Apache HTTP servers providing Web services, virtu- alised as Kernel-based Virtual Machines (KVMs) as in real-world cloud environments, with the same topology as in Figure 6. 4.1 System Platform The KVMs are virtualised on4 UCS B200 M4 servers, each with one Intel Xeon E5-2690 v3 processor (12 physical cores and 48 logical cores), interconnected by UCS 6332 16UP fabric. Operating sys- tems are Ubuntu 18.04.3 LTS(GNU/Linux 4.15.0-128-generic x86_64). The programmable software network stackVPP v20.05 is used to implement the network layer (the data plane) of LB agents for feature collection and policy updates. The KVMs are deployed on the same layer-2 link, with statically configured routing tables. 4.2 Apache HTTP Servers Apache HTTP servers share the same VIP address on one end of GRE tunnels with the load balancer on the other end. The Apache servers usempm_prefork module to boost performance. Each server has max 32 worker threads. The TCP backlog is configured as 128. The tcp_abort_on_overflow flag is set, so that, in the Linux kernel, when the TCP connection backlog is full, a TCP RST is sent directly to signify the termination of the connection, instead of silently dropping the packet and waiting for a SYN retransmit. This configuration allows measuring flow completion time as the application response delays without taking into account additional TCP SYN retransmission delays. 4.3 24-Hour Wikipedia Replay Trace In order to evaluate MARL algorithms using real-world environ- ments, this paper creates replicas of Wikipedia servers using the instance of MediaWiki2 of version 1.30. On each application server 2https://www.mediawiki.org/wiki/DownloadMulti-Agent Reinforcement Learning for Network Load Balancing in Data Center CIKM ’22, October 17–21, 2022, Atlanta, GA, USA. Table 1: Two testbed configurations Testbed Configuration𝑛 Moderate Scale Large Scale Server Group1 4 ×2-CPU 12×4-CPU Server Group2 3 ×4-CPU 12×8-CPU LB Agents 2 ×8-CPU 6 ×8-CPU Network Trace Wikipedia Replay Poisson Traffic Traffic Rates [518.8,796.3] [391.5,436.7] JCT Distribution Real-world distribution exp(200𝑚𝑠) instance, a MySQL server and the memcached cache daemon are installed. To populate MySQL databases, this paper uses theWikiLo- ader tool and a copy of the English version of Wikipedia database. The 24-hour trace, for privacy reasons, does not contain any infor- mation that exposes user identities. 4.4 PHP for-Loop Trace Besides the 24-hour Wikipedia replay trace which is based on MySQL, a PHP for-loop script is created to study CPU-bound applications. The number of for-loop iterations #iter directly determines the expected workload for each request. The data size that is transmitted for each task is also proportional to the number of iterations, which follows an exponential distribution. This allows to generate a distribution of flow durations and transmitted bytes that preserve the long-tail characteristic of network traffic [22]. 4.5 Network Settings Two configurations are implemented to study both moderate- and large-scale DC network environments, which is noted in Table 1. In the moderate-scale configuration, network trace samples are extracted and replayed from a real-world24-hour replay [7], which consists of requests for CPU-intensive Wiki pages3 and IO-intensive static pages. In the large-scale configuration, a synthesised Poisson traffic of CPU-intensive network flows is applied. The traffic rates of the two network traces in both configurations are selected to con- sume 80% ∼95% average provisioned computational resources. 4.6 MARL Settings To apply the QMIX algorithm, the action space is discretised so that the action set for each LB agent𝑖is A𝑖 = {1.0,1.2,1.4,1.6,1.8,2.0}𝑛. QMIX follows a CTDE manner, with each LB having a𝑄network for specifying the action choice. In order to implement the centralised training of the QMIX algorithm, TCP sockets are maintained among LB agents. Through these TCP sockets, a master LB agent orches- trates the periodic process (every 250ms) of making actions and collecting observations for all LB agents to create synchronised trajectories for training. During each episode, each LB agent col- lects their locally observed system states and rewards. At the end of each episode, their collected trajectories are merged on the master LB agent for centralised training. A global reward–the mean of rewards on all LB agents–is computed for each time step as. 4.7 Benchmark LB Methods In experimental evaluations, the QMIX-based MARL (RLB-QMIX) is evaluated and compared against other methods, namely inde- pendent SAC (I-SAC) agents, single-agent SAC (S-SAC), and SOTA 3Wiki pages are identifiable by the string /wiki/index.php/ in URLs. Table 2: Hyperparameters in MARL-based LB. Hyperparameter Moderate-Scale Large-Scale Learning rate 1 ×10−3 3 ×10−4 Hidden units 128 512 Batch size 12 12 Replay Buffer Size 3000 3000 Episodes 72 72 Episode Length 60s 30s Step Interval 0.25s 0.25s Update Iterations 25 25 Target Entropy (SAC) −|A| −|A| heuristic methods including Equal-Cost Multi-Path (ECMP) [18], Weighted-Cost Multi-Path (WCMP) [10], active WCMP (AWCMP) [1], Local Shotest Queue (LSQ) [11], and SED. Among these heuristics, WCMP and SED configure server weights proportional to their provisioned CPU power. AWCMP relies on TCP channels to peri- odically probe server resource utilisation information (number of busy Apache threads) to update server load state estimation and recompute server weights. For I-SAC, each LB node has an indepen- dent SAC agent with local observations. There is no communication among LB agents. Each LB agent follows the independent learning procedure as introduced in Section 3.2. For S-SAC, a single LB node is deployed to distribute and balance all the workloads across the server cluster. This single LB agent has global observation on sys- tem states and it is trained using a SAC algorithm. The original SAC algorithm works for continuous action spaces only. Modifications are made based on [5] to support discrete action space. 4.8 Hyperparameters and Training Details The hyperparameters for each learning agent (QMIX, I-SAC, S- SAC) and different experimental setups are provided in Table 2. RL-based load balancing methods are trained in both moderate- and large-scale testbed setups for 72 episodes. When replay buffer gathers enough samples (more than 25 episodes of trajectory sam- ples), the LB agents train and update RL models for 25 iterations before running the next episode. QMIX and SAC models use the the same neural network architecture for both Q networks and policy networks–2 fully-connected layers, followed by 1 GRU layer, followed by 2 other fully-connected layers. The hidden dimension for all layers is 128. The activation function of all fully-connected layers is ReLU. Given the total provisioned computational resource, the traffic rates of network traces for training are carefully selected so that the RL models can learn from sensitive cases where work- loads should be carefully placed to avoid overloaded less powerful servers. The traffic rates for large-scale setup is lower than the one for moderate-scale setup (see Table 1), because the synthesised Poisson traffic has heavier per-job workloads than the real-world Wikipedia Web trace. 5 EVALUATION AND RESULTS 5.1 Moderate-Scale Testbed Evaluations As depicted in Figure 7, MARL-based LB methods show improved performance after 600 iterations of updates during training while the single agent RLB-S-SAC struggles to learn. Trained RL-based LB methods are then compared with all the heuristic LB methodsCIKM ’22, October 17–21, 2022, Atlanta, GA, USA. Zhiyuan Yao, Zihan Ding, and Thomas Clausen 0 200 400 600 800 1000 1200 1400 Update #Iter. 0.025 0.05 0.1 0.2 0.4 0.8 1.6 JCT (s) RLB-QMix RLB-I-SAC RLB-S-SAC avg. std. Figure 7: JCT distribution during training using 3 different RL algorithms. 10 2  10 1  100 101 JCT (s) 0.0 0.5 1.0 wiki page ECMP AWCMP WCMP LSQ SED RLB-I-SAC RLB-QMix RLB-S-SAC 10 3  10 2  10 1  100 101 JCT (s) static page CDF Figure 8: JCT comparison using different load balancing algorithms under different traffic rates (more than 600 queries/s, 5 runs per traffic rate). AWCMP WCMP LSQ SED RLB-I-SAC RLB-QMix RLB-S-SAC Method 0 20#Apache 4-CPU 2-CPU Figure 9: Comparison of number of busy Apache threads on two groups of application servers with different processing capacities. on 4 unseen network traces, which covers a various range of traffic rates from 518.8 to 796.3 flows/s. As shown in in Tables 3 to 6, RLB-QMIX achieves superior performance over all other methods in most scenarios, including the SOTA heuristic method (SED) and other learning agents (I-SAC and S-SAC). Only when the system is subject to the highest traffic rate (796.3 flows/s), the SED for Wiki pages and the I-SAC agents for static pages win over RLB- QMIX by a slight margin. Figure 8 depicts the overall performance comparisons by aggregating the JCTs over the 4 tested scenarios. RLB-QMIX is 1.44×and 5.11×faster than SED at 90th-percentile, which is an important QoS metric. Figure 9 shows the distribution of the number of busy Apache threads on two groups of servers. With manually configured server weights, SED assigns 2.329×more workloads on more powerful servers while RLB-QMIX maintains the equivalence between the two groups of servers. 5.2 Large-Scale Testbed Evaluations. As shown in Table 7, although the best performances are achieved with LSQ for 398.5 flows/s traffic rate and SED for 419.3 flows/s traffic rate, MARL methods (QMIX and I-SAC) both have a very close performance to the superior method, which demonstrates a certain level of scalability for these learning-based methods to work in real-world large-scale systems. Among all the heuristic LB methods, SED has the best perfor- mance since it takes both the queue occupation and server process- ing capacity information into account. However, when there are multiple LB agents, SED will be mis-guided because of the partially Table 3: Comparison under traffic rate 518.8 flows/second. Method Traffic Type Wiki Static ECMP 529.4 ±110.5 258.6 ±94.4 AWCMP 140.9 ±4.1 27.6 ±4.0 WCMP 92.6 ±16.2 12.7 ±8.1 LSQ 78.7 ±29.9 8.4 ±11.0 SED 67.2 ±5.1 6.7 ±3.1 RLB-I-SAC 84.3 ±16.9 9.4 ±7.4 RLB-QMix 63.4 ±3.9 3.1 ±0.1 RLB-S-SAC 84.2 ±13.0 14.4 ±14.4 Table 4: Comparison under traffic rate 690.9 flows/second. Method Traffic Type Wiki Static ECMP 3178.9 ±615.9 2835.3 ±542.8 AWCMP 430.7 ±154.5 153.3 ±112.5 WCMP 443.6 ±268.1 201.0 ±188.5 LSQ 236.6 ±164.4 69.3 ±105.7 SED 189.3 ±118.4 47.3 ±48.6 RLB-I-SAC 349.6 ±397.8 152.9 ±260.5 RLB-QMix 166.9 ±62.3 22.0 ±14.1 RLB-S-SAC 397.6 ±258.2 156.3 ±155.9 Table 5: Comparison under traffic rate 696.5 flows/second. Method Traffic Type Wiki Static ECMP 2748.5 ±371.1 2424.6 ±388.3 AWCMP 348.3 ±80.3 101.3 ±48.1 WCMP 530.7 ±411.7 287.1 ±355.9 LSQ 207.9 ±67.6 40.3 ±41.0 SED 182.6 ±85.7 40.0 ±35.1 RLB-I-SAC 146.4 ±54.7 19.8 ±16.9 RLB-QMix 88.0 ±10.4 4.0 ±0.7 RLB-S-SAC 169.1 ±56.4 27.0 ±24.1 Table 6: Comparison under traffic rate 796.3 flows/second. Method Traffic Type Wiki Static ECMP 3018.5 ±837.3 2636.8 ±859.7 AWCMP 539.1 ±152.4 203.6 ±103.2 WCMP 466.8 ±269.4 192.5 ±181.5 LSQ 208.8 ±117.5 50.8 ±38.0 SED 150.9 ±69.2 22.8 ±18.5 RLB-I-SAC 155.0 ±97.0 17.5 ±21.9 RLB-QMix 188.8 ±104.7 38.2 ±32.1 RLB-S-SAC 398.9 ±367.3 163.4 ±212.3 observed numbers of on-going flows. In the large-scale setup, as de- picted in Figure 10, SED assigns 2.67×and 2.25×more workloads to more powerful servers under 398.5 and 419.3 flows/s traffic rates, while the capacity ratio between the two groups of servers isMulti-Agent Reinforcement Learning for Network Load Balancing in Data Center CIKM ’22, October 17–21, 2022, Atlanta, GA, USA. Table 7: Comparison under different traffic rates (398.5,419.3 flows/second) with synthesised CPU-intensive Poisson traffic for large-scale system setup. Method Traffic Rate (flows/second) 398.5 419.3 ECMP 5907.2 ±550.1 7841.1 ±484.7 AWCMP 467.7 ±5.5 595.4 ±6.7 WCMP 629.9 ±25.0 1027.5 ±65.5 LSQ 332.7 ±1.6 420.2 ±2.0 SED 338.6 ±0.7 410.3 ±2.3 RLB-I-SAC 344.8 ±2.0 425.3 ±1.8 RLB-QMix 340.7 ±1.5 419.7 ±2.8 RLB-S-SAC 353.0 ±3.5 454.6 ±8.4 ECMP AWCMP WCMP LSQ SED RLB-QMix Method 0 20#Apache 4-CPU 2-CPU Figure 10: Comparison of the distribution of busy Apache threads on two groups of servers with different processing speeds in the large-scale scenario. 103 2 × 103 3 × 103 4 × 103 6 × 103 CPU Cycles 0.0 0.5 1.0CDF ECMP WCMP AWCMP LSQ SED RLB-QMix Figure 11: Load balancing decision making latency for each network flow measured under 800 flows/s traffic rate. 2. This behavior will lead to overloaded powerful servers, which is the reason why LSQ performs better than SED with low traffic rates. Future studies need to be conducted to learn and adapt to use different strategies under different scenarios. 5.3 Decision Making Latency The decision making latency is compared among all load balanc- ing methods by computing CPU cycles required on the LB node for dispatching every single network flow in the data plane. As depicted in Figure 11, the RLB-QMIX method has 3.6% and 8.6% additional processing latency than SED and LSQ respectively. The average number of CPU cycles required for each flow is 4326.27, which consumes 1.66𝜇𝑠 on 2.6GHz-CPU devices. This allows han- dling high-throughput network traffic (more than 600M packet per second) for real-world systems in production. Therefore, the proposed RL framework for network load balancing problem is able to incorporate intelligence while making high-frequent decisions. 5.4 Centralised Training and Communication Overhead. QMIX adopts the centralised training scheme, which is challenging to implement in real-world distributed system. This paper syn- chronises all the LB agents by way of TCP connections among 0.250 0.275 0.300 0.325 0.350 0.375 0.400 0.425 0.450 Time-Step Interval (s) 0.0 0.5 1.0CDF QMix (Train) QMix (Test) 2LB & 7 Servers 6LB & 24 Servers Figure 12: Time-step interval is incremented when using QMIX because of the synchronisation process. all agents. Only one master agent takes the responsibility of or- chestrating the actions of the other agents so that the interactions between agents and the environments are synchronised and the gathered trajectories follows the Dec-POMDP specification. This implementation shows good performance in this paper, especially in the moderate-scale setup. However, in the large-scale setup, RLB- QMIX is outperformed by SED and LSQ with a small margin. One of the reasons is that the increased communication overhead (latency) and delayed actions at the presence of more LB agents. As depicted in Figure 12, in the large-scale setup, the time interval between two consecutive controls (actions) is 1.24×larger than in the moderate- scale setup. This additional communication delay fails to effectuate the latest action in time, which deteriorate performance especially in dynamic environments. Future studies need to be conducted to alleviate this issue. 6 CONCLUSIONS AND FUTURE WORK This paper presents a MARL framework for network load balanc- ing problem, and evaluates different methods for the cooperative game in a real-world system. The learning-based methods includ- ing QMIX, independent-SAC and single-agent SAC are tailored for this application and compared with SOTA heuristic methods. Experiments show that in moderate-scale system with different traffic rates and types, the MARL method RLB-QMIX achieve su- perior performance in most settings. While for large-scale system, learning agents like RLB-QMIX and I-SAC also achieve close perfor- mance to the best heuristic methods. This verify the scalability of the proposed MARL methods for real-world large-scale load balanc- ing system. Although promising results are achieved, limitations exist in current work: (1) the QMIX algorithm makes additional structural assumption that the joint-action value is monotonic in individual agent value, which may be restrictive for the load balanc- ing problem; (2) reducing the communication cost among agents during training and decision making latency is important for ap- plication in real-world load balancing; (3) there are other types of scoring mechanism other than the linear-product fairness for load balancing system, like maxinising Jain’s fairness, which does not suffice as minimising the makespan yet still worths exploring since it has been used to evaluate load balancing performances [7]. Future work includes (1) evaluating more different types of MARL algorithms [29] on the current system as well as a simulation sys- tem in diverse and flexible settings, and (2) the contribution of each proposed component (e.g., with or without the GRU), to further improve the performance of MARL solutions.CIKM ’22, October 17–21, 2022, Atlanta, GA, USA. Zhiyuan Yao, Zihan Ding, and Thomas Clausen REFERENCES [1] Ashkan Aghdai, Cing-Yu Chu, Yang Xu, David H Dai, Jun Xu, and H Jonathan Chao. 2018. Spotlight: Scalable Transport Layer Load Balancing for Data Center Networks. arXiv preprint arXiv:1806.08455 (2018). [2] Ashkan Aghdai, Michael I-C Wang, Yang Xu, Charles H-P Wen, and H Jonathan Chao. 2018. In-network Congestion-aware Load Balancing at Transport Layer. arXiv preprint arXiv:1811.09731 (2018). [3] Ghada Alsuhli, Hassan A Ismail, Kareem Alansary, Mahmoud Rumman, Mostafa Mohamed, and Karim G Seddik. 2021. Deep reinforcement learning-based CIO and energy control for LTE mobility load balancing. In 2021 IEEE 18th Annual Consumer Communications & Networking Conference (CCNC) . IEEE, 1–6. [4] Li Chen, Justinas Lingys, Kai Chen, and Feng Liu. 2018. Auto: Scaling deep reinforcement learning for datacenter-scale automatic traffic optimization. In Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication. ACM, 191–205. [5] Petros Christodoulou. 2019. Soft actor-critic for discrete action settings. arXiv preprint arXiv:1910.07207 (2019). [6] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014). [7] Yoann Desmouceaux, Pierre Pfister, Jérôme Tollet, Mark Townsley, and Thomas Clausen. 2018. 6LB: Scalable and Application-Aware Load Balancing with Seg- ment Routing. IEEE/ACM Transactions on Networking 26, 2 (2018), 819–834. [8] Nicola Dragoni, Saverio Giallorenzo, Alberto Lluch Lafuente, Manuel Mazzara, Fabrizio Montesi, Ruslan Mustafin, and Larisa Safina. 2017. Microservices: yester- day, today, and tomorrow. In Present and Ulterior Software Engineering . Springer, 195–216. [9] Daniel E Eisenbud, Cheng Yi, Carlo Contavalli, Cody Smith, Roman Kononov, Eric Mann-Hielscher, Ardas Cilingiroglu, Bin Cheyney, Wentao Shang, and Jinnah Dy- lan Hosein. 2016. Maglev: A fast and reliable software network load balancer. In 13th {USENIX}Symposium on Networked Systems Design and Implementation ({NSDI}16). 523–535. [10] Daniel E Eisenbud, Cheng Yi, Carlo Contavalli, Cody Smith, Roman Kononov, Eric Mann-Hielscher, Ardas Cilingiroglu, Bin Cheyney, Wentao Shang, and Jin- nah Dylan Hosein. 2016. Maglev: A Fast and Reliable Software Network Load Balancer.. In NSDI. 523–535. [11] Guy Goren, Shay Vargaftik, and Yoram Moses. 2020. Distributed Dispatching in the Parallel Server Model. arXiv:2008.00793 [cs] (Aug 2020). arXiv: 2008.00793. [12] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning . PMLR, 1861– 1870. [13] Xiaotian Hao, Weixun Wang, Hangyu Mao, Yaodong Yang, Dong Li, Yan Zheng, Zhen Wang, and Jianye Hao. 2022. API: Boosting Multi-Agent Reinforce- ment Learning via Agent-Permutation-Invariant Networks. arXiv preprint arXiv:2203.05285 (2022). [14] Omar Houidi, Djamal Zeghlache, Victor Perrier, Pham Tran Anh Quang, Nicolas Huin, Jérémie Leguay, and Paolo Medagliani. 2022. Constrained Deep Reinforce- ment Learning for Smart Load Balancing. In 2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC) . IEEE, 207–215. [15] Adithya Kumar, Iyswarya Narayanan, Timothy Zhu, and Anand Sivasubrama- niam. 2020. The Fast and The Frugal: Tail Latency Aware Provisioning for Coping with Load Variations. In Proceedings of The Web Conference 2020 . 314–326. [16] Tianle Mai, Haipeng Yao, Zehui Xiong, Song Guo, and Dusit Tao Niyato. 2020. Multi-agent actor-critic reinforcement learning based in-network load balance. In GLOBECOM 2020-2020 IEEE Global Communications Conference . IEEE, 1–6. [17] Hongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan, Zili Meng, and Mohammad Alizadeh. 2018. Learning scheduling algorithms for data pro- cessing clusters. arXiv preprint arXiv:1810.01963 (2018). [18] Rui Miao, Hongyi Zeng, Changhoon Kim, Jeongkeun Lee, and Minlan Yu. 2017. SilkRoad: Making Stateful Layer-4 Load Balancing Fast and Cheap Using Switch- ing ASICs. In Proceedings of the Conference of the ACM Special Interest Group on Data Communication (SIGCOMM ’17) . ACM, 15–28. event-place: Los Angeles, CA, USA. [19] Amin Mohajer, Maryam Bavaghar, and Hamid Farrokhi. 2020. Mobility-aware load balancing for reliable self-organization networks: Multi-agent deep rein- forcement learning. Reliability Engineering & System Safety 202 (2020), 107056. [20] Frans A Oliehoek and Christopher Amato. 2016. A concise introduction to decen- tralized POMDPs . Springer. [21] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. 2018. Qmix: Monotonic value function factori- sation for deep multi-agent reinforcement learning. In International Conference on Machine Learning . PMLR, 4295–4304. [22] Arjun Roy, Hongyi Zeng, Jasmeet Bagga, George Porter, and Alex C. Snoeren. 2015. Inside the Social Network’s (Datacenter) Network. InProceedings of the 2015 ACM Conference on Special Interest Group on Data Communication (SIGCOMM ’15) . ACM, 123–137. https://doi.org/10.1145/2785956.2787472 event-place: London, United Kingdom. [23] Siddhartha Sen, David Shue, Sunghwan Ihm, and Michael J Freedman. 2013. Scal- able, optimal flow routing in datacenters via local link balancing. In Proceedings of the ninth ACM conference on Emerging networking experiments and technologies . 151–162. [24] Viswanath Sivakumar, Tim Rocktäschel, Alexander H Miller, Heinrich Küttler, Nantas Nardelli, Mike Rabbat, Joelle Pineau, and Sebastian Riedel. 2019. Mvfst-rl: An asynchronous rl framework for congestion control with delayed actions. arXiv preprint arXiv:1910.04054 (2019). [25] Jun Wu, Xin Xu, Pengcheng Zhang, and Chunming Liu. 2011. A novel multi-agent reinforcement learning approach for job scheduling in grid computing. Future Generation Computer Systems 27, 5 (2011), 430–439. [26] Yue Xu, Wenjun Xu, Zhi Wang, Jiaru Lin, and Shuguang Cui. 2019. Load Balancing for Ultra-Dense Networks: A Deep Reinforcement Learning Based Approach. IEEE Internet of Things Journal 6, 6 (Dec 2019), 9399–9412. https://doi.org/10. 1109/JIOT.2019.2935010 arXiv: 1906.00767. [27] Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao Tang. 2020. Qatten: A general framework for cooperative multiagent reinforcement learning. arXiv preprint arXiv:2002.03939 (2020). [28] Zhiyuan Yao, Zihan Ding, and Thomas Heide Clausen. 2021. Reinforced Workload Distribution Fairness. arXiv preprint arXiv:2111.00008 (2021). [29] Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. 2021. The surprising effectiveness of ppo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955 (2021). [30] J. Zhang, F. R. Yu, S. Wang, T. Huang, Z. Liu, and Y. Liu. 2018. Load Balancing in Data Center Networks: A Survey. IEEE Communications Surveys Tutorials 20, 3 (2018), 2324–2352. https://doi.org/10.1109/COMST.2018.2816042",
      "references": [
        "Spotlight: Scalable Transport Layer Load Balancing for Data Center Networks",
        "In-network Congestion-aware Load Balancing at Transport Layer",
        "Deep reinforcement learning-based CIO and energy control for LTE mobility load balancing",
        "Auto: Scaling deep reinforcement learning for datacenter-scale automatic traffic optimization",
        "Soft actor-critic for discrete action settings",
        "Empirical evaluation of gated recurrent neural networks on sequence modeling",
        "6LB: Scalable and Application-Aware Load Balancing with Seg- ment Routing",
        "Microservices: yesterday, today, and tomorrow",
        "Maglev: A fast and reliable software network load balancer",
        "Distributed Dispatching in the Parallel Server Model",
        "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
        "API: Boosting Multi-Agent Reinforce- ment Learning via Agent-Permutation-Invariant Networks",
        "Constrained Deep Reinforce- ment Learning for Smart Load Balancing",
        "The Fast and The Frugal: Tail Latency Aware Provisioning for Coping with Load Variations",
        "Multi-agent actor-critic reinforcement learning based in-network load balance",
        "Learning scheduling algorithms for data pro- cessing clusters",
        "SilkRoad: Making Stateful Layer-4 Load Balancing Fast and Cheap Using Switch- ing ASICs",
        "Mobility-aware load balancing for reliable self-organization networks: Multi-agent deep rein- forcement learning",
        "A concise introduction to decen- tralized POMDPs",
        "Qmix: Monotonic value function factori- sation for deep multi-agent reinforcement learning",
        "Inside the Social Network’s (Datacenter) Network",
        "Scal- able, optimal flow routing in datacenters via local link balancing",
        "Mvfst-rl: An asynchronous rl framework for congestion control with delayed actions",
        "A novel multi-agent reinforcement learning approach for job scheduling in grid computing",
        "Load Balancing for Ultra-Dense Networks: A Deep Reinforcement Learning Based Approach",
        "Qatten: A general framework for cooperative multiagent reinforcement learning",
        "Reinforced Workload Distribution Fairness",
        "The surprising effectiveness of ppo in cooperative, multi-agent games",
        "Load Balancing in Data Center Networks: A Survey"
      ],
      "meta_data": {
        "arxiv_id": "2201.11727v4",
        "doi": "10.1145/3511808.3557133",
        "authors": [
          "Zhiyuan Yao",
          "Zihan Ding",
          "Thomas Clausen"
        ],
        "published_date": "2022-01-27T18:47:59Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This paper addresses the network load balancing problem in data centers, which traditional heuristic solutions (like WCMP and LSQ) struggle with due to dynamic workloads and poor balance among multiple load balancers. It formulates the cooperative network load balancing task as a Dec-POMDP problem, enabling the application of Multi-Agent Reinforcement Learning (MARL) methods. The key contributions include proposing a novel MARL-based load balancing method (RLB) that leverages learning capabilities for micro-second level, data-driven decisions despite partial observations. The research demonstrates the superior performance of the MARL solution, particularly RLB-QMIX, over independent 'selfish' strategies and five state-of-the-art heuristic algorithms in real-world moderate-scale setups. It also defines a fairness index as the objective function, proving its sufficiency for minimizing makespan, and analyzes the challenges and future directions for MARL in this domain.",
        "methodology": "The network load balancing problem is formulated as a cooperative Dec-POMDP. Three different reinforcement learning schemes are implemented and compared: QMIX for Centralised Training, Decentralised Execution (CTDE), Soft Actor-Critic (SAC) for a single agent (S-SAC), and independent SAC agents (I-SAC). QMIX estimates a total Q-value function from individual agent Q-values, while SAC optimizes for maximum entropy. To achieve high-frequency decision-making (sub-ms), the action of LB agents is converted from per-request server assignment to periodically (every 250ms) and dynamically estimating server processing speed weights. These weights are then used with a Shortest Expected Delay (SED) algorithm for per-flow decisions. Observations consist of the number of on-going tasks and statistical evaluations of task elapsed time on each server, collected using reservoir sampling to infer server load states and mitigate partial observation issues. The reward function is based on a fairness index derived from the exponentially weighted average of flow elapsed times, which is proven to be sufficient for minimizing makespan. Gated Recurrent Units (GRUs) are used in the neural network architectures for QMIX and SAC agents to process sequential history information.",
        "experimental_setup": "Experiments were conducted on a real-world system using physical servers virtualized as Kernel-based Virtual Machines (KVMs) on 4 UCS B200 M4 servers. The network layer of LB agents was implemented using the programmable software network stack VPP v20.05. Apache HTTP servers were used as application servers. Two testbed configurations were utilized: a moderate-scale setup with 2 LBs and 7 application servers, and a large-scale setup with 6 LBs and 24 application servers. Network traces included a 24-hour Wikipedia replay trace (for moderate-scale, 518.8-796.3 flows/s) and a synthesized CPU-intensive Poisson traffic (for large-scale, 391.5-436.7 flows/s). The MARL methods (QMIX, I-SAC, S-SAC) were trained for 72 episodes with a step interval of 0.25s, and a replay buffer size of 3000. Performance was evaluated against five SOTA heuristic methods: ECMP, WCMP, AWCMP, LSQ, and SED. Key metrics included Job Completion Time (JCT) distribution, the number of busy Apache threads, and decision-making latency (CPU cycles).",
        "limitations": "The QMIX algorithm assumes a monotonic dependence relationship for the joint-action value, which may be a restrictive structural assumption for the load balancing problem. Additionally, communication overhead among agents during centralised training and decision-making latency can significantly impact performance, especially in large-scale distributed systems, leading to delayed actions. The study primarily focused on linear-product fairness as the scoring mechanism, while other fairness metrics (e.g., Jain’s fairness) which have been used to evaluate load balancing performance were not explored.",
        "future_research_directions": "Future work includes evaluating a wider variety of MARL algorithms on the current real-world system and in diverse simulation settings. Another direction is to analyze the individual contribution of each proposed component, such as the use of Gated Recurrent Units (GRUs), to further enhance the performance of MARL solutions. Addressing and reducing the communication cost among agents during training and decision-making latency is crucial for practical application in real-world load balancing scenarios. Furthermore, exploring other types of scoring mechanisms for load balancing systems, beyond the linear-product fairness, such as Jain’s fairness, is also suggested. The paper also implies a need for future studies to learn and adapt different strategies under varying scenarios, such as different traffic rates, to optimize performance.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Large Language Models are Human-Level Prompt Engineers",
      "full_text": "arXiv:2309.03446v1  [math.GR]  7 Sep 2023 Skew Product Groups for 2-Groups of Maximal Class Wenjuan Luo and Hao Yu 1 Capital Normal University, School of Mathematical Sciences, Beijing 100048, People’s Republic of China Abstract Skew morphisms, which generalise automorphisms for groups , provide a fun- damental tool for the study of regular Cayley maps and, more g enerally, for ﬁnite groups with a complementary factorisation X = GY , where Y is cyclic and core-free in X. X is called the skew product group associated with G and Y . In this paper, we classify skew product groups for the maximal class 2-grou ps. 1 Introduction All groups in this paper are assumed to be ﬁnite. A skew-morphism of a group G is a permutation σ on G, having the properties that σ(1G) = 1 G and there exists an integer- valued function π on G such that σ(gh) = σ(g)σπ (g)(h) for all g, h ∈ G. π is σ’s associated power function . Note that if π(g) = 1 for all g ∈ G, then the skew-morphism σ is an automorphism of G. Thus skew morphisms generalise the concept of automorphisms fo r groups. The investigation of skew-morphisms is at least related to the followin g two topics. (1) Group factorizations : Use LG := {Lg | g ∈ G} to denote the left regular represen- tation of G. Then σ , Lg ∈ Sym(G). For any g, h ∈ G, we have (σLg)(h) = σ(gh) = σ(g)σπ (g)(h) = ( Lσ (g)σπ (g))(h), and so σLg = Lσ (g)σπ (g). Therefore, ⟨σ⟩LG ⊆ LG⟨σ⟩. Since |⟨σ⟩LG| = |LG⟨σ⟩|, we have ⟨σ⟩LG = LG⟨σ⟩, which implies that X := LG⟨σ⟩ is a subgroup of Sym( G), called the skew-product of LG by σ, see [4, 42]. Moreover, one can show that ⟨σ⟩ is core-free in X, meaning that there is no nontrivial normal subgroup of X contained in ⟨σ⟩. Conversely, let X be a ﬁnte group admitting a factorization X = GY with G ∩ Y = 1 and Y = ⟨y⟩ being cyclic and core-free in X. Then for any g ∈ G, there exists a unique g′ ∈ G and a unique i ∈ { 1, 2, . . . , |Y |−1} such that yg = g′yi. This induces a permutation 1Corresponding author: 3485676673@qq.com. This work is support ed in part by the National Natural Science Foundation of China (12071312). Keywords skew product groups, 2-groups, regular Cayley map, skew morph ism MSC(2010) 20F19, 20B20, 05E18, 05E45. 1σ on G by σ(g) = g′, and an integer-valued function π on G by π(g) = i. Then one may check that σ is a skew-morphism of G with power function π. (2) Cayley maps : The concept of skew morphism was ﬁrst introduced as a fundamen tal tool for the study of regular Cayley maps [16]. Let G be a group and let S be a subset of G such that 1 G ̸∈S, S = S−1 and G = ⟨S⟩. Let ρ be a cycle on S. A Cayley map M = CM( G, S, ρ ) is a 2-cell embedding of a Cayley graph Cay( G, S ) into an orientable closed surface such that, at each vertex g of M, the local orientation Rg of the darts (g, gx ) incident with g agrees with ρ on S, that is, Rg(g, gx ) = ( g, gx ρ) for all g ∈ G and x ∈ S. The automorphism group Aut ( M) of a Cayley map M contains a vertex-regular subgroup induced by left multiplication of the elements of G and acts semi-regularly on the darts of M. If Aut ( M) is regular, then the map M is called a regular Cayley map . It was shown by Jajcay and ˇSir´ aˇ n that a Cayley mapM is regular if and only if ρ extends to a skew-morphism of G, see [16, Theorem 1]. Thus the problem of determining all regular Cayley maps of a group G is equivalent to the problem of determining all skew-morphisms of G containing a generating orbit which is closed under taking inverses. T herefore, it is suﬃcient for us to consider skew product groups X = GY with G ∩ Y = 1 and Y = ⟨y⟩ being cyclic and core-free in X. Now we are ready to recall the studying history of skew-morphisms of groups. An interesting and important problem in this area is a determination of th e skew-morphisms of a given family of groups. The problem seems challenging because ev en skew-morphisms of the cyclic groups have not yet been completely determined. For p artial results of cyclic groups, see [4, 5, 8, 18, 19, 24]. For ﬁnite nonabelian simple group or ﬁnite nonabelian characteristically simple groups, they were classiﬁed in [2] and [3], res pectively, and for elementary abelian p-groups, a global structure was characterized in [9]. Based on big eﬀorts of several authors working on regular Cayley maps (see [4, 12, 25, 20, 21, 22, 31, 23, 33, 34, 38, 39, 40, 41, 42]), the ﬁnal classiﬁcation of skew produc t groups of dihedral groups was given in [12]. For generalized quaternion groups, there are some partial results, see [13] and [26]. A 2-group of order 2 n ≥ 8 is said to be of maximal class if it has nilpotency class n − 1. In this paper, we shall classify skew product groups for 2-grou ps of maximal class. Given the skew product group LG⟨σ⟩ of LG by σ, for the purpose of this paper, we may deﬁne the skew product group X := G⟨σ⟩ of G by σ as follows: every element of X is uniquely written as gσ i where g ∈ G and i is a positive integer less than the order of σ; for each pair of elements aσi, bσ j ∈ X, we have ( aσi)(bσj ) = aσi(b)σ ∑ i−1 k=0 π (σ kb)+j . It is straightforward to check by using the deﬁnition of the skew-morp hism σ that X is indeed a group with operation deﬁned above. Sometimes, we just say X a skew-product group of G for short. Throughout this paper, set C = ⟨c⟩ and Q = ⟨a, b | a2n = 1 , b 2 = an, a b = a−1⟩ ∼= Q4n, n ≥ 2, D = ⟨a, b | an = b2 = 1 , a b = a−1⟩ ∼ = D2n, n ≥ 2. (1) 2Let G ∈ { Q, D } and let X = X(G) = GC = ⟨a, b ⟩⟨c⟩ be a group. In Theorem 1.3, a classiﬁcation of X(Q) is given, provided that C is core-free. For skew product groups of p-groups, we have the following characterization: Theorem 1.1 Let X = GC be a group, where G is a p-group and C is a cyclic group such that G∩C = 1 . Set C = C1 ×C2, where C1 is the Sylow p-subgroup of C. If CX = 1 , then F (X) = Op(X) = G1C1, where G1 = Op(X) ∩ G ̸= 1 and G1C1 ⋊ C2 ⊳ X. Theorem 1.2 Let X = GC be a group, where C is a cyclic group, and suppose that G is a maximal class 2-group and |G| = 2 n ≥ 32. Assume that G ∩ C = 1 and that CX = 1 . Then X is a 2-group. Theorem 1.3 Let X = GC be a 2-group, where G is a maximal class group, C is a cyclic group and G ∩ C = 1 . If CX = 1 , then GX is ⟨a0⟩, ⟨a2, b ⟩ or G. Theorem 1.4 Let X = GC be a 2-group, where G is a maximal class group, C is a cyclic group and G ∩ C = 1 . Set R is the deﬁned relation of G. Then X is isomorphic to one of the following groups: (1) X = ⟨a, b, c |R, a c = ar, b c = asb⟩, where r2m ≡ 1(2n−1), and r2m−1 ̸≡1(2n−1) or sr2m−1 −1 r−1 ̸≡0(2n−1). Moreover, if G is a semidihedral 2-groups, then 2|s; (2) X = ⟨a, b, c |R, a 2 = a2r, c b = a2sc, c a = a2tbucv⟩, where r2m ≡ 1(mod 2 n−2), s ∑ 2m l=1 rl ≡ 0(mod 2 n−2), either (2.1) u = 0 , rv−1 ≡ 1(mod 2 n−2), (s + 2 t)r ≡ (1 − r) + s ∑ v l=1 rl(mod 2 n−2), t ∑ 2m l=1 rl ≡ 0(mod 2 n−2).v 2 ≡ 1(mod 2 m) and 1 −r ≡ tr+t ∑ v l=1 rl(mod 2 n−2). (2.2) u = 1 , rv−1+1 ≡ 0(mod 2 n−2). (sr+1−r) ∑ v−1 l=0 rl ≡ (s+2t+1)r(mod 2 n−1). (t(1− r−1) + s ∑ v−1 l=0 rl) ∑ 2m−1−1 l=0 r2l ≡ 0(mod 2 n−2).r 2[t(1 − r−1) + srv−1 r−1 ]rv−1−1 r2−1 + 2n−3i ≡ 0(2n−2); (3) X = ⟨a, b, c |R, (a2)c2 = a2, (c2)a = a2sc−2, (c2)b = a2uc2, a c = bc2y⟩, where sy ≡ 1 +i2n−3(mod 2 n−2) and yu ≡ − 1(mod 2 n−3), i = 1 if G is a generalized quaternion group and i = 0 if G is either a dihedral group or a semidihedral group. 2 Preliminaries In this section, the notation and elementary facts used in this pape r are collected. 32.1 Notation In this paper, all the groups are supposed to be ﬁnite. We set up th e notation below, where G and H are groups, M is a subgroup of G, n is a positive integer and p is a prime number. |G| and o( g): the order of G and an element g in G, resp.; H ≤ G and H < G : H is a subgroup of G and H is a proper subgroup of G, resp.; [G : H]: the set of cosets of G relative to a subgroup H; H ⊳ G and H char G: H is a normal and characteristic subgroup of G, resp.; G′ and Z(G): the derived subgroup and the center of G resp.; MG: the core of M in G which is the maximal normal subgroup of G contained in M; G ⋊ H: a semidirect product of G by H, in which G is normal; G.H : an extension of G by H, where G is normal; CM (G): centralizer of M in G; NM (G): normalizer of M in G; Sylp(G): the set of all Sylow p-subgroups of G; [a, b ] := a−1b−1ab, the commutator of a and b in G; Ω 1(G): the subgroup ⟨g ∈ G | gp = 1 ⟩ of G where G is a p-group; ℧n(G): the subgroup ⟨gpn | g ∈ G⟩ of G where G is a p-group; Op(G) and Op′ (G): the maximal p-subgroup and p′-subgroup of G,resp.; F (X): the Fitting subgroup (the product of all nilpotent normal subgr oups of G). 2.2 Elementary facts Proposition 2.1 [14, Theorem 11.9] Let G be a maximal class group and |G| = 2 n. Then G is isomorphic to one of the following three groups: (1) D2n := ⟨a, b |a2n−1 = b2 = 1 , a b = a−1⟩, n ≥ 3; (2) Q2n := ⟨a, b |a2n−1 = 1 , b 2 = a2n−2 , a b = a−1⟩, n ≥ 3; (3) SD2n := ⟨a, b |a2n−1 = b2 = 1 , a b = a−1+2n−2 ⟩, n ≥ 4. Lemma 2.2 Let G be a maximal class group and |G| = 2 n, where n ≥ 5. Then Aut (G) is a 2-group. 4Proposition 2.3 [14, Theorem 4.5] Let H be the subgroup of G. Then NG(H)/C G(H) is isomorphic to a subgroup of Aut (H). Proposition 2.4 [29, Theorem] If G is a transitive permutation group of degree n with a cyclic point-stabilizer, then |G| ≤ n(n − 1). Proposition 2.5 [15, Satz 1 and Satz 2] Let G = AB be a group, where both A and B are abelian subgroups of G. Then (1) G is meta-abelian, that is, G′ is abelian; (2) if G ̸= 1 , then A or B contains a normal subgroup N ̸= 1 of G. Proposition 2.6 [14, 9, Kap. III, Satz 4.2(b)] Suppose that G is a solvable group and F (G) is the Fitting subgroup of G. Then CG(F (G)) ≤ F (G). Proposition 2.7 [8, Remark 1.2(i)] The order of each skew morphisms of a cycli c group of order 2n is equal to 2m for some m < n and so the corresponding skew-product group is a bicyclic 2-group with a core-free cyclic factor. 3 Proof of Theorem 1.1 Lemma 3.1 Let X = GC be a group, where G is a p-group and C is a cyclic group such that G∩C = 1 . Set C = C1 ×C2, where C1 ∈ Sylp(C). If CX = 1 , then F (X) = Op(X) = G1C1, where G1 = Op(X) ∩ G ̸= 1 and G1C1 ⋊ C2 ⊳ X. Proof Since X is a product of two nilpotent groups, it is solvable and so F (X) ̸= 1. Note that Op′ (X) ≤ C. Thus Op′ (X) = 1 as CX = 1. Then F (X) = Op(X). Let P = GC1 ∈ Sylp(X). Obviously, Op(X) = ⋂ x∈C2 P x, hence C1 ≤ Op(X) and so Op(X) = (Op(X) ∩ G)C1. Note that G1 := Op(X) ∩ G ̸= 1 as CX = 1. Let X = X/O p(X) = GC2. Observe that Op(X) = 1, which implies F (X) = Op′ (X) ≤ C2. By Proposition ??, we have C2 ≤ CX (F (X)) ≤ F (X) ≤ C2, and therefor Op′ (X) = C2. Thus Op(X) ⋊ C2 = G1C1 ⋊ C2 ⊳ X. □ 4 Proof of Theorem 1.2 Note that both D4 and D8 have a skew-morphism Z3. However, when n ≥ 4, we have the following results. Lemma 4.1 Let X = GC be a group, where C is a cyclic group, and suppose that G is a maximal class group and |G| = 2 n ≥ 32. Assume that G ∩ C = 1 and that CX = 1 . Then X is a 2-group. 5Proof The result is true for n = 5, and so we assume that n > 5, and we shall proceed by induction on n. Set C = C1 × C2, where C1 ∈ Syl2(C). Obviously, GC1 is a Sylow 2-subgroup of X. By Lemma 3.1, we have F (X) = O2(X) and O2(X) ⋊ C2 ⊳ X. Assume that O2(X) < P . Take X1 := G1C. Then G1 := O2(X) ∩ G < G , and observe that G1 is a cyclic group or a maximal class group. For the former case, X1/C X1 is a 2-group by Proposition 2.7, and hence C2 char X1 ⊳ X. Then C2 ⊳ X. Since CX = 1, we get C2 = 1, as desired. For the latter case, 32 ≤ | G1| = |G| 2 , by the induction hypothesis, X1/C X1 is a 2-group, and hence C2 char G1C ⊳ X. Obviously, C2 = 1 as CX = 1. Now assume that O2(X) = P . Let G = ⟨a, b ⟩ and C1 = ⟨c1⟩, where |a| = 2 n−1. Set a0 = a2n−2 . Note that P = GC1 = ⟨a, b, c 1⟩. Let Φ( P ) be the Frattini subgroup of P . Observe that |P : Φ( P )| is either 4 or 8 as P = ⟨a, b, c 1⟩. Let X = X/ Φ( P ) = GC. For the former case, |G| = |G/ Φ( P ) ∩ G| = 2, then G∩Φ( P ) = ⟨a2, b ⟩or ⟨a⟩. If G∩Φ( P ) = ⟨a2, b ⟩, then X = ( ⟨a⟩ × ⟨c1⟩) ⋊ ⟨c2⟩, and therefor ⟨c2⟩⊳ X. Note that ⟨a2, b ⟩C = Φ( P )C2 ⊳ X. Since 32 ≤ |⟨ a2, b ⟩| = |G| 2 , by the induction hypothesis, C2 char ⟨a2, b ⟩C, and therefor C2 ⊳ X. Observe that C2 = 1 as CX = 1. If Φ( P ) ∩ G = ⟨a⟩, then X = ( ⟨b⟩ × ⟨c1⟩) ⋊ ⟨c2⟩, and therefor ⟨c2⟩⊳ X. Note that ⟨a⟩C = Φ( P )C2 ⊳ X. By Proposition 2.7, C2 char ⟨a⟩C, and therefor C2 ⊳ X. Note that C2 = 1 as CX = 1. For the latter case, ⟨a, c 1⟩ = ⟨a⟩⟨c1⟩ is a subgroup of X. Observe that a2 ∈ Φ( P ) and c12 ∈ Φ( P ). Then Φ( P ) = ⟨a2⟩⟨c12⟩. If C2 ̸= 1, then |c1| < |a|, and therefor Ω (m) = ⟨a2m ⟩ ̸= 1. Since ⟨a0⟩char Ω (m) char P char X, we get ⟨a0⟩⊳ X. Since 32 ≤ | G/ ⟨a0⟩| = |G| 2 and G/ ⟨a0⟩ is the maximal class 2-group, by the induction hypothesis, ⟨a0⟩⟨c2⟩/ ⟨a0⟩ ⊳ X/ ⟨a0⟩, and therefor ⟨a0⟩⟨c2⟩ ⊳ X. Note that C2 ⊳ X as C2 char ⟨a0⟩⟨c2⟩⊳ X, contradicting with CX = 1. □ 5 Proof of Theorem 1.3 Notation: Recall D2n = ⟨a, b |a2n−1 = b2 = 1 , a b = a−1⟩, Q2n = ⟨a, b |a2n−1 = 1 , b 2 = a2n−2 , a b = a−1⟩ and SD2n = ⟨a, b |a2n−1 = b2 = 1 , a b = a−1+2n−2 ⟩. Set n ≥ 5. Then let X = GC be a 2-group where G ∈ { D2n , Q 2n , SD 2n }, C = ⟨c⟩ ∼= Z2m , G ∩ C = 1 and CX = 1. Then X is a skew product group of G. By Proposition 2.4, we get m < n , that is o(c) ≤ o(a). Set a0 := a2n−2 and z := c2m−1 . Recall Φ( X) is the Frattini subgroup of X. It follows from X = ⟨a, b, c ⟩, that is d(G) ≤ 3. We prove Theorem 1.3 in the following three lemma proofs. Lemma 5.1 GX ̸= 1 . Proof Since X is 2-group, we get Z(X) ̸= 1. Then for any gck ∈ Z(X) where gck ̸= 1, we have g ̸= 1 as CX = 1. Since [ gck, c ] = [ g, c ]ck = 1, we have [ g, c ] = 1 and g ∈ ∩ci∈C Gci = GX . Thus GX ̸= 1. □ Lemma 5.2 If GX ≨ ⟨a⟩ and |GX | ≥ 4, then ⟨a⟩⟨c⟩ < X . 6Proof Suppose that ⟨a⟩⟨c⟩ is not a group. Then X = ⟨a, c ⟩ as ⟨a⟩⟨c⟩ ⊆ ⟨ a, c ⟩ and |⟨a⟩⟨c⟩| = X 2 , and thus |Φ( X)| = |X| 4 . Observe that G < G Φ( X) < X as G < X and C < X. Then Φ( X)∩G = ⟨a2, b 1⟩ for some b1 ∈ G\\⟨a⟩ because 2 ≤ | GΦ( X)/ Φ( X)| ≤ 4. Note that Φ( X) = ⟨a2, b 1⟩⟨c2⟩ as c2 ∈ Φ( X). Since |GX | ≥ 4 and GX ≤ ⟨a⟩, ⟨a2n−3 ⟩char GX ⊳ X, and so ⟨a2n−3 ⟩ ⊳ X. Let H = CX (⟨a2n−3 ⟩). Note that X/H ≲ Aut (⟨a2n−3 ⟩) and |Aut (⟨a2n−3 ⟩)| = 2, and therefor |X/H | = 2. Then Φ( X) < H , and so [ a2n−3 , b 1] = 1, a contradiction. Thus ⟨a⟩⟨c⟩ < X . □ Lemma 5.3 If ⟨a⟩⟨c⟩ ≤ X, then GX is either ⟨a2, b ⟩ or G. Proof For the contrary, assume that GX is neither G nor ⟨a2, b ⟩. Then GX ≤ ⟨a⟩. Let z be deﬁned as above. Pick a2 ∈ GX such that ⟨c⟩⟨a2 2⟩/ ⟨a2 2⟩ is core-free in X/ ⟨a2 2⟩, but ⟨c⟩⟨a2⟩/ ⟨a2⟩ has the nontrivial core, say ⟨ci⟩⟨a2⟩/ ⟨a2⟩ in X/ ⟨a2⟩. Then in X := X/ ⟨a2 2⟩, ℧1(⟨ a2⟩ × ⟨ci⟩) = ⟨c2i⟩⊳ X, which implies ci = z. In particular, ⟨a2⟩⋊ ⟨z⟩⊳ X. Considering the conjufacy of G on ⟨a, z⟩ ∼= D4, there exists an involution aib ∈ G exchanging z and a2z (simply we denote aib by b). Since X = GC = ( ⟨a⟩⟨c⟩) ⋊ ⟨b⟩, ﬁrstly we write cb = asct, where t ̸= 0. Then c = cb 2 = ( asct)b = a−s(asct)t = ct(asct)t−1, that is ( asct)t−1 = c1−t. Then we have (ct−1)b = ( cb)t−1 = ( asct)t−1 = c1−t. If t ̸= 1, then zb ∈ ⟨ct−1⟩b = ⟨ct−1⟩, contradicting to zb = a2z. So t = 1, that is cb = asc. Secondly, we write cb = ct1 as1 . With the same arguments, we may get t1 = 1 and cb = cas1 . Therefore, we have asc = cb = cas1 , that is ( as)c = as1 . Clearly ⟨as⟩ = ⟨as1 ⟩, that is c normalises ⟨as, b⟩. Then ⟨as, b⟩ ≤ ∩ ci∈⟨c⟩G ci = ∩x∈X G x = GX < ⟨a⟩, a contradiction. □ 6 Classiﬁcation To prove Theorem 1.3, set R := {a2n = cm = 1 , b 2 = an, a b = a−1}. Then we shall deal with the ﬁve cases in Theorem 1.1 in the following ﬁve subsections , separately. Let A = G. ⟨t⟩ where G ⊳ A and tl = g ∈ G. Then t induces an automorphism τ of G by conjugacy. Recall that by the cyclic extension theory of groups, this extension is valid if and only if τl = Inn( g) and τ(g) = g. 76.1 G ⊳ X Lemma 6.1 Suppose that G ⊳ X and CX = 1 . Then X = ⟨a, b, c |R, a c = ar, b c = asb⟩, where r2m ≡ 0(2n−1), and r2m−1 ̸≡1(2n−1) or sr2m−1 −1 r−1 ̸≡0(2n−1). Moreover, if G is a semidihedral 2-groups, then 2|s. Proof Since G ⊳ X, we set ac = ar and bc = asb. Let π ∈ Aut (G) such that π(a) = ar and π(b) = asb. Then o(π(a)) = o(a) and π2m = 1, that is r2m ≡ 1(2n−1). Note that if G is a semidihedral 2-groups, then o(b) = o(π(b)) = o(asb) = 2, and so 2 |s. Insure ⟨c⟩X = 1: az = ac2m−1 = ar2m−1 ̸= a or bz = bc2m−1 = as r2m−1 r−1 b ̸= b, that is r2m−1 ̸≡1(2n−1) or sr2m−1 −1 r−1 ̸≡0(2n−1). □ 6.2 GX = ⟨a2, b ⟩ Lemma 6.2 Suppose that GX = ⟨a2, b ⟩. Then X = ⟨a, b, c |R, a 2 = a2r, c b = a2sc, c a = a2tbucv⟩, where r2m ≡ 1(mod 2 n−2), s ∑ 2m l=1 rl ≡ 0(mod 2 n−2), either (1) u = 0 , rv−1 ≡ 1(mod 2 n−2), (s + 2t)r ≡ (1 − r) + s ∑ v l=1 rl(mod 2 n−2), t ∑ 2m l=1 rl ≡ 0(mod 2 n−2). v 2 ≡ 1(mod 2 m) and 1 − r ≡ tr + t ∑ v l=1 rl(mod 2 n−2). (2) u = 1 , rv−1 +1 ≡ 0(mod 2 n−2). (sr +1 −r) ∑ v−1 l=0 rl ≡ (s+2t+1)r(mod 2 n−1). (t(1− r−1) + s ∑ v−1 l=0 rl) ∑ 2m−1−1 l=0 r2l ≡ 0(mod 2 n−2). r 2[t(1 − r−1) + srv−1 r−1 ]rv−1−1 r2−1 + 2n−3i ≡ 0(2n−2). Proof X = (( ⟨a2⟩⋊ ⟨c⟩). ⟨b⟩). ⟨a⟩. Set a2 = a2r, c b = a2sc, c a = a2tbucv, where u ∈ { 0, 1}. What we should to determine the parameters r, s, t, u and v by analysing three exten- sions. (1) ⟨a2⟩ ⋊ ⟨c⟩, where ( a2)c = a2r. Set π1 ∈ Aut (⟨a2⟩) such that π1(a2) = a2r. As mentioned before, this extension is valid if and only if o( π1(a2)) = o( a2) = 2 n−1 and π2m 1 = 1, that is r2m ≡ 1(mod 2 n−2). (2) (2) ( ⟨a2⟩⋊ ⟨c⟩). ⟨b⟩, where cb = a2sc. Set π2 ∈ Aut ((⟨a2⟩⋊ ⟨c⟩): a2 → a−2 and c → a2sc. This extension is valid if and only if the following three equalities hold: (i) π2 preserves ( a2)c = a2r, as desired. 8(ii) o( π2(c)) = 2 m: (a2sc)2m = c2m (a2s)c2m · · · (a2s)c = c2m a2s ∑ 2m l=1 rl = c2m a2s ∑ 2m l=1 rl = 1 , that is s 2m ∑ l=1 rl ≡ 0(mod 2 n−2). (3) (iii) π2 2 = Inn( b2) : Since b2 ∈ Z(X), we get c = cb2 = ( a2sc)b = a−2sa2sc, as desired. (3) (( ⟨a2⟩ ⋊ ⟨c⟩). ⟨b⟩). ⟨a⟩, where ca = a2tbucv and u ∈ { 0, 1}. Set π3 ∈ Aut ((⟨a2⟩ ⋊ ⟨c⟩). ⟨b⟩): a2 → a2, b → ba2 and c → a2tbucv. We divide the proof into two cases according to u, separately. Case 1: u = 0 . In this case, we get ca = a2tcv and π3(c) = a2tcv. (i) π3 preserves ( a2)c = a2r: rv−1 ≡ 1(mod 2 n−2). (4) (ii) π3 preserves cb = a2sc: (cb)a = ( ca)ba2 = ( a2tcv)ba2 = ( a−2t(a2sc)v)a2 = ( a−2tcva2s ∑ v l=1 rl )a2 = a−2t(ca2−2r)va2s ∑ v l=1 rl = a−2tcva(2−2r) ∑ v−1 l=0 rl a2s ∑ v l=1 rl = cva−2trv+2(1−rv )+2s ∑ v l=1 rl , (a2sc)a = a2(s+t)cv = cva2(s+t)rv , that is (s + 2t)r ≡ (1 − r) + s v∑ l=1 rl(mod 2 n−2). (5) (iii) o( π3(c)) = 2 m: 1 = ( a2tcv)2m = a2t ∑ 2m l=1 rvl , that is t 2m ∑ l=1 rl ≡ 0(mod 2 n−2). (6) (iv) π2 3 = Inn( a2): ca2−2r = Inn( a2)(c) = π2 3(c) = ( a2tcv)a = a2t(a2tcv)v = cv2 a2trv2 +2t ∑ v l=1 rvl , 9that is v2 ≡ 1(mod 2 m) and 1 − r ≡ tr + t v∑ l=1 rl(mod 2 n−2). (7) Case 2: u = 1 . In this case, we get ca = a2tbcv and π3(c) = a2tbcv. (i) π3 preserves ( a2)c = a2r: rv−1 + 1 ≡ 0(mod 2 n−2). (8) (ii) π3 preserves cb = a2sc: (cb)a = ( ca)ba2 = ( a2tbcv)ba2 = ( a−2tb(a2sc)v)a2 = a−2ta−4b(a2sca2−2r)v = a−2t−4b(ca2sr+2−2r)v = a−2t−4−2(sr+1−r)r−v ∑ v−1 l=0 rl bcv = a−2t−4+2(sr+1−r)r−1 ∑ v−1 l=0 rl bcv, (a2sc)a = a2(s+t)bcv, that is (sr + 1 − r) v−1∑ l=0 rl ≡ (s + 2t + 1)r(mod 2 n−1). (9) (iii) o( π3(c)) = 2 m: 1 = ( a2tbcv)2m = ( a2tbcvba−2tcv)2m−1 = a2r2(t(1−r−1)+s ∑ v−1 l=0 rl) ∑ 2m−1−1 l=0 r2l , that is (t(1 − r−1) + s v−1∑ l=0 rl) 2m−1−1∑ l=0 r2l ≡ 0(mod 2 n−2). (10) (iv) π2 3 = Inn( a2): ca2−2r = Inn( a2)(c) = π2 3(c) = ( a2tbcv)a = a2t−2b(a2tbcv)v = ca−2r+2r2[t(1−r−1)+s rv−1 r−1 ] rv−1−1 r2−1 ai 0, where i = v+1 2 if G is a generalized quaternion group, and i = 0 if G is not a generalized quaternion group. Hence r2[t(1 − r−1) + srv − 1 r − 1 ]rv−1 − 1 r2 − 1 + 2n−3i ≡ 0(2n−2). (11) 10(4) Insure ⟨c⟩X = 1: If u = 0, then za = ( c2m−1 )a = ( a2tcv)2m−1 = za2tr r2m−1−1 r−1 ̸= z or zb = ( c2m−1 )b = ( a2sc)2m−1 = za2sr r2m−1−1 r−1 ̸= z, that is tr2m−1−1 r−1 ̸≡0(2n−2) or sr2m−1−1 r−1 ̸≡ 0(2n−2). If u = 1, then za = ( c2m−1 )a = ( a2tbcv)2m−1 = za2r2[t(1−r−1)+s rv−1 r−1 ] r2m−1 −1 r2−1 ̸= z or zb = za2sr r2m−1−1 r−1 ̸= z, that is [ t(1 − r−1) + srv−1 r−1 ]r2m−1 −1 r2−1 ̸≡0 or sr2m−1−1 r−1 ̸≡0. □ 6.3 |GX | = 2 Lemma 6.3 Suppose that n ≥ 5 and |GX | = 2 . Then X = KC = (( ⟨a2, b ⟩⋊⟨c2⟩). ⟨a⟩). ⟨c⟩, where K = G⟨c2⟩. Moreover, K′ = ⟨a2⟩ × ⟨c2 1⟩, GK = ⟨a2, b ⟩ and ⟨a2⟩⋊ ⟨c1⟩⊳ X. Proof Suppose that GX = ⟨a0⟩. Consider the faithful permutation representation of X on [ X : G]. Since if M, M c ≤ G, then |M| ≤ 4, and so Gc−1G contains at least |G| 4 = 2 n−2 coset of G, and 1 + 2 n−2 ≤ | [X : G]| = 2 m. Note that m = n − 1 as n − 2 < m < n . Then o(a) = o(c). Since X/G X = X/ ⟨a0⟩ = ( G/ ⟨a0⟩)(C⟨a0⟩/ ⟨a0⟩) and |G/ ⟨a0⟩| = |C⟨a0⟩/ ⟨a0⟩|, the core of C⟨a0⟩/ ⟨a0⟩ in X/ ⟨a0⟩ is ⟨z⟩⟨a0⟩. Set X = X/ ⟨a0, z ⟩ = GC, where G ∼= D2n−1 and C is core-free. Let H = GX , with ⟨a0, z ⟩ ≤ H, and note that H ⊳ X. Observe that GX is either dihedral or cyclic. Then we have the following two cases: Case 1: GX is dihedral. In this case, GX is either G or ⟨a2, b⟩. Then H = ⟨a, b ⟩⋊ ⟨z⟩ or H = ⟨a2, b ⟩⋊ ⟨z⟩. Note that ⟨a4⟩ ≤ H′ < G and H′ char H ⊳ X, and thus ⟨a4⟩ ≤ H′ ≤ GX ⊳ X. Since GX = ⟨a0⟩, we have a4 = a0, and so n = 4, contradict to n ≥ 5. Case 2: GX is cyclic. Assume that GX = ⟨ai⟩ is cyclic. By Lemma 5.2, we get GX = ⟨a1⟩, and therefor H := ⟨a1⟩ ⋊ ⟨z⟩ ⊳ X. Observe that H ∼= D8 or H ∼= Z4 × Z2. If H ∼ = D8, then ⟨a1⟩char H ⊳X, which implies ⟨a1⟩ ≤ GX , a contradiction. Then H = Z4 ×Z2. Note that X/C X(⟨a0⟩ × ⟨z⟩) ∼= Z2 and c ∈ CX (⟨a0⟩ × ⟨z⟩). By Lemma 5.3, ⟨a⟩⟨c⟩ is not a group, and hence a ̸∈CX (⟨a0⟩ × ⟨z⟩). Set ac 1 = ai 1z and za = a0z, where i ∈ { 1, −1}. Note that ⟨a2, c 2⟩ ≤ CX (H) as ac2 1 = ai2 1 = a1 and za2 = z. Since [ a1, b ] ̸= 1 and [ a, z ] ̸= 1, we have G/C X (H) ∩ G ∼ = Z2 ×Z2. If X = GCX (H), then ⟨a1⟩⊳X, a contradiction. Thus GCX (H) < X and |X| |CX (H)| ≥ 23. Note that X/C X(H) ⪅ Aut (H) ∼= D8. Hence CX (H) = ⟨a2⟩⟨c2⟩⊳ X and X/C X(H) ∼= D8. Let K := GCX (H) = G⟨c2⟩. Note that a1 ∈ GK , by Theorem 1.3, GK = G or GK = ⟨a2, b ⟩ for some b ∈ G \\ ⟨a⟩. If GK = G, then ⟨a2⟩ ≤ K′ ≤ G, and so K′ ⊳ X, and hence ⟨a2⟩ ≤ GX , a contradiction. Thus GK = ⟨a2, b ⟩ and X = (( ⟨a2, b ⟩⋊ ⟨c2⟩). ⟨a⟩)⟨c⟩. Note that ⟨a2⟩⊳ K as ⟨a2⟩char GK ⊳ K. Obviously, ⟨a2⟩ ⊳ CX (H), and hence CX (H)′ ≤ ⟨ a2⟩. Note that CX (H)′ ⊳ X as CX (H)′ char CX (H) ⊳ X. Then CX (H)′ ≤ ⟨a0⟩. Thus [ a2, c 4] = 1. Since 11K/ ⟨a2⟩ ⋊ ⟨c2⟩ ∼= Z2 2 and G < K , this means that K′ ≤ Ω( K) < ⟨a2⟩⟨c2⟩, and so we set K′ = ⟨a2⟩×⟨c4j ⟩ for some integer j. Since GX = ⟨a0⟩ and Ω o(c4j )(K′) char K ⊳X, therefor Ω o(c4j )(K′) = ⟨a0⟩, and so K′ = ⟨a2⟩ × ⟨c4⟩. □ Lemma 6.4 Suppose that |GX | = 2 . Then X = ⟨a, b, c |R, (a2)c2 = a2, (c2)a = a2sc−2, (c2)b = a2uc2, a c = bc2y⟩, where sy ≡ 1 + i2n−3(mod 2 n−2) and yu ≡ − 1(mod 2 n−3), i = 1 if G is a generalized quaternion group and i = 0 if G is either a dihedral group or a semidihedral group. Proof Since X/ ⟨a2⟩⋊⟨c2⟩ = ⟨ a, b⟩⟨c⟩ ∼= D8, we can choose b such that the form of X/M X is the following: ac = b and b c = a. Set c1 := c2 and a1 := a2. Noting ⟨a2, b ⟩ ⊳ ⟨a, b, c 1⟩, we can set ac1 1 = ar 1, c a 1 = as 1ct 1, c b 1 = au 1 c1 and ac = bcy 1. Then one can check bc = a1−2src1−t−y 1 . Set H := ⟨a1⟩ ⋊ ⟨c1⟩. Then H′ ≤ ⟨ a1⟩. Since H′ char H ⊳ X and |GX | = 2, we get H′ ≤ ⟨ a0⟩, which implies c2 1 ∈ CX (a1). If y ≡ 0(mod 2 n−1), then o( a) = o( ac) = o( b) is either 2 or 4, which implies |G| = 4 or 8, a contradiction. Therefore, y ̸≡0(mod 2 n−1). What we should to determine the parameters r, s, t, u and v by analysing three exten- sions. (1) ⟨a1⟩ ⋊ ⟨c1⟩, where ac1 1 = ar 1. Set π1 ∈ Aut (⟨a2⟩) such that π1(a2) = a2r. As mentioned before, this extension is valid if and only if o( π1(a2)) = o( a2) = 2 n−1 and π2 1 = 1, that is r is either 1 or 1 + 2 n−3. (2) ( ⟨a1⟩ ⋊ ⟨c1⟩). ⟨b⟩, where cb 1 = au 1 c1. Set π2 ∈ Aut ((⟨a1⟩ ⋊ ⟨c1⟩): a1 → a−1 1 and c1 → au 1 c1. Note that b2 ∈ ⟨a0⟩ ≤ Z(X), thus one can check that π2 preserves ac 1 = ar 1, o(π2(c)) = 2 m and π2 2 = Inn( b2). (3) (( ⟨a1⟩ ⋊ ⟨c1⟩). ⟨b⟩). ⟨a⟩, where ca 1 = as 1ct 1. Set π3 ∈ Aut (⟨a1⟩ ⋊ ⟨c1⟩). ⟨b⟩): a1 → a1, c1 → as 1ct 1 and b → ba1 or ba1a0. (i) π3 preserves ( a2)c = a2r, as desired. (ii) π3 preserves cb 1 = au 1 c1, that is ( as 1ct 1)ba = au 1 c1: au 1 c1 = ( as 1ct 1)ba = a−s 1 ct2 1 a (u+s) ∑ t l=1 rl 1 , that is t2 ≡ 1(mod 2 n−2) and ( u + s)(r + 1) t − 1 2 ≡ 0(mod 2 n−2). (12) (iii) o( π3(c1)) = 2 n−2: 1 = ( as 1ct 1)2n−2 = a s ∑ 2n−2 l=1 rl 1 , 12that is s 2n−2 ∑ l=1 rl ≡ 0(mod 2 n−2). (13) (iv) π2 3 = Inn( a1): Recall that Inn( a1)(c1) = c1a1−r 1 . c1a1−r 1 = Inn( a2)(c1) = π2 3(c1) = ( as 1ct 1)a = ct2 asr+s ∑ t l=1 rl , that is t2 ≡ 1(mod 2 n−2) and 1 − r ≡ sr + s t∑ l=1 rl(mod 2 n−2). (14) (4) ((⟨a2, b ⟩⋊⟨c2⟩). ⟨a⟩). ⟨c⟩, where ac = bcy 1 and bc = a1−2src1−t−y 1 . Set π4 ∈ Aut (⟨a, b, c 1⟩) : a → bcy 1, b → a1−2src1−t−y 1 and c1 → c1. Let i = 1 if G is a generalized quaternion group and i = 0 if G is either a dihedral group or a semidihedral group. We need to carry out the following seven steps: (i) o( π(a)) = 2 n−1: Since a0 ∈ Z(X), we only show ( ac)2n−2 = a0 (bc2w)2n−2 = a u2n−3 ∑ y l=1 rl 1 = a2n−3 1 , that is u y∑ l=1 rl ≡ 1(mod 2) , (15) which implies that both u and y are odd. (ii) π4 preserves ac1 1 = ar 1: (ac1 1 )c = c2yau ∑ y l=1 rl+i2n−3 and ( ar 1)c = c2yr au ∑ y l=1 rl+i2n−3 , that is 2y(r − 1) ≡ 0(mod 2 n−2). (16) (iii) π4 preserves ca 1 = as 1ct 1: (ca 1)c = aur 1 c1 and ( as 1ct 1)c = a s(ur ∑ y l=1 rl+i2n−3) 1 ct+2ys 1 , that is ur ≡ s(ur y∑ l=1 rl + i2n−3)(mod 2 n−2) and 1 ≡ t + 2ys(mod 2 n−2). (17) 13(iv) π preserves cb 1 = au 1 c1: (cb 1)c = ct 1asr+2(sr−1)(1−r) 1 and ( au 1 c1)c = c2yu 1 a u(ur ∑ y l=1 rl+i2n−3) 1 c1, that is, t ≡ 1 + 2 yu(mod 2 n−2) and sr ≡ u2 y∑ l=1 rl + i2n−3(mod 2 n−2). (18) (v) π2 = Inn( c1): Recall Inn( c1)(a) = a1−2src1−t 1 , Inn(c1)(a1) = ar 1 and Inn( c1)(b) = aur 1 b. a1−2src2−2t = Inn( c1)(a) = π2(a) = bcc2y = a1−2src2−2t−2y+2y, as desired; ar 1 = Inn( c1)(a2) = π2(a2) = ( c2y 1 a ur ∑ y l=1 rl+i2n−3 1 )c = c 2y+2yur ∑ y l=1 rl 1 a (u ∑ y l=1 rl)2 1 , that is 2y + 2yur y∑ l=1 rl ≡ 0(mod 2 n−2) and r ≡ (u y∑ l=1 rl)2(mod 2 n−2); (19) and a2urb = Inn( c1)(b) = π2(b) = ( a1−2src1−t−y 1 )c = a −su ∑ y l=1 rl+i2n−3 1 c−2sy 1 bc1−t 1 , as desired. Now we are ready to determine the parameters by summarizing Eq(1 2)-Eq(19). By Eq(17) and Eq(18), we get 2( u+s) ≡ 0(mod 2 n−2) and s2r ≡ u2(mod 2 n−2). Noting that s2 ≡ u2(mod 2 n−2) and u is odd, we get that s is odd and r = 1. Inserting r = 1 into Eq(12)-Eq(19), we get t = −1 by Eq(14). Then we get sy ≡ 1 + i2n−3(mod 2 n−2) by Eq(17) and yu ≡ − 1(mod 2 n−3) by Eq(18). □ References [1] B. Amberg, and L. Kazarin, Factorizations of groups and r elated topics. Sci. China Ser. A 52 (2009)(2), 217–230. [2] M. Bachrat´ y, M. Conder and G. Verret, Skew-product grou ps for monolithic groups, arXiv:1905.00520v1, 2019. [3] J.Y. Chen, S.F. Du and C.H. Li, Skew-morphisms of nonabel ian characteristically simple groups, J. Combin. Theory Ser. A185 (2022), paper No. 105539, 17 pp. 14[4] M. Conder, R. Jajcay and T. Tucker, Cyclic complements an d skew-morphisms of groups, J. Algebra453 (2016), 68–100. [5] M. Conder and R. Tucker, Regular Cayley maps for cyclic gr oups, Trans. Amer. Math. Soc. 366 (2014), 3585–3609. [6] J. Douglas, On the supersolvability of bicyclic groups, Proc. Nat. Acad. Sci. U.S.A.47 (1961), 1493–1495. [7] S.F. Du, A. Malniˇ c and D. Maruˇ siˇ c, Classiﬁcation of 2- arc-transitive dihedrants, Journal of Combinatorial Theory98.6 (2008), 1349–1372. [8] S.F. Du and K. Hu, Skew-morphisms of cyclic 2-groups, J. Group Theory 22 (2019)(4), 617–635. [9] S.F. Du, W.J. Luo, H. Yu and J.Y. Zhang, Skew-morphisms of elementary abelian p-groups, arXiv:2205.07734, 2022. [10] W. Gasch¨ utz, Zur Erweiterunstheorie endlicher Grupp en, J. Math. 190 (1952), 93–107. [11] M. Hall, The Theory of Groups, Macmillan 1959. [12] K. Hu, I. Kov´ acs and Y. S. Kwon, Classiﬁcation of skew mo rphisms of dihedral groups, J. Group Theory(2022), https://doi.org/10.1515/jgth-2022-0085. [13] K. Hu and D.Y. Ruan, Smooth skew morphisms of dicyclic gr oups, J. Algebraic Combin 56 (2022), 1119–1134. [14] B. Huppert, Endliche Gruppen I, Springer, Berlin,(1967). [15] N. Itˆ o, ¨Uber das Produkt von zwei abelschen Gruppen, Math. Z. 62 (1955), 400–401. [16] R. Jajcay and J. ˇSir´ aˇ n, Skew-morphisms of regular Cayley maps, Disc. Math. 224(2002), 167–179. [17] O.H. Kegel, Produkte nilpotenter Gruppen, Arch Math12 (1961), 90–93. [18] I. Kov´ acs and R. Nedela, Decomposition of skew-morphi sms of cyclic groups, Ars Math. Contemp. 4 (2011), 329–349. [19] I. Kov´ acs and R. Nedela, Skew-morphisms of cyclic p-groups, J. Group Theory20 (2017)(6), 135–154. [20] I. Kov´ acs, D. Maruˇ siˇ c and M.E. Muzychuk, On G-arc-regular dihedrants and regular dihe- dral maps, J. Algebraic Combin.38 (2013), 437–455. [21] I. Kov´ acs and Y. S. Kwon, Classiﬁcation of reﬂexible Ca yley maps for dihedral groups, J. Combin. Theory Ser. B127 (2017), 187–204. [22] I. Kov´ acs and Y.S. Kwon, Regular Cayley maps on dihedra l groups with smallest kernel, J. Algebraic Combin.44 (2016), 831–847. 15[23] I. Kov´ acs and Y.S. Kwon, Regular Cayley maps for dihedr al groups, J. Combin. Theory Ser. B 148 (2021), 84–124. [24] Y.S. Kwon, A classiﬁcation of regular t-balanced Cayley maps for cyclic groups, Disc. Math. 313 (2013), 656–664. [25] J.H. Kwak, Y.S. Kwon and R. Feng, A classiﬁcation of regu lar t-balanced Cayley maps on dihedral groups, European J. Combin.27 (2006), 382–393. [26] J.H. Kwak and J.M. Oh, A classiﬁcation of regular t-bala nced Cayley maps on dicyclic groups, European J. Combin.29 (2008), 1151–1159. [27] C.H. Li and B.Z. Xia, Factorizations of almost simple gr oups with a solvable factor, and Cayley graphs of solvable groups. Mem. Amer. Math. Soc.279 (2022)(1375), v+99 pp. [28] M.W. Licheck, C. E. Prager and J. Saxl, The maximal facto rizationof the ﬁnite simple groups and their automorphism groups, Mem. Amer: Math.Soc.432 (1990), 1–151. [29] A. Lucchini, On the order of transitive permutation gro ups with cyclic point-stabilizer, Atti. Accad. Naz. Lincei CI. Sci. Fis. Mat. Natur. Rend. Lincei (9) Mat. Appl. 9 (1998), 241–243. [30] V.S. Monakhov, The product of two groups, one of which co ntains a cyclic subgroup of index ≤ 2, Mathematical Notes of the Academy of Sciences of the Ussr16.2 (1974), 757–762. [31] B. Richter, J. ˇSir´ aˇ n, R. Jajcay, T. Tucker, and M. Watkins. Cayley maps, J. Combin. Theory Ser. B95 (2005), 189–245. [32] M. Suzuki, Group Theory. I, Springer 1982. [33] Y. Wang and R. Feng, Regular Cayley maps for cyclic, dihe dral and generalized quaternion groups, Acta Math. Sin. (Engl. Ser.)21 (2005), 773–778. [34] N.E. Wang, K. Hu, K. Yuan, and J.Y. Zhang, Smooth skew mor phisms of dihedral groups, Ars Math. Contemp.16 (2019), 527–547. [35] H. Wielandt, Finite permutation groups. Academic Pr.1964. [36] H. Wielandt, ¨Uber Produkte von nilpotenter Gruppen, Illinois J. Math2 (1958), 611–618. [37] J.S. Wilson, Products of Groups, Oxford Mathematical Monographs1999. [38] K. Yuan, Y. Wang and H.P. Qu, Classiﬁcation of regular ba lanced Cayley maps of minimal non-abelian metacyclic groups, Ars Math. Contemp.14 (2018) 433–443. [39] K. Yuan, Y. Wang and H.P. Qu, Regular balanced Cayley map s on nonabelian metacyclic groups of odd order, Art Discrete Appl. Math.3 (2020) P1.05. [40] J.Y. Zhang, A classiﬁcation of regular Cayley maps with trivial Cayley-core for dihedral groups, Discrete Math. 338 (2015), 1216–1225. 16[41] J.Y. Zhang, Regular Cayley maps of skew-type 3 for dihed ral groups, Discrete Math 388 (2015), 1163–1172. [42] J.Y. Zhang and S.F. Du, On the skew-morphisms of dihedra l groups, J. Group Theory19 (2016), 993–1016. 17",
      "references": [
        "Factorizations of groups and related topics",
        "Skew-product groups for monolithic groups",
        "Skew-morphisms of nonabelian characteristically simple groups",
        "Cyclic complements and skew-morphisms of groups",
        "Regular Cayley maps for cyclic groups",
        "On the supersolvability of bicyclic groups",
        "Classification of 2-arc-transitive dihedrants",
        "Skew-morphisms of cyclic 2-groups",
        "Skew-morphisms of elementary abelian p-groups",
        "Zur Erweiterunstheorie endlicher Gruppen",
        "The Theory of Groups",
        "Classification of skew morphisms of dihedral groups",
        "Smooth skew morphisms of dicyclic groups",
        "Endliche Gruppen I",
        "Über das Produkt von zwei abelschen Gruppen",
        "Skew-morphisms of regular Cayley maps",
        "Produkte nilpotenter Gruppen",
        "Decomposition of skew-morphisms of cyclic groups",
        "Skew-morphisms of cyclic p-groups",
        "On G-arc-regular dihedrants and regular dihedral maps",
        "Classification of reflexible Cayley maps for dihedral groups",
        "Regular Cayley maps on dihedral groups with smallest kernel",
        "Regular Cayley maps for dihedral groups",
        "A classification of regular t-balanced Cayley maps for cyclic groups",
        "A classification of regular t-balanced Cayley maps on dihedral groups",
        "A classification of regular t-balanced Cayley maps on dicyclic groups",
        "Factorizations of almost simple groups with a solvable factor, and Cayley graphs of solvable groups",
        "The maximal factorizationof the finite simple groups and their automorphism groups",
        "On the order of transitive permutation groups with cyclic point-stabilizer",
        "The product of two groups, one of which contains a cyclic subgroup of index ≤ 2",
        "Cayley maps",
        "Group Theory. I",
        "Regular Cayley maps for cyclic, dihedral and generalized quaternion groups",
        "Smooth skew morphisms of dihedral groups",
        "Finite permutation groups",
        "Über Produkte von nilpotenter Gruppen",
        "Products of Groups",
        "Classification of regular balanced Cayley maps of minimal non-abelian metacyclic groups",
        "Regular balanced Cayley maps on nonabelian metacyclic groups of odd order",
        "A classification of regular Cayley maps with trivial Cayley-core for dihedral groups",
        "Regular Cayley maps of skew-type 3 for dihedral groups",
        "On the skew-morphisms of dihedral groups"
      ],
      "meta_data": {
        "arxiv_id": "2309.03446v1",
        "authors": [
          "Wenjuan Luo",
          "Hao Yu"
        ],
        "published_date": "2023-09-07T02:00:02Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This paper classifies skew product groups for maximal class 2-groups. It addresses the main research problem of determining skew-morphisms, which generalize automorphisms and are fundamental for studying regular Cayley maps and finite groups with a complementary factorization X = GY where Y is cyclic and core-free in X. The paper presents several theorems (1.1, 1.2, 1.3, 1.4) that characterize and classify these groups under specific conditions, particularly when G is a p-group or a maximal class 2-group, and the cyclic factor C is core-free in X.",
        "methodology": "The methodology is rooted in finite group theory. It involves defining and utilizing concepts such as skew-morphisms, skew product groups, core-free subgroups, and characteristic subgroups. The classification relies on analyzing group extensions, properties of Sylow subgroups, Fitting subgroups, and Frattini subgroups. Proofs often employ induction (e.g., on the order of the group), and detailed algebraic derivations to determine parameters (r, s, t, u, v) that define the group structures. The paper systematically deals with different cases based on the normal subgroup GX (G ⊳ X, GX = ⟨a2, b⟩, or |GX| = 2) using established group theory propositions and lemmas.",
        "experimental_setup": "This is a theoretical mathematics paper, specifically in group theory, and therefore does not involve an 'experimental setup' in the traditional sense of empirical data, benchmarks, or physical experiments. The 'setup' involves defining abstract group structures and their relations, such as D2n, Q2n, and SD2n, and imposing conditions like |G| = 2^n ≥ 32, G ∩ C = 1, and CX = 1. Validation is achieved through rigorous mathematical proofs and logical derivations based on established axioms and theorems of group theory.",
        "limitations": "The classification presented is specifically for 'maximal class 2-groups,' limiting its direct applicability to other types of p-groups or 2-groups that do not meet the maximal class criterion. Several results are contingent on the order of the group, specifically for n ≥ 5 (e.g., |G| ≥ 32), with smaller cases like D4 and D8 noted as exceptions for certain lemmas. The crucial assumption that the cyclic factor C is core-free in X (CX = 1) is a significant constraint. The paper also acknowledges that the broader problem of completely determining skew-morphisms for even cyclic groups remains unsolved, highlighting the complexity and partial nature of current understanding in the field.",
        "future_research_directions": "The paper implicitly suggests several avenues for future research. One primary direction is the complete determination of skew-morphisms for cyclic groups, which is noted as an ongoing challenging problem. Another is to extend the classification of skew product groups to other families of groups beyond maximal class 2-groups. Further exploration could involve investigating skew-morphisms for p-groups where p > 2, or for 2-groups that do not fall into the 'maximal class' category. The implications of such classifications for the study of regular Cayley maps of these different group families also present fertile ground for future work.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Complexity-Based Prompting for Multi-step Reasoning",
      "full_text": "Statistical Efﬁciency of Score Matching: The View from Isoperimetry Frederic Koehler* Alexander Heckett† Andrej Risteski‡ December 26, 2022 Abstract Deep generative models parametrized up to a normalizing constant (e.g. energy-based models) are difﬁcult to train by maximizing the likelihood of the data because the likelihood and/or gradients thereof cannot be explicitly or efﬁciently written down. Score matching is a training method, whereby instead of ﬁtting the likelihood log p(x) for the training data, we instead ﬁt the score function ∇x log p(x) — obviating the need to evaluate the partition function. Though this estimator is known to be consistent, its unclear whether (and when) its statistical efﬁciency is comparable to that of maximum likelihood — which is known to be (asymptotically) optimal. We initiate this line of inquiry in this paper, and show a tight connection between statistical efﬁciency of score matching and the isoperi- metric properties of the distribution being estimated — i.e. the Poincar ´e, log-Sobolev and isoperimetric constant — quantities which govern the mixing time of Markov processes like Langevin dynamics. Roughly, we show that the score matching estimator is statistically comparable to the maximum likelihood when the distribution has a small isoperimetric constant. Conversely, if the distribution has a large isoperimetric constant — even for simple families of distributions like exponential families with rich enough sufﬁcient statistics — score matching will be substantially less efﬁcient than maximum likelihood. We suitably formalize these results both in the ﬁnite sample regime, and in the asymptotic regime. Finally, we identify a direct parallel in the discrete setting, where we connect the statistical properties of pseudolikelihood estimation with approximate tensorization of entropy and the Glauber dynamics. 1 Introduction Energy-based models (EBMs) are deep generative models parametrized up to a constant of parametrization, namely p(x) ∝exp(f(x)). The primary training challenge is the fact that evaluating the likelihood (and gradients thereof) requires evaluating the partition function of the model, which is generally computationally intractable — even when using relatively sophisticated MCMC techniques. Recent works, including the seminal paper of Song and Ermon [2019], circumvent this difﬁculty by instead ﬁtting the score function of the model, that is ∇xlog p(x). Though not obvious how to evaluate this loss from training samples only, Hyv¨arinen [2005] showed this can be done via integration by parts, and the estimator is consistent (that is, converges to the correct value in the limit of inﬁnite samples). The maximum likelihood estimator is the de-facto choice for model-ﬁtting for its well-known property of being statistically optimal in the limit where the number of samples goes to inﬁnity [Van der Vaart, 2000]. It is unclear how much worse score matching can be — thus, it’s unclear how much statistical efﬁciency we sacriﬁce for the algorithmic convenience of avoiding partition functions. In the seminal paper [Song and Ermon, 2019], it was conjectured that multimodality, as well as a low-dimensional manifold structure may cause difﬁculties for score matching — which was the reason the authors proposed annealing by convolving the input samples with a sequence of Gaussians with different variance. Though the intuition for this is natural: having poor estimates for the score in “low probability” regions of *fkoehler@stanford.edu, Stanford University. Supported in part by NSF award CCF-1704417, NSF award IIS-1908774, and N. Anari’s Sloan Research Fellowship. †aheckett@andrew.cmu.edu, Carnegie Mellon University. ‡aristesk@andrew.cmu.edu, Carnegie Mellon University. Supported in part by NSF award IIS-2211907 and an Amazon Research Award on “Causal + Deep Out-of-Distribution Learning”. 1 arXiv:2210.00726v2  [cs.LG]  22 Dec 2022the distribution can “propagate” into bad estimates for the likelihood once the score vector ﬁeld is “integrated” — making this formal seems challenging. We show that the right mathematical tools to formalize, and substantially generalize such intuitions are functional analytic tools that characterize isoperimetric properties of the distribution in question. Namely, we show three quanti- ties, the Poincar´e, log-Sobolev and isoperimetric constants (which are all in turn very closely related, see Section 2), tightly characterize how much worse the efﬁciency of score matching is compared to maximum likelihood. These quantities can be (equivalently) viewed as: (1) characterizing the mixing time of Langevin dynamics — a stochastic differential equation used to sample from a distribution p(x) ∝exp(f(x)), given access to a gradient oracle for f; (2) characterizing “sparse cuts” in the distribution: that is sets S, for which the surface area of the set Scan be much smaller than the volume of S. Notably, multimodal distributions, with well-separated, deep modes have very big log- Sobolev/Poincar´e/isoperimetric constants [Gayrard et al., 2004, 2005], as do distributions supported over manifold with negative curvature [Hsu, 2002] (like hyperbolic manifolds). Since it is commonly thought that complex, high di- mensional distribution deep generative models are trained to learn do in fact exhibit multimodal and low-dimensional manifold structure, our paper can be interpreted as showing that in many of these settings, score matching may be substantially less statistically efﬁcient than maximum likelihood. Thus, our results can be thought of as a formal jus- tiﬁcation of the conjectured challenges for score matching in Song and Ermon [2019], as well as a vast generalization of the set of “problem cases” for score matching. This also shows that surprisingly, the same obstructions for efﬁcient inference (i.e. drawing samples from a trained model, which is usual done using Langevin dynamics for EBMs) are also an obstacle for efﬁcient learning using score matching. We roughly show the following results: 1. For ﬁnite number of samplesn, we show that if we are trying to estimate a distribution from a class with Rademacher complexity bounded by Rn, as well as a log-Sobolev constant bounded by CLS, achieving score matching loss at most ϵimplies that we have learned a distribution that’s no more than ϵCLSRn away from the data distribution in KL divergence. The main tool for this is showing that the score matching objective is at most a multiplicative factor of CLS away from the KL divergence to the data distribution. 2. In the asymptotic limit (i.e. as the number of samples n →∞), we focus on the special case of estimating the parameters θ of a probability distribution of an exponential family {pθ(x) ∝exp(⟨θ,F (x)⟩) for some sufﬁcient statistics F using score matching. If the distribution pθ we are estimating has Poincar ´e constant bounded by CP have asymptotic efﬁciency that differs by at most a factor ofCP. Conversely, we show that if the family of sufﬁcient statistics is sufﬁciently rich, and the distribution pθ we are estimating has isoperimetric constant lower bounded by CIS, then the score matching loss is less efﬁcient than the MLE estimator by at least a factor of CIS. 3. Based on our new conceptual framework, we identify a precise analogy between score matching in the continuous setting and pseudolikelihood methods in the discrete (and continuous) setting. This connection is made by replacing the Langevin dynamics with its natural analogue — the Glauber dynamics (Gibbs sampler). We show that the approximation tensorization of entropy inequality [Marton, 2013, Caputo et al., 2015], which guarantees rapid mixing of the Glauber dynamics, allows us to obtain ﬁnite-sample bounds for learning distributions in KL via pseudolikelihood in an identical way to the log-Sobolev inequality for score matching. A variant of this connection is also made for the related ratio matching estimator of Hyv¨arinen [2007b]. 4. In Section 7, we perform several simulations which illustrate the close connection between isoperimetry and the performance of score matching. We give examples both when ﬁtting the parameters of an exponential family and when the score function is ﬁt using a neural network. 2 Preliminaries Deﬁnition 1 (Score matching). Given a ground truth distribution p with sufﬁcient decay at inﬁnity and a smooth distribution q, the score matching loss (at the population level) is deﬁned to be Jp(q) := 1 2EX∼p[∥∇log p(X) −∇log q(X)∥2] + Kp = EX∼p [ Tr∇2 log q+ 1 2∥∇log q∥2 ] (1) 2where Kp is a constant independent of q. The last equality is due to Hyv ¨arinen [2005]. Given samples from p, the training loss ˆJp(q) is deﬁned by replacing the rightmost expectation with the average over data. Functional and Isoperimetric Inequalities. Let q(x) be a smooth probability density over Rd. A key role in this work is played by the log-Sobolev, Poincar´e, and isoperimetric constants of q— closely related geometric quantities, connected to the mixing of the Langevin dynamics, which have been deeply studied in probability theory and geometric and functional analysis (see e.g. [Gross, 1975, Ledoux, 2000, Bakry et al., 2014]). Deﬁnition 2. The log-Sobolev constant CLS(q) ≥0 is the smallest constant so that for any probability density p(x) KL(p,q) ≤CLS(q)I(p|q) (2) where KL(p,q) = EX∼p[log(p(X)/q(X))] is the Kullback-Leibler divergence or relative entropy and the relative Fisher information I(p|q) is deﬁned 1 as I(p|q) := Eq ⣨ ∇log p q,∇p q ⟩ . The log-Sobolev inequality is equivalent to exponential ergodicity of the Langevin dynamics for q, a canonical Markov process which preserves and is used for sampling q, described by the Stochastic Differential Equation dXt = −∇log q(Xt) dt+ √ 2 dBt. Precisely, if ptis the distribution of the continuous-time Langevin Dynamics2 for qstarted from X0 ∼p, then I(p|q) = −d dt KL(pt,q) |t=0 and so by integrating KL(pt,q) ≤e−t/CLS KL(p,q). (3) This holding for anypand tis an equivalent characterization of the log-Sobolev constant (Theorem 3.20 of Van Handel [2014]). For a class of distributions P, we can also deﬁne the restricted log-Sobolev constant CLS(q,P) to be the smallest constant such that (2) holds under the additional restriction that p∈P — see e.g. Anari et al. [2021b]. For P an inﬁnitesimal neighborhood of p, the restricted log-Sobolev constant of qbecomes half of the Poincar´e constant or inverse spectral gap CP(q): Deﬁnition 3. The Poincar´e constant CP(q) ≥0 is the smallest constant so that for any smooth function f, Varq(f) ≤CP(q)Eq∥∇f∥2. (4) It is related to the log-Sobolev constant by CP ≤2CLS (Lemma 3.28 of Van Handel [2014]). Similarly, the Poincar´e inequality implies exponential ergodicity for the χ2-divergence: χ2(pt,q) ≤e−2t/CPχ2(p,q). This holding for every pand tis an equivalent characterization of the Poincar´e constant (Theorem 2.18 of Van Handel [2014]). We can equivalently view the Langevin dynamics in a functional-analytic way through its deﬁnition as a Markov semigroup, which is equivalent to the SDE deﬁnition via the Fokker-Planck equation [Van Handel, 2014, Bakry et al., 2014]. From this perspective, we can write pt = qHt p q where Ht is the Langevin semigroup for q, so Ht = etL with generator Lf = ⟨∇log q,∇f⟩+ ∆f. In this case, the Poincar ´e constant has a direct interpretation in terms of the inverse spectral gap of L, i.e. the inverse of the gap between its two largest eigenvalues. Both Poincar ´e and log-Sobolev inequalities measure the isoperimetric properties of q from the perspective of functions; they are closely related to the isoperimetric constant: 1There are several alternatives formulas for I(p | q), see Remark 3.26 of Van Handel [2014]. 2See e.g. Vempala and Wibisono [2019] for more background and the connection to the discrete time dynamics. 3Deﬁnition 4. The isoperimetric constant CIS(q) is the smallest constant, s.t. for every set S, min {∫ S q(x)dx, ∫ SC q(x)dx } ≤CIS(q) lim inf ϵ→0 ∫ Sϵ q(x)dx− ∫ Sq(x)dx ϵ . (5) where Sϵ = {x : d(x,S) ≤ϵ}and d(x,S) denotes the (Euclidean) distance of xfrom the set S. The isoperimetric constant is related to the Poincar´e constant by CP ≤4C2 IS (Proposition 8.5.2 of Bakry et al. [2014]). Assuming Sis chosen so ∫ Sq(x)dx< 1/2, the left hand side can be interpreted as the volume and the right hand side as the surface area of Swith respect to q. A strengthened isoperimetric inequality (Bobkov inequality) upper bounds the log-Sobolev constant, see Ledoux [2000], Bobkov [1997]. Molliﬁers We recall the deﬁnition of one of the standard molliﬁers/bump functions, as used in e.g. H ¨ormander [2015]. Molliﬁers are smooth functions useful for approximating non-smooth functions: convolving a function with a molliﬁer makes it “smoother”, in the sense of the existence and size of the derivatives. Precisely, deﬁne the (inﬁnitely differentiable) function ψ: Rd →R as ψ(y) = { 1 Id e−1/(1−|y|2) for |y|<1 0 for |y|≥ 1 where Id := ∫ e−1/(1−|y|2)dy. We will use the basic estimate 8−dBd < Id < Bd where Bd is the volume of the unit ball in Rd, which follows from the fact that e−1/(1−|y|2) ≥1/4 for ∥y∥≤ 1/2 and e−1/(1−|y|2) ≤1 everywhere. ψis inﬁnitely differentiable and its gradient is ∇yψ(y) = −(2/Id)e−1/(1−∥y∥2) y (1 −∥y∥2)2 = −2y (1 −∥y∥2)2 ψ(y) It is straightforward to check that supy∥∇yψ(y)∥<1/Id. For γ >0, we’ll also deﬁne a “sharpening” ofψ, namely ψγ(y) = γ−dψ(y/γ) so that ∫ ψγ = 1 and (by chain rule) ∇yψγ(y) = γ−d−1(∇ψ)(y/γ) = −2y/γ2 (1 −∥y/γ∥2)ψγ(y/γ) so in particular ∥∇yψγ∥2 ≤γ−d−1/Id. Glauber dynamics. The Glauber dynamics will become important in Section 5 as the natural analogue of the Langevin dynamics. The Glauber dynamics or Gibbs sampler for a distribution p is the standard sampler for dis- crete spin systems — it repeatedly selects a random coordinate and then resamples the spin Xi there according to the distribution pconditional on all of the other ones (i.e. conditional on X∼i). See e.g. Levin and Peres [2017]. This is the standard sampler for discrete systems, but it also applies and has been extensively studied for continuous ones (see e.g. Marton [2013]). Reach and Condition Number of a Manifold. For a smooth submanifold Mof Euclidean space, the reach τM is the smallest radius r so that every point with distance at most r to the manifold Mhas a unique nearest point on M[Federer, 1959]; the reach is guaranteed to be positive for compact manifolds. The reach has a few equivalent characterizations (see e.g. Niyogi et al. [2008]); a common terminology is that the condition number of a manifold is 1/τM. Notation. For a random vector X, ΣX := E[XXT] −E[X]E[X]T denotes its covariance matrix. 43 Learning Distributions from Scores: Nonasymptotic Theory Though consistency of the score matching estimator was proven in Hyv ¨arinen [2005], it is unclear what one can conclude about the proximity of the learned distribution from a ﬁnite number of samples. Precisely, we would like a guarantee that shows that if the training loss (i.e. empirical estimate of (1)) is small, the learned distribution is close to the ground truth distribution (e.g. in the KL divergence sense). However, this is not always true! We will see an illustrative example where this is not true in Section 7 and also establish a general negative result in Section 4. In this section, we prove (Theorem 1) that minimizing the training loss does learn the true distribution, assuming that the class of distributions we are learning have bounded complexity and small log-Sobolev constant. First, we formalize the connection to the log-Sobolev constant: Proposition 1. The log-Sobolev inequality for qis equivalent to the following inequality over all smooth probability densities p: KL(p,q) ≤2CLS(q)(Jp(q) −Jp(p)). (6) More generally, for a class of distributionp∈P the restricted log-Sobolev constant is the smallest constant such that KL(p,q) ≤CLS(q,P)(Jp(q) −Jp(p)) for all distributions p. Proof. This follows from the following equivalent form for the relative Fisher information (e.g. Shao et al. [2019], Vempala and Wibisono [2019]) I(p|q) = Eq⟨∇p q,∇log p q⟩ = Ep⟨q p∇p q,∇log p q⟩= Ep⟨∇log p q,∇log p q⟩= Ep∥∇log p−∇log q∥2. (7) Using this and (1) the log-Sobolev inequality can be rewritten as KL(p,q) ≤CLS(Jp(q) −Jp(q)) which proves the ﬁrst claim, and the same argument shows the second claim. Remark 1 (Interpretation of Score Matching) . The left hand side of (6) is KL(p,q) = Ep[log p] −Ep[log q]. The ﬁrst term is independent of qand the second term is the likelihood, the objective for Maximum Likelihood Estimation. So (6) shows that the score matching objective is a relaxation (within a multiplicative factor of CLS(q)) of maximum- likelihood via the log-Sobolev inequality. We discuss connections to other proposed interpretations in Appendix A. Remark 2. Interestingly, the log-Sobolev constant which appears in the bound is that of qand not pthe ground truth distribution. This is useful because q is known to the learner whereas pis only indirectly observed. If q is actually close to p, the log-Sobolev constants are comparable due to the Holley-Stroock perturbation principle (Proposition 5.1.6 of Bakry et al. [2014]). The connection between the score matching loss and the relative Fisher information used in (7) is not new to this work—see the Related Work section for more discussion and references. The useful statistical implications which we discuss next are new to the best of our knowledge. Combining Proposition 1, bounds on log-Sobolev constants from the literature, and fundamental tools from generalization theory allows us to derive ﬁnite-sample guarantees for learning distributions in KL divergence via score matching. 3 Theorem 1. Suppose that Pis a class of probability distributions containing pand deﬁne CLS(P,P) := sup q∈P CLS(q,P) ≤sup q∈P CLS(q) to be the worst-case (restricted) log-Sobolev constant in the class of distributions. (For example, if every distribution in Pis α-strongly log concave then CLS ≤1/2αby Bakry-Emery theory [Bakry et al., 2014].) Let Rn := EX1,...,Xn,ϵ1,...,ϵn sup q∈P 1 n n∑ i=1 ϵi [ Tr∇2 log q(Xi) + 1 2∥∇log q(Xi)∥2 ] 3We use the simplest version of Rademacher complexity bounds to illustrate our techniques. Standard literature, e.g. Shalev-Shwartz and Ben-David [2014], Bartlett et al. [2005] contains more sophisticated versions, and our techniques readily generalize. 5be the expected Rademacher complexity of the class givennsamples X1,...,X n ∼pi.i.d. and independentϵ1,...,ϵ n ∼ Uni{±1}i.i.d. Rademacher random variables. Let ˆp be the score matching estimator from n samples, i.e. ˆp = arg minq∈P ˆJp(q). Then EKL(p,ˆp) ≤4 CLS(P,P)Rn. In particular, if CLS(P,P) <∞then limn→∞EKL(p,ˆp) = 0 as long as limn→∞Rn = 0. Proof. By the standard symmetrization argument (Theorem 26.3 of Shalev-Shwartz and Ben-David [2014]) we have EJp(ˆp) −Jp(p) ≤2Rn, so by Proposition 1 we have EKL(p,ˆp) ≤ECLS(P)(Jp(ˆp) −Jp(p)) ≤2CLS(P)Rn. Example 1. Suppose we are ﬁtting an isotropic Gaussian inddimensions with unknown meanµ∗satisfying ∥µ∗∥≤ R. The class of distributionsPis qµwith ∥µ∥≤ Rof the form qµ(x) ∝exp ( −∥x−µ∥2/2 ) so the expected Rademacher complexity can be upper bounded as so: Rn = Esup µ 1 n n∑ i=1 ϵi [ −d/2 + 1 2∥Xi −µ∥2 ] = Esup µ ⟨ 1 n n∑ i=1 ϵiXi,µ ⟩ = RE  1 n n∑ i=1 ϵiXi ≤R √E  1 n n∑ i=1 ϵiXi  2 = R √ R2 + d n where the inequality is Jensen’s inequality and in the last step we expanded the square and used thatEϵiϵj = 1(i= j) and E∥Xi∥2 ≤R2 + d. Recall that the standard Gaussian distribution is 1-strongly log concave so CLS ≤1/2. Hence we have the concrete bound EKL(p,ˆp) ≤R √ R2+d n . 4 Statistical cost of score matching: asymptotic results In this section, we compare the asymptotic efﬁciency of the score matching estimator in exponential families to the efﬁency of the maximum likelihood estimator. Because we are considering asymptotics, we might expect (recall the discussion in Section 2) that the relevant functional inequality will be the local version of the log-Sobolev inequal- ity around the true distribution p, which is the Poincar ´e inequality for p. Our results will show precisely how this occurs and characterize the situations where score matching is substantially less statistically efﬁcient than maximum likelihood. Setup. In this section, we will focus on distributions from exponential families. We will consider estimating the parameters of an exponential family using two estimators, the classical maximum likelihood estimator (MLE), and the score matching estimator; we will use that the score matching estimator arg minθ′ ˆJp(pθ′) admits a closed-form formula in this setting. Deﬁnition 5 (Exponential family). For sufﬁcient statistics F : Rd →Rm, the exponential family of distributions associated with F is {pθ(x) ∝exp (⟨θ,F (x)⟩) |θ∈Θ ⊆Rm}. Deﬁnition 6 (MLE, Van der Vaart [2000]). Given i.i.d. samples x1,...,x n ∼pθ, the maximum likelihood estimator is ˆθMLE = arg maxθ′∈Θ ˆE[log pθ′(X)], where ˆE denotes the expectation over the samples. As n →∞ and under appropriate regularity conditions, we have √n ( ˆθMLE −θ ) →N(0,ΓMLE), where ΓMLE := Σ −1 F and ΣF is known as the Fisher information matrix. Proposition 2 (Score matching estimator, Equation (34) of Hyv¨arinen [2007b]). Given i.i.d. samplesx1,...,x n ∼pθ, the score matching estimator equals ˆθSM = −ˆE[(JF)X(JF)T X]−1 ˆE∆F, where (JF)X : m×dis the Jacobian of F at the point X, ∆f = ∑ i∂2 if is the Laplacian and it is applied coordinate wise to the vector-valued function F. 64.1 Asymptotic normality Next, we recall the asymptotic normality of the score matching estimator and give a formula for the limiting renor- malized covariance matrix ΓSM established by Forbes and Lauritzen [2015] (see also Theorem 6 of Barp et al. [2019] and Corollary 1 of Song et al. [2020]). Since the MLE also satisﬁes asymptotic normality with an explicit covariance matrix, we can then proceed in the next sections to compare their relative efﬁciency (as in e.g. Section 8.2 of Van der Vaart [2000]) by comparing the asymptotic covariancesΓSM and ΓMLE. Proposition 3 (Asymptotic normality, Forbes and Lauritzen [2015]). As n→∞, and assuming sufﬁcient smoothness and decay conditions so that score matching is consistent (see Hyv¨arinen [2005]) we have the following convergence in distribution: √n(ˆθSM −θ) →N(0,ΓSM), where ΓSM := E[(JF)X(JF)T X]−1Σ(JF)X(JF)T Xθ+∆FE[(JF)X(JF)T X]−1. (8) Proof. We include the proof for the reader’s convenience. From Hyv ¨arinen [2005], we have consistency of score matching (Theorem 2) and in particular the formula θ= −E[(JF)X(JF)T X]−1E∆F. (9) We now compute the limiting distribution of the estimator as the number of samples n →∞. We will need to use some standard results from probability theory such as Slutsky’s theorem and the central limit theorem, see e.g. Van der Vaart [2000] or Durrett [2019] for references. To minimize ambiguity, let ˆEn denote the empirical expectation over n i.i.d. samples samples and let ˆθn denote the score matching estimator ˆθSM from nsamples. Deﬁne δn,1 and δn,2 by the equations ˆEn[(JF)X(JF)T X] = E[(JF)X(JF)T X] + δn,1/√n and ˆEn∆F = E∆f + δn,2/√n. By the central limit theorem, δn = (δn,1,δn,2) converges in distribution to a multivariate Gaussian (with a covariance matrix that we won’t need explicitly) asn→∞. From the deﬁnition ˆθn = −ˆEn[(JF)X(JF)T X]−1 ˆE∆F = −[E[(JF)X(JF)T X]−1 ˆEn[(JF)X(JF)T X]]−1E[(JF)X(JF)T X]−1 ˆE∆F and we now simplify the expression on the right hand side. By applying (9) we have E[(JF)X(JF)T X]−1 ˆEn∆F = E[(JF)X(JF)T X]−1(E∆F + δn,2/√n) = −θ+ E[(JF)X(JF)T X]−1δn,2/√n Since E[(JF)X(JF)T X]−1 ˆEn[(JF)X(JF)T X] = I+ E[(JF)X(JF)T X]−1δn,1/√n and (I+ X)−1 = I−X+ X2 −··· we have by applying Slutsky’s theorem that E[(JF)X(JF)T X]−1 ˆEn[(JF)X(JF)T X]]−1 = I−E[(JF)X(JF)T X]−1δn,1/√n+ OP(1/n) where we use the standard notation Yn = OP(1/n) to indicate that nYn/f(n) →0 in probability for any function f with f(n) →∞. Hence ˆθn = −[E[(JF)X(JF)T X]−1 ˆEn[(JF)X(JF)T X]]−1E[(JF)X(JF)T X]−1 ˆEn∆F = − [ I−E[(JF)X(JF)T X]−1δn,1/√n+ OP(1/n) ] (−θ+ E[(JF)X(JF)T X]−1δn,2/√n) and applying Slutsky’s theorem again, we ﬁnd √n(ˆθn −θ) = E[(JF)X(JF)T X]−1(−δn,1θ−δn,2) + OP(1/√n) 7From the deﬁnition, we know 1√n(δn,1θ−δn,2) = ˆEn[−(JF)X(JF)T Xθ−∆F] −E[−(JF)X(JF)T Xθ−∆F] so altogether by the central limit theorem, we have √n(ˆθn −θ) →N ( 0,E[(JF)X(JF)T X]−1Σ(JF)X(JF)T Xθ+∆FE[(JF)X(JF)T X]−1 ) as claimed. 4.2 Statistical efﬁciency of score matching under a Poincar ´e inequality Our ﬁrst result will show that if we are estimating a distribution with a small Poincar ´e constant (and some relatively mild smoothness assumptions), the statistical efﬁciency of the score matching estimator is not much worse than the maximum likelihood estimator. Theorem 2 (Efﬁciency under a Poincar ´e inequality). Suppose the distribution pθ satisﬁes a Poincar´e inequality with constant CP. Then we have ∥ΓSM∥OP ≤2C2 P∥ΓMLE∥2 OP ( ∥θ∥2E∥(JF)X∥4 OP + E∥∆F∥2 2 ) . More generally, the same bound holds assuming only the following restricted version of the Poincar ´e inequality: for any w, Var(⟨w,F (x)⟩) ≤CPE∥∇⟨w,F (x)⟩∥2 2. Remark 3. To interpret the terms in the bound, the quantitiesEpθ∥(JF)X∥4 OP and E∥∆F∥2 2 can be seen as a measure of the smoothness of the sufﬁcient statistics F, and ∥θ∥as a bound on the radius of parameters for the exponential family. In Section 7 we will give an example to show bounded smoothness is indeed necessary for score matching to be efﬁcient. Remark 4. A direct consequence of this result is that with 99% probability and for sufﬁciently largen, n∥θ−ˆθSM∥2 ≤ ( nE∥θ−ˆθMLE∥2 )2 ·O ( C2 Pm ( ∥θ∥2E∥(JF)X∥4 OP + E∥∆F∥2 2 )) (10) . So if the the distribution is smooth and Poincar ´e, score matching achieves small ℓ2 error provided MLE does. To show this, since √n(θ−ˆθSM) →N(0,ΓSM) by Proposition 3, for all sufﬁciently large nit follows from Markov’s inequality that with probability at least 99%, n∥θ−ˆθSM∥2 = O(EZ∼N(0,ΓSM)∥Z∥2) = O(Tr ΓSM) = O(m∥ΓSM∥OP). On the other hand, by Fatou’s lemma we have that lim inf n→∞ nE∥θ−ˆθMLE∥2 ≥EZ∼N(0,ΓMLE)∥Z∥2 = Tr(ΓMLE) ≥∥ΓMLE∥OP where in the ﬁrst expressionˆθMLE implicitly depends on n, the number of samples. Combining these two observations with Theorem 2 and gives inequality 10. The main lemma to prove the theorem is the following: Lemma 1. E[(JF)X(JF)T X]−1 ⪯CPΣ−1 F where CP is the Poincar´e constant of pθ. Proof. For any vector w∈Rm, we have by the Poincar´e inequality that CP⟨w,E[(JF)X(JF)T X]w⟩= CPE∥∇x⟨w,F (x)⟩|X∥2 2 ≥Var(⟨w,F (x)⟩) = ⟨w,ΣFw⟩ This shows CPE[(JF)X(JF)T X] ⪰ΣF and inverting both sides, using the well-known fact that the matrix inverse is operator monotone [Toda, 2011], gives the result. 8We will also need the following helper lemma: Lemma 2. For any random vectorsA,B we have ΣA+B ⪯2ΣA + 2ΣB. Proof. For any vector wwe have Var(⟨w,A + B⟩) = Var(⟨w,A⟩) + 2Cov(⟨w,A⟩⟨w,B⟩) + Var(⟨w,B⟩) ≤Var(⟨w,A⟩) + 2 √ Var(⟨w,A⟩)Var(⟨w,B⟩) + Var(⟨w,B⟩) ≤2Var(⟨w,A⟩) + 2Var(⟨w,B⟩) where the ﬁrst inequality is Cauchy-Schwarz for variance and the second is ab≤a2/2 +b2/2. We proved for this for every vector which proves the PSD inequality. With this in mind, we can proceed to the proof of Theorem 2: Proof of Theorem 2. Recall from Proposition 3 that ΓSM := E[(JF)X(JF)T X]−1Σ(JF)X(JF)T Xθ+∆FE[(JF)X(JF)T X]−1. By Lemma 1 and submultiplicativity of the operator norm, we have ∥E[(JF)X(JF)T X]−1Σ(JF)X(JF)T Xθ+∆FE[(JF)X(JF)T X]−1∥OP ≤C2 P∥Σ−1 F ∥2 OP∥Σ(JF)X(JF)T Xθ+∆F∥OP. We will ﬁnally bound the two operator norms on the right hand side. By Lemma 2, we have Σ(JF)X(JF)T Xθ+∆F ⪯2Σ(JF)X(JF)T Xθ + 2Σ∆F Furthermore, we have ∥Σ(JF)X(JF)T Xθ∥OP ≤∥E[(JF)X(JF)T XθθT(JF)X(JF)T X]∥OP ≤E∥(JF)X∥4 OP∥θ∥2 and ∥Σ∆F∥OP ≤∥E(∆F)(∆F)T∥OP ≤TrE(∆F)(∆F)T ≤E∥∆F∥2 2 which implies the statement of the theorem. 4.3 Statistical efﬁciency lower bounds from sparse cuts In this section, we prove a converse to Theorem 2: whereas a small (restricted) Poincar ´e constant upper bounds the variance of the score matching estimator, if the Poincar ´e constant of our target distribution is large and we have sufﬁciently rich sufﬁcient statistics, score matching will be extremely inefﬁcient compared to the MLE. In fact, we will be able to do so by taking an arbitrary family of sufﬁcient statistics, and adding a single sufﬁcient statistic ! Informally, we’ll show the following: Consider estimating a distribution pθ in an exponential family with isoperimetric constant CIS. Then, pθ can be viewed as a member of an enlarged exponential family with one more (O∂S(1)-Lipschitz) sufﬁcient statistic, such that score matching has asymptotic relative efﬁciency Ω∂S(CIS) compared to the MLE, where ∂S denotes the boundary of the isoperimetric cut of pθ and Ω∂S indicates a constant depending only on the geometry of the manifold ∂S. As noted in Section 2, a large Poincar´e constant implies a large isoperimetric constant — so we focus on showing that the score matching estimator is inefﬁcient when there is a setSwhich is a “sparse cut”. Our proof uses differential geometry, so our ﬁnal result will depend on standard geometric properties of the boundary ∂S — e.g., we use the concept of the reach τMof a manifold which was deﬁned in the preliminaries (Section 2). The full proofs are in Appendix B. We now give the formal statement. 9Theorem 3 (Inefﬁciency of score matching in the presence of sparse cuts) . There exists an absolute constant c >0 such that the following is true. Suppose that pθ∗ 1 is an element of an exponential family with sufﬁcient statistic F1 and parameterized by elements of Θ1. Suppose S is a set with smooth boundary ∂S which has reach τ∂S > 0. Suppose that 1S is not an afﬁne function of F1, so there exists δ1 >0 such that sup w1:Var(⟨w1,F1⟩)=1 Cov ( ⟨w1,F1⟩, 1S√ Var(1S) )2 ≤1 −δ1. (11) Suppose that γ >0 satisﬁes γ <min { cd (1+∥θ1∥) supx:d(x,∂S)≤γ∥(JF1)x∥OP ,cτ∂S d } and is small enough so that 0 < δ := 1 − (√1 −δ1 + 2 √ γ ∫ x∈∂S p(x)dx Pr(X∈S)(1−Pr(X∈S)) )2 . Deﬁne an additional sufﬁcient statistic F2 = 1S ∗ψγ so that the enlarged exponential family contains distributions of the form p(θ1,θ2)(x) ∝exp(⟨θ1,F1(x)⟩+ θ2F2(x)) and consider the MLE and score matching estimators in this exponential family with ground truthp(θ∗ 1 ,0). Then there exists some wso that the relative (in)efﬁciency of the score matching estimator compared to the MLE for estimating ⟨w,θ⟩admits the following lower bound ⟨w,ΓSMw⟩ ⟨w,ΓMLEw⟩≥c′ γ min{Pr(X ∈S),Pr(X /∈S)}∫ x∈∂S p(x)dx where c′:= δcd 1+∥ΣF1 ∥OP . Remark 5. If we choose S to be the set achieving the worst isoperimetric constant, then the right hand side of the bound is simply c′ γCIS. (See the appendix for details.) Finally, we observe that although c′is exponentially small in d, the bound is still useful in high dimensions because in the bad cases of interest CIS is often exponentially large in d. For example, this is the case for a mixture of standard Gaussians with Ω( √ d) separation between the means (see e.g. Chen et al. [2021a]). Remark 6. The assumption δ1 >0 is a quantitative way of saying that the function 1S, the cut we are using to deﬁne the new sufﬁcient statistic F2, is not already a linear combination of the existing sufﬁcient statistics. The assumptions will always holds with some δ1 ≥0 by the Cauchy-Schwarz inequality. The equality case is when 1S is an afﬁne function of ⟨w1,F1⟩— if such a linear dependence exists, the parameterization is degenerate and the coefﬁcient of F2 is not identiﬁable as γ →0. Proof sketch. The proof of the theorem proceed in two parts: we lower bound ⟨w,ΓMLEw⟩and upper bound ⟨w,ΓMLEw⟩. The former part, which ends up to be somewhat involved, proceeds by proving a lower bound on the spectral norm of ΓSM (the full proof is in Subsection B.1) — by picking a direction in which the quadratic form is large. The upper bound onσ2 MLE(w) (the full proof is in Subsection B.2) will proceed by relating the Fisher matrix for the augmented sufﬁcient statistic(F1,F2) with the Fisher matrix for the original sufﬁcient statisticF1. Since the Fisher matrix is a covariance matrix in exponential families, this is where the numerator min{Pr(X ∈S),Pr(X /∈S)}, which is up to constant factors the variance of 1S, naturally arises in the theorem statement. For the lower bound, it is clear that we should select wwhich changes the distribution a lot, but not the observed gradients. The wwe choose that satisﬁes these desiderata is proportional to E[(JF)X(JF)T X](0,1). This walso has the property that it results in a simple expression for the quadratic form ⟨w,ΓSMw⟩, using the fact ΓSM = E[(JF)X(JF)T X]−1Σ(JF)X(JF)T Xθ+∆FE[(JF)X(JF)T X]−1 The result of this calculation (details in Lemma 4, Appendix B) is that ⟨w,ΓSMw⟩ ∥w∥2 ≥ 8−dγ2 Pr[d(X,∂S) ≤γ] EX|d(X,∂S)≤γ ( (∇F2)T X(JF)T Xθ+ ∆F2 )2 supd(x,∂S)≤γ∥(JF)x∥2 OP . 10Note that the key term Pr[d(X,∂S) ≤γ], when divided by γ and in the limit γ →0, corresponds to the surface area ∫ x∈∂S p(x)dxof the cut. Showing that the other terms do not “cancel” this one out and determining the precise dependence on γ requires a differential-geometric argument, which is somewhat more intricate. The two key ideas are to use the divergence theorem (or generalized Stokes theorem) to rewrite the numerator as a more interpretable surface integral and then rigorously argue that asγ →0 and we “zoom in” to the manifold, we can compare to the case when the surface looks ﬂat. The quantitative version of this argument involves geometric properties of the manifold (precisely, the curvature and reach). For example, Lemma 6 makes rigorous the statement that well-conditioned (i.e. large reach) manifolds are locally ﬂat. More details, as well as the full proof is included in Appendix B. Example application. We provide an instantiation of the theorem for a simple example of a bimodal distribution: Example 2. A concrete example in one dimension with a single sufﬁcient statistic is F1(x) = − 1 8a2 (x−a)2(x+ a)2 = −x4/8a2 + x2/4 −a2/8 and θ= (1,0) for a parameter a> 1 to be taken large. This looks similar to a mixture of standard Gaussians centered at −aand a. Specializing Theorem 3 to this case, we get: Corollary 1. There exists absolute constants γ0 > 0 and c >0 so that the following is true. Suppose that a >1, θ = (1 ,0), and expanded exponential family {pθ′}θ′ with pθ′(x) ∝exp (⟨θ′,(F1(x),F2(x))⟩) and new sufﬁcient statistic F2 is the output of Theorem 3 applied to F1, S = {x : x >0}, and γ = γ0. Then there exists wso that the relative (in)efﬁciency of estimating ⟨w,θ⟩is lower bounded as ⟨w,ΓSMw⟩ ⟨w,ΓMLEw⟩≥cea2/8. Proof of Corollary 1. First observe that ∫ ∞ −∞ e−F1(x)dx= 2 ∫ ∞ 0 e−(1/8)(x−a)2(x/a+1)2 dx≤2 ∫ ∞ −∞ e−(1/8)(x−a)2 dx = 2 ∫ ∞ −∞ e−(x2/8)dx=: C where Cis a positive constant independent of a. Using that F1(x) = (1/8)(x−a)2(x/a+ 1)2 it then follows that Pr(X ∈[a−1,a + 1]) = ∫a+1 a−1 e−F1(x)dx∫∞ −∞e−F1(x)dx ≥e−(1/8)(x/a+1)2 C ≥C′>0 where C′ is a positive constant independent of a. From this, we see by the law of total variance that Var(F1) ≥ Var(F1 |X ∈[a−1,a + 1]) Pr(X ∈[a−1,a + 1]) ≥C′′ > 0 where C′′ > 0 is another positive constant independent of a. Hence ∥Σ−1 F1 ∥OP = O(1) independent of a. Also, if we deﬁne S = {x: x> 0}then Cov(F1(x),1S) = 0 becuase F1(x) is even, 1S is odd and the distribution is symmetric about zero. So we can take δ1 = 1 in the statement of Theorem 3. Therefore, applying Theorem 3 to S and using that F1(0) = −a2/8, we therefore get for γ smaller than an absolute constant, that the inefﬁciency is lower bounded by Ω(ea2/8/γ). By taking γequal to a ﬁxed constant we get the result. In Section 7, we perform simulations which show the performance of score matching indeed degrades exponen- tially as abeomes large. 115 Discrete Analogues: Pseudolikelihood, Glauber Dynamics, and Approxi- mate Tensorization 5.1 Pseudolikelihood Several authors have proposed variants of score matching for discrete probability distributions, e.g. Lyu [2009], Shao et al. [2019], Hyv¨arinen [2007b]. Furthermore, Hyv ¨arinen [2005, 2006, 2007b,a] pointed out some connections between pseudolikelihood methods (a classic alternative to maximum likelihood in statistics Besag [1975, 1977]), Glauber dynamics (a.k.a. Gibbs sampler, see Preliminaries), and score matching. Finally, just like the log-Sobolev inequality controls the rapid mixing of Langevin dynamics, there are functional inequalities [Gross, 1975, Bobkov and Tetali, 2006] which bound the mixing time of Glauber dynamics. Thus, we ask: Is there a discrete analogue of the relationship between score matching and the log-Sobolev inequality? The answer is yes. To explain further, we need a key concept recently introduced by Marton [2013, 2015] and Caputo et al. [2015]: if (Ω1,F1),... (Ωd,Fd) are arbitrary measure spaces, we say a distribution q on ⨂d i=1 Ωi satisﬁes approximation tensorization of entropy with constant CAT(q) if KL(p,q) ≤CAT(q) d∑ i=1 EX∼i∼p∼i[KL(p(Xi |X∼i),q(Xi |X∼i))]. (12) This inequality is sandwiched between two discrete versions of the log-Sobolev inequality (Proposition 1.1 of Caputo et al. [2015]): it is weaker than the standard discrete version of the log-Sobolev inequality [Diaconis and Saloff-Coste, 1996] and stronger than the Modiﬁed Log-Sobolev Inequality [Bobkov and Tetali, 2006] which characterizes expo- nential ergodicity of the Glauber dynamics. 4 We deﬁne a restricted version CAT(q,P) analogously to the restricted log-Sobolev constant. Finally, we recall the pseudolikelihood objective [Besag, 1975] based on entrywise conditional probabilities: Lp(q) := ∑d i=1 EX∼p[log q(Xi |X∼i)]. With these deﬁnition in place, we have: Proposition 4. We have KL(p,q) ≤CAT(q)(Lp(p) −Lp(q)) and more generally for any class Pcontaining p, we have KL(p,q) ≤CAT(q,P)(Lp(p) −Lp(q)). Proof. Observe that Lp(p) −Lp(q) = ∑d i=1 EX∼i|p∼i[KL(p(Xi |X∼i),q(Xi |X∼i))], so the result follows by expanding the deﬁnition. Thus, just as the score matching objective is a relaxation of maximum likelihood through the log-Sobolev inequal- ity, pseudolikelihood is a relaxation through approximate tensorization of entropy. Remark 7. Pseudolikelihood methods (and variants like node-wise regression) are one of the dominant approaches to ﬁtting fully-observed graphical models, e.g. [Wu et al., 2019, Lokhov et al., 2018, Klivans and Meka, 2017, Kelner et al., 2020]. Like score matching, pseudolikelihood methods do not require computing normalizing constants which can be slow or computationally hard (e.g. Sly and Sun [2012]). Pseudolikelihood is applicable in both discrete and continuous settings, as is our connection with approximate tensorization. We state explicitly the analogue of Theorem 1 for pseudolikelihood, which follows from the same proof by replac- ing Proposition 1 with Proposition 4. Theorem 4. Suppose that Pis a class of probability distributions containingpand CAT(P,P) := supq∈PCAT(q,P) is the worst-case (restricted) approximate tensorization constant in the class of distributions (e.g. bounded by a con- stant if all of the distributions in Psatisfy a version of Dobrushin’s uniqueness condition [Marton, 2015]). Let Rn := EX1,...,Xn,ϵ1,...,ϵn sup q∈P 1 n n∑ i=1 ϵi   d∑ j=1 log q((Xi)j |(Xi)∼j)   4In most cases where the MLSI is known, approximate tensorization of entropy is also, e.g. Chen et al. [2021b], Anari et al. [2021a], Marton [2015], Caputo et al. [2015]. 12be the expected Rademacher complexity of the class givennsamples X1,...,X n ∼pi.i.d. and independentϵ1,...,ϵ n ∼ Uni{±1}i.i.d. Rademacher random variables. Let ˆp be the pseudolikelihood estimator from n samples, i.e. ˆp = arg minq∈PˆLp(q). Then EKL(p,ˆp) ≤2CAT(P,P)Rn. In particular, if CAT <∞then limn→∞EKL(p,ˆp) = 0 as long as limn→∞Rn = 0. 5.2 Ratio Matching [Hyv¨arinen, 2007b] proposed a version of score matching for distributions on the hypercube {±1}d and observed that the resulting method (“ratio matching”) bears similarity to pseudolikelihood. A similar calculation as the proof of Proposition 4 allows us to arrive at ratio matching based on a strengthening of approximate tensorization studied in [Marton, 2015]. Our derivation seems more conceptual than the original derivation, explains the similarity to pseudolikelihood, and establishes some useful connections. Marton [2015] studied a strengthened version of approximate tensorization of the form KL(p,q) ≤CAT2(q) d∑ i=1 EX∼i∼p∼iTV2(p(Xi |X∼i),q(Xi |X∼i)) (13) where TV denotes the total variation distance (see Cover [1999]). (This is known to hold for a class of distributions qsatisfying a version of Dobrushin’s condition and marginal bounds [Marton, 2015].) This inequality is stronger than the standard approximate tensorization because of Pinsker’s inequality TV2(P,Q) ≲ KL(P,Q) [Cover, 1999]. In the case of distributions on the hypercube, we have TV2(p(Xi |X∼i),q(Xi |X∼i)) = |p(Xi = +1 |X∼i) −q(Xi = +1 |X∼i)|2 = EXi∼pXi|X∼i |1(Xi = +1) −q(Xi = +1 |X∼i)|2 −EXi∼pXi|X∼i |1(Xi = +1) −p(Xi = +1 |X∼i)|2 where in the last step we used the Pythagorean theorem applied to the pXi|X∼i-orthogonal decomposition 1(Xi = +1) −q(Xi = +1 |X∼i) = [1(Xi = +1) −p(Xi = +1 |X∼i)] + [p(Xi = +1 |X∼i) −q(Xi = +1 |X∼i)] Hence, there exists a constant K′ p not depending on qsuch that d∑ i=1 EX∼i∼p∼iTV2(p(Xi |X∼i),q(Xi |X∼i)) = Kp + Mp(q) (14) where we deﬁne the ratio matching objective function to be Mp(q) := d∑ i=1 EX∼p|1(Xi = +1) −q(Xi = +1 |X∼i)|2 (15) This objective is now straightforward to estimate from data, by replacing the expectation with the average over data. Analogous to before, we have the following proposition: Proposition 5. We have KL(p,q) ≤CAT2(q)(Mp(q) −Mp(p)) and more generally for any class Pcontaining p, we have KL(p,q) ≤CAT2(q,P)(Mp(q) −Mp(p)). 13We now show how to rewriteMp(q) to match the formula from the original reference. Observe Mp(q) = 1 4 d∑ i=1 EX∼p|Xi −Eq[Xi |X∼i]|2 = 1 4 d∑ i=1 EX∼p|1 −XiEq[Xi |X∼i]|2 Observe that for any z∈{±1}we have zEq[Xi |X∼i] = q(Xi = z|X∼i) −q(Xi = −z|X∼i) q(Xi = z|X∼i) + q(Xi = −z|X∼i) and 1 −zEq[Xi |X∼i] = 2q(Xi = −z|X∼i) q(Xi = z|X∼i) + q(Xi = −z|X∼i) = 2 1 + q(Xi = z|X∼i)/q(Xi = −z|X∼i). Also for z ∈{±1}d we have q(Xi = zi |X∼i = z∼i)/q(Xi = −zi |X∼i = z∼i) = q(z)/q(z−i) where z−i reprsents zwith coordinate iﬂipped, so Mp(q) = d∑ i=1 EX∼p ( 1 1 + q(X)/q(X−i) )2 which matches the formula in Theorem 1 of Hyv¨arinen [2007b]. Summarizing, minimizing the ratio matching objective makes the right hand side of the strengthened tensorization estimate (13) small, so whenCAT2(q) is small it will imply successful distribution learing in KL. (The obvious variant of Theorem 4 will therefore hold.) In this way ratio matching can also be understood as a relaxation of maximum likelihood. 6 Related work Score matching was originally introduced by Hyv ¨arinen [2005], who also proved that the estimator is asymptotically consistent. In [Hyv ¨arinen, 2007b], the authors propose estimators that are deﬁned over bounded domains. [Song and Ermon, 2019] scaled the techniques to neurally parameterized energy-based models, leveraging score matching versions like denoising score matching Vincent [2011], which involves an annealing strategy by convolving the data distribution with Gaussians of different variances, and sliced score matching [Song et al., 2020]. The authors conjec- tured that annealing helps with multimodality and low-dimensional manifold structure in the data distribution — and our paper can be seen as formalizing this conjecture. The connection between Hyv¨arinen’s score matching objective and the relative Fisher information in (7) is known in the literature — see e.g. [Shao et al., 2019, Nielsen, 2021, Barp et al., 2019, Vempala and Wibisono, 2019, Yamano, 2021]. Relatedly, Hyv ¨arinen [2007a] pointed out some connections between the score matching objective and con- trastive divergence using the lens of Langevin dynamics. We also remark that since I(p|q) = −d dt KL(pt,q) |t=0 for pt the output of Langevin dynamics at time t, score matching can be interpreted as ﬁnding a q to minimize the contraction of the Langevin dynamics for q started at p. Previously, [Guo, 2009, Lyu, 2009] observed that the score matching objective can be interpreted as the inﬁnitesimal change in KL divergence as we add Gaussian noise — see Appendix A for an explanation why these two quantities are equal. We note that Hyv¨arinen [2008] also gave a related interpretation of score matching in terms of adding an inﬁnitesimal amount of Gaussian noise. In the discrete setting, it was recently observed that approximate tensorization has applications to identity testing of distributions in the “coordinate oracle” query model [Blanca et al., 2022], which is another application of approximate tensorization outside of sampling otherwise unrelated to our result. Finally, [Block et al., 2020, Lee et al., 2022a] show guarantees on running Langevin dynamics, given estimates on∇log pthat are only ϵ-correct in the L2(p) sense. They show that when the Langevin dynamics are run for some moderate amount of time, the drift between the true Langevin 14dynamics (using ∇log pexactly) and the noisy estimates can be bounded. Recent concurrent works [Lee et al., 2022b, Chen et al., 2022] show results of a similar ﬂavor for denoising diffusion model score matching, speciﬁcally when the forward SDE is an Ornstein-Uhlenbeck process. 7 Simulations 7.1 Exponential family experiments. Fitting a bimodal distribution with a cut statistic. First, we show the result of ﬁtting a bimodal distribution (as in Example 2) from an exponential family. In Figure 1, the difference of the two sufﬁcient statistics we consider corresponds to the cut statistic used in our negative result (Theorem 3). As predicted (Corollary 1) score matching performs poorly compared to the MLE as the distance between modes grows. In Figure 2, we illustrate the distribution of the errors in the bimodal experiment with the cut statistic. As expected based on the theory, the direction where score matching with large offset performs very poorly corresponds to the difference between the two sufﬁcient statistics, which encodes the sparse cut in the distribution. Fitting a bimodal distribution without a cut statistic. In Figure 3 we show the result of ﬁtting the same bimodal distribution using score matching, but we remove the second sufﬁcient statistic (which is correlated with the sparse cut in the distribution). In this case, score matching ﬁts the distribution nearly as well as the MLE. This is consistent with our theory (e.g. the failure of score matching in Theorem 3 requires that we have a sufﬁcient statistic approximately representing the cut) and justiﬁes some of the distinctions we made in our results: even though the Poincar´e constant is very large, the asymptotic variance of score matching within the exponential family is upper bounded by therestricted Poincar´e constant (see Theorem 2) which is much smaller. Example 3 (Application of Theorem 2 to this example) . To brieﬂy expand the last point, we show how to apply Theorem 2 in this example (Example 2, where we havenot added a bad cut statistic.) The restricted Poincar´e constant for applying Theorem 2 will be C := Var(F1(X)) E(F′ 1(X))2 = Var(X2 −X4/2a2) E(2X−2X3/a2)2 (16) which asymptotically goes to a constant, rather than blowing up exponentially, as a goes to inﬁnity. (This can be made formal using arguments as in the proof of Corollary 1; informally, the distribution is similar to a mixture of two standard Gaussians centered at ±a so the numerator is close to VarZ∼N(0,1)((a+ Z)2 −(a+ Z)4/2a2) = Var(2aZ+ Z2 −(4aZ+ 6Z2 + 4Z3/a+ Z4)/2) = Θ(1) and the denominator is approximately EZ∼N(0,1)(2(a+ Z) −2(a+ Z)3/a2)2 = E(2Z−2(3Z+ 3Z3/a+ Z3/a2))2 = Θ(1).) Given this bound on the restricted Poincar ´e constant, we can apply Theorem 2. Based on similar reasoning to above, one can show that EF′ 1(X)4 = ( −1/4a2)4E((X −a)(X + a)2 + (X −a)2(X + a))4 = Θ(1) and EF′′ 1 (X)2 = E(−3x2/2a2 + 1/2)2 = Θ(1), so we conclude that ∥ΓSM∥OP = O(∥ΓMLE∥2 OP). This proves that score matching will perform not much worse than the MLE, as we saw in the experimental result of Figure 3. Remark 8. Example 3 shows a case where there is a large gap between the restricted and unrestricted Poincar ´e constants. This also implies a completely analogous gap between appropriate restricted and unrestricted log-Sobolev constants, as used e.g. in the context of Theorem 1. To elaborate, we know that the unrestricted log-Sobolev constant blows up exponentially in a, just like the unrestricted Poincar´e constant, because CLS ≥CP/2 [Van Handel, 2014]. On the other hand, if we ﬁx the ground truth distribution pa consider the class of distributions Pr = {pa′ : |a−a′|≤ r}, we have that lim r→0 CLS(q,Pr) = C/2 15where Cis the constant deﬁned in(16) in terms of a(and which isO(1) as a→∞). This is because from the deﬁnition as an exponential family, we have pa(x)/pa′(x) = exp ((a−a′)F1(x)) Ea′exp ((a−a′)F1(x)) so lim a′→a KL(pa,pa′) I(pa |pa′) = lim a′→a (a−a′)2Varpa′(F1(x)) 2(a−a′)2Epa′∥∇F1(x)∥2 = C/2 where the ﬁrst equality is by a standard Taylor expansion argument (see proof of Lemma 3.28 of [Van Handel, 2014]). Fitting a unimodal distribution with rapid oscilation. In Figure 5, we demonstrate what happens when the distri- bution is unimodal (and has small isoperimetric constant), but the sufﬁcient statistic is not quantitatively smooth. More precisely, we consider the case pθ(x) ∝e−θ0x2/2−θ1 sin(ωx) as ωincreases. In the ﬁgure, we used the formulas from asymptotic normality to calculate the distribution over parameter estimates from 100,000 samples. We also veriﬁed via simulations that the asymptotic formula almost exactly matches the actual error distribution. The result is that while the MLE can always estimate the coefﬁcient θ1 accurately, score matching performs much worse for large values of ω. This demonstrates that the dependence on smoothness in our results (in particular, Theorem 2) is actually required, rather than being an artifact of the proof. Conceptually, the reason score matching fails even when though the distribution has no sparse cuts is this: the gradient of the log density becomes harder to ﬁt as the distribution becomes less smooth (for example, the Rademacher complexity from Theorem 1 will become larger as it scales with ∇xlog pand ∇2 xlog p). 7.2 Score matching with neural networks Fitting a mixture of Gaussians with a one-layer network. We also show that empirically, our results are robust even beyond exponential families. In Figure 4 we show the results of ﬁtting a mixture of two Gaussians via score matching5 , where the score function is parameterized as a one hidden-layer network with tanh activations. We see that the predictions of our theory persist: the distribution is learned successfully when the two modes are close and is not when the modes are far. This matches our expectations, since the Poincar ´e, log-Sobolev, and isoperimetric constants blow up exponentially in the distance between the two modes (see e.g. Chen et al. [2021a]) and the neural network is capable of detecting the cut between the two modes. In the right hand side example (the one with large separation between modes), the shape of the two Gaussian components is learned essentially perfectly — it is only the relative weights of the two components which are wrong. This closely matches the idea behind the proof of the lower bound in Theorem 3; informally, the feedforward network can naturally represent a function which detects the cut between the two modes of the distribution, i.e. the additional bad sufﬁcient statistic F2 from Theorem 3. The fact that the shapes are almost perfectly ﬁt where the distribution is concentrated indicates that the test loss Jp is near its minimum. Recall from (1) that the suboptimality of a distribution q in score matching loss is given by Jp(q) −Jp(p) = Ep∥∇log p −∇log q∥2. If we let q be the distribution recovered by score matching, we see from the ﬁgure that the slopes of the distribution were correctly ﬁt wherever pis concentrated, so Ep∥∇log p−∇log q∥2 is small. However near-optimality of the test loss Jp(q) does not imply that qis actually close to p: the test loss does not heavily depend on the behavior of log qin between the two modes, but the value of ∇log qin between the modes affects the relative weight of the two modes of the distribution, leading to failure. Both models illustrated in the ﬁgure have 2048 tanh units and are trained via SGD on fresh samples for 300000 steps. After training the model, the estimated distribution is computed from the learned score function using numerical integration. 5We note that this experiment is similar in ﬂavor to plots in (Figure 2) in Song and Ermon [2019], where they show that the score is estimated poorly near the low-probability regions of a mixture of Gaussians. In our plots, we numerically integrate the estimates of the score to produce the pdf of the estimated distribution. 168 Conclusion In this paper, we studied the statistical efﬁciency of score matching and identiﬁed a close connection to functional inequalities which characterize the ergodicity of Langevin dynamics. For future work, it would be interesting to characterize formally the improvements conferred by annealing strategies like [Song and Ermon, 2019], like it has been done in the setting of sampling using Langevin dynamics [Lee et al., 2018]. Acknowledgements. We are grateful to Lester Mackey and Aapo Hyv¨arinen, as well as to the anonymous reviewers, for feedback on an earlier draft. Figure 1: Statistical efﬁciency of score matching vs MLE for ﬁtting the distribution with ground truth parameters (θ0,θ1) = (1 ,0) of the form pθ(x) ∝eθ0(x2−x4/(2a2))+θ1(x2−x4/(2a2)+erf(x)) as we vary the offset abetween 1 and 7 and train with ﬁxed number of samples ( 105). We see score matching (red) performs very poorly compared to the MLE (blue) as the offset (distance between modes) grows, by plotting the log of the Euclidean distance to the true parameter for both estimators. 17Figure 2: Level sets for the distribution over estimates in the same example as Figure 1. We see that as the distance a between modes increases, the direction of large variance for the score matching estimator (right ﬁgure) corresponds to the difference of the sufﬁcient statistics which encodes the sparse cut in the distribution. On the other hand, the MLE (left ﬁgure) does not exhibit this behavior and has low variance in all directions. Figure 3: Here we see the result of running an identical experiment to Figure 1, only we remove the second sufﬁcient statistic, so our distribution is now pθ(x) ∝eθ0(x2−x4/(2a2)) where θ0 = 1 and we again vary the offset abetween 1 and 7. With only the single sufﬁcient statistic, score matching performs comparably to MLE. 18Figure 4: Training a single hidden-layer network to score match a mixture of Gaussians (ground truth orange, score matching output blue) succeeds at learning the distribution when the modes are close (left, small isoperimetric con- stant), but not when they are distant (right, large isoperimetric constant) in which case it weighs the modes incorrectly. 19Figure 5: Score matching vs MLE for a distribution with a rapidly oscillating sufﬁcient statistic, pθ(x) ∝ e−θ0x2/2−θ1 sin(ωx) where (θ0,θ1) = (1,1), and increasing ω. On the top, for increasing ωwe show a log-log plot of the average Euclidean distance in parameter space betweenθand the output of each estimator. On the bottom, for each value of ω, we draw a level set of the distribution within which a ﬁxed fraction of returned estimates lie (MLE left, score matching right). Score matching becomes increasingly inaccurate asωincreases while the MLE stays extremely accurate. 20References Nima Anari, Vishesh Jain, Frederic Koehler, Huy Tuan Pham, and Thuy-Duong Vuong. Entropic independence in high-dimensional expanders: Modiﬁed log-sobolev inequalities for fractionally log-concave polynomials and the ising model. arXiv preprint arXiv:2106.04105, 2021a. Nima Anari, Vishesh Jain, Frederic Koehler, Huy Tuan Pham, and Thuy-Duong Vuong. Entropic independence ii: op- timal sampling and concentration via restricted modiﬁed log-sobolev inequalities.arXiv preprint arXiv:2111.03247, 2021b. Dominique Bakry, Ivan Gentil, Michel Ledoux, et al. Analysis and geometry of Markov diffusion operators , volume 103. Springer, 2014. Alessandro Barp, Francois-Xavier Briol, Andrew Duncan, Mark Girolami, and Lester Mackey. Minimum stein dis- crepancy estimators. Advances in Neural Information Processing Systems, 32, 2019. Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The Annals of Statistics, 33(4):1497–1537, 2005. Julian Besag. Statistical analysis of non-lattice data. Journal of the Royal Statistical Society: Series D (The Statisti- cian), 24(3):179–195, 1975. Julian Besag. Efﬁciency of pseudolikelihood estimation for simple gaussian ﬁelds. Biometrika, pages 616–618, 1977. Antonio Blanca, Zongchen Chen, Daniel ˇStefankoviˇc, and Eric Vigoda. Identity testing for high-dimensional distribu- tions via entropy tensorization. arXiv preprint arXiv:2207.09102, 2022. Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and langevin sampling. arXiv preprint arXiv:2002.00107, 2020. Sergey G Bobkov. An isoperimetric inequality on the discrete cube, and an elementary proof of the isoperimetric inequality in gauss space. The Annals of Probability, 25(1):206–214, 1997. Sergey G Bobkov and Prasad Tetali. Modiﬁed logarithmic sobolev inequalities in discrete settings. Journal of Theo- retical Probability, 19(2):289–336, 2006. Pietro Caputo, Georg Menz, and Prasad Tetali. Approximate tensorization of entropy at high temperature. In Annales de la Facult´e des sciences de Toulouse: Math´ematiques, volume 24, pages 691–716, 2015. Hong-Bin Chen, Sinho Chewi, and Jonathan Niles-Weed. Dimension-free log-sobolev inequalities for mixture distri- butions. Journal of Functional Analysis, 281(11):109236, 2021a. Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215, 2022. Zongchen Chen, Kuikui Liu, and Eric Vigoda. Optimal mixing of glauber dynamics: Entropy factorization via high- dimensional expansion. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing , pages 1537–1550, 2021b. Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999. Persi Diaconis and Laurent Saloff-Coste. Logarithmic sobolev inequalities for ﬁnite markov chains. The Annals of Applied Probability, 6(3):695–750, 1996. Rick Durrett. Probability: theory and examples, volume 49. Cambridge university press, 2019. Herbert Federer. Curvature measures. Transactions of the American Mathematical Society, 93(3):418–491, 1959. 21Peter GM Forbes and Steffen Lauritzen. Linear estimating equations for exponential families with application to gaussian linear concentration models. Linear Algebra and its Applications, 473:261–283, 2015. V´eronique Gayrard, Anton Bovier, Michael Eckhoff, and Markus Klein. Metastability in reversible diffusion processes i: Sharp asymptotics for capacities and exit times. Journal of the European Mathematical Society , 6(4):399–424, 2004. V´eronique Gayrard, Anton Bovier, and Markus Klein. Metastability in reversible diffusion processes ii: Precise asymptotics for small eigenvalues. Journal of the European Mathematical Society, 7(1):69–99, 2005. Alfred Gray. Tubes, volume 221. Springer Science & Business Media, 2003. Leonard Gross. Logarithmic sobolev inequalities. American Journal of Mathematics, 97(4):1061–1083, 1975. Dongning Guo. Relative entropy and score function: New information-estimation relationships through arbitrary additive perturbation. In 2009 IEEE International Symposium on Information Theory, pages 814–818. IEEE, 2009. Lars H ¨ormander. The analysis of linear partial differential operators I: Distribution theory and Fourier analysis . Springer, 2015. Elton P Hsu. Stochastic analysis on manifolds. Number 38. American Mathematical Soc., 2002. Aapo Hyv¨arinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005. Aapo Hyv¨arinen. Consistency of pseudolikelihood estimation of fully visible boltzmann machines. Neural Computa- tion, 18(10):2283–2292, 2006. Aapo Hyv¨arinen. Connections between score matching, contrastive divergence, and pseudolikelihood for continuous- valued variables. IEEE Transactions on neural networks, 18(5):1529–1531, 2007a. Aapo Hyv¨arinen. Some extensions of score matching. Computational statistics & data analysis , 51(5):2499–2512, 2007b. Aapo Hyv¨arinen. Optimal approximation of signal priors. Neural Computation, 20(12):3087–3110, 2008. Jonathan Kelner, Frederic Koehler, Raghu Meka, and Ankur Moitra. Learning some popular gaussian graphical models without condition number bounds. Advances in Neural Information Processing Systems, 33:10986–10998, 2020. Adam Klivans and Raghu Meka. Learning graphical models using multiplicative weights. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 343–354. IEEE, 2017. Michel Ledoux. The geometry of markov diffusion generators. In Annales de la Facult ´e des sciences de Toulouse: Math´ematiques, volume 9, pages 305–366, 2000. Holden Lee, Andrej Risteski, and Rong Ge. Beyond log-concavity: Provable guarantees for sampling multi- modal distributions using simulated tempering langevin monte carlo. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems , vol- ume 31. Curran Associates, Inc., 2018. URLhttps://proceedings.neurips.cc/paper/2018/file/ c6ede20e6f597abf4b3f6bb30cee16c7-Paper.pdf. Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial com- plexity. arXiv preprint arXiv:2206.06227, 2022a. Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distribu- tions. arXiv preprint arXiv:2209.12381, 2022b. David A Levin and Yuval Peres. Markov chains and mixing times, volume 107. American Mathematical Soc., 2017. 22Andrey Y Lokhov, Marc Vuffray, Sidhant Misra, and Michael Chertkov. Optimal structure and parameter learning of ising models. Science advances, 4(3):e1700791, 2018. Siwei Lyu. Interpretation and generalization of score matching. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, pages 359–366, 2009. Katalin Marton. An inequality for relative entropy and logarithmic sobolev inequalities in euclidean spaces. Journal of Functional Analysis, 264(1):34–61, 2013. Katalin Marton. Logarithmic sobolev inequalities in discrete product spaces: a proof by a transportation cost distance. arXiv preprint arXiv:1507.02803, 2015. Frank Nielsen. Fast approximations of the jeffreys divergence between univariate gaussian mixtures via mixture conversions to exponential-polynomial distributions. Entropy, 23(11):1417, 2021. Partha Niyogi, Stephen Smale, and Shmuel Weinberger. Finding the homology of submanifolds with high conﬁdence from random samples. Discrete & Computational Geometry, 39(1):419–441, 2008. Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014. Stephane Shao, Pierre E Jacob, Jie Ding, and Vahid Tarokh. Bayesian model comparison with the hyv ¨arinen score: Computation and consistency. Journal of the American Statistical Association, 2019. Allan Sly and Nike Sun. The computational hardness of counting in two-spin models on d-regular graphs. In 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science, pages 361–369. IEEE, 2012. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019. Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In Uncertainty in Artiﬁcial Intelligence, pages 574–584. PMLR, 2020. Alexis Akira Toda. Operator reverse monotonicity of the inverse.The American Mathematical Monthly, 118(1):82–83, 2011. Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000. Ramon Van Handel. Probability in high dimension. Technical report, PRINCETON UNIV NJ, 2014. Santosh Vempala and Andre Wibisono. Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suf- ﬁces. Advances in neural information processing systems, 32, 2019. Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7): 1661–1674, 2011. Hermann Weyl. On the volume of tubes. American Journal of Mathematics, 61(2):461–472, 1939. Shanshan Wu, Sujay Sanghavi, and Alexandros G Dimakis. Sparse logistic regression learns all discrete pairwise graphical models. Advances in Neural Information Processing Systems, 32, 2019. Takuya Yamano. Skewed jensen—ﬁsher divergence and its bounds. Foundations, 1(2):256–264, 2021. 23A Recovering an interpretation of score matching We remarked that if we use the fact I(p|q) = −d dt KL(pt,q) |t=0, the score matching objective has a natural inter- pretation in terms of select q to minimize the contraction of the Langevin dynamics for q started at p. On the other hand, Guo [2009] and Lyu [2009] previously observed that the score matching objective can be interpreted as the inﬁnitesimal change in KL divergence between pand q as we add noise to both of them, which is closely related to the de Bruijn identity. We now explain why these two quantities are equal by giving a proof of their equality (which is shorter than the one you get by going through the proof in Lyu [2009]). Before giving the formal proof, we give some intuition for why the statement should be true. The Langevin dynamics approximately adds a noise of size N(0,2t) and subtracts a gradient step along ∇log q, and this dynamics preserves q. For small t, the gradient step is essentially reversible and preserves the KL. So heuristically, reversing the gradient step gives KL(pt,q) ≈KL(N(0,2t) ∗p,N(0,2t) ∗q). We now give the formal proof. Lemma 3. Assuming smooth probability densities p(x) and q(x) decay sufﬁciently fast at inﬁnity, d dtKL(pt,q) ⏐⏐⏐ t=0 = d dtKL(p∗N(0,2t),q ∗N(0,2t)) ⏐⏐⏐ t=0 where ∗denotes convolution. Proof. Recalling from Section 2 that Ht = etL we have that d dt pt q = d dtHt p q = Lp q. Since KL(pt,q) = Eq[pt q log pt q ] and d dx[xlog x] = log x+ 1, it follows by the chain rule that d dtKL(pt,q) = Eq [( log p q + 1 ) Lp q ] = Eq [( log p q + 1 )( ⟨∇log q,∇p q⟩+ ∆p q )] = Eq [( log p q + 1 )( −⟨∇log q,∇p q⟩+ ∆p q −p∆q q2 )] where in the last step we used the quotient rule ∆p q = ∆p q −2 ⣨ ∇log q,∇p q ⟩ −p∆q q2 . On the other hand, by using the Fokker-Planck equation ∂ ∂t(p∗N(0,2t)) = ∆p(Lemma 2 of Lyu [2009]) and the chain rule we have d dtKL(p∗N(0,2t),q ∗N(0,2t)) = d dt ∫ (q∗N(0,2t))p∗N(0,2t) q∗N(0,2t) log p∗N(0,2t) q∗N(0,2t)dx = ∫ (∆q)p q log p qdx+ Eq [( log p q + 1 )(∆p q −p∆q q2 )] Since by the chain rule and integration by parts we have Eq [( log p q + 1 )⟨ ∇log q,∇p q ⟩] = ∫ [⟨ ∇q,∇p q log p q ⟩] dx= − ∫ (∆q)p q log p qdx, we see that the two derivatives are indeed equal. B Proof of Theorem 3 and Applications We restate Theorem 3 for the reader’s convenience and in a slightly more detailed form (we include an upper bound on the covariance of the MLE error which follows from the proof). Theorem 5 (Inefﬁciency of score matching in the presence of sparse cuts, Restatement of Theorem 3). There exists an absolute constant c >0 such that the following is true. Suppose that pθ∗ 1 is an element of an exponential family with sufﬁcient statistic F1 and parameterized by elements of Θ1. Suppose S is a set with smooth and compact boundary 24∂S. Let τ∂S >0 denote the reach of ∂S (see Section 2) Suppose that 1S is not an afﬁne function of F1, so there exists δ1 >0 such that sup w1:Var(⟨w1,F1⟩)=1 Cov ( ⟨w1,F1⟩, 1S√ Var(1S) )2 ≤1 −δ1. (17) Suppose that γ >0 satisﬁes γ <min { cd (1+∥θ1∥) supx:d(x,∂S)≤γ∥(JF1)x∥OP ,cτ∂S d } and is small enough so that 0 < δ := 1 − (√1 −δ1 + 2 √ γ ∫ x∈∂S p(x)dx Pr(X∈S)(1−Pr(X∈S)) )2 . Deﬁne an additional sufﬁcient statistic F2 = 1S ∗ψγ so that the enlarged exponential family contains distributions of the form p(θ1,θ2)(x) ∝exp(⟨θ1,F1(x)⟩+ θ2F2(x)) and consider the MLE and score matching estimators in this exponential family with ground truthp(θ∗ 1 ,0). Then the asymptotic renormalized covariance matrix ΓMLE of the MLE is bounded above as ΓMLE ⪯ 1 1 −δ [Σ−1 F1 0 0 1 Pr(X∈S)(1−Pr(X∈S)) ] and there there exists some wso that the relative (in)efﬁciency of the score matching estimator compared to the MLE for estimating ⟨w,θ⟩admits the following lower bound ⟨w,ΓSMw⟩ ⟨w,ΓMLEw⟩≥c′ γ min{Pr(X ∈S),Pr(X /∈S)}∫ x∈∂S p(x)dx where c′:= δcd 1+∥ΣF1 ∥OP . B.1 Lower bounding the spectral norm of ΓSM We recall the new statistic F2, deﬁned in terms of the molliﬁer ψintroduced in Section 2: F2(x) := (1S ∗ψγ)(x) = ∫ Rd 1S(y)ψγ(x−y)dy= ∫ S ψγ(x−y)dy and the new sufﬁcient statistic is F(x) = ( F1(x),F2(x)). We ﬁrst show the following lower bound on the largest eigenvalue of ΓSM, the renormalized limiting covariance of score matching: Lemma 4 (Largest eigenvalue of ΓSM). The largest eigenvalue of ΓSM satisﬁes λmax(ΓSM) ≥ 8−dγ2 Pr[d(X,∂S) ≤γ] EX|d(X,∂S)≤γ ( (∇F2)T X(JF)T Xθ+ ∆F2 )2 supd(x,∂S)≤γ∥(JF)x∥2 OP . (18) Proof. We have ∇xF2(x) = ∫ S ∇xψγ(x−y)dy, ∇2 xF2(x) = ∫ S ∇2 xψγ(x−y)dy. Deﬁning u:= E[(JF)X(JF)T X](0,1) = E[(JF)X∇xF2(x)] we have, by the variational characterization of eigenvalues of symmetric matrices, that λmax(ΓSM) ≥ ⟨u,E[(JF)X(JF)T X]−1Σ(JF)X(JF)T Xθ+∆FE[(JF)X(JF)T X]−1u⟩ ∥u∥2 2 . (19) 25To upper bound the denominator we observe that if Bd is the volume of the unit ball, ∥∇xF2(x)∥2 =  ∫ S (∇ψγ)(x−y)dy  2 (20) ≤1(d(x,∂S) ≤γ)γ−d−1vol(B(X,γ))/Id (21) ≤8d1(d(x,∂S) ≤γ)γ−1 (22) and so ∥u∥2 ≤8dγ−1 Pr[d(X,∂S) ≤γ] sup d(x,∂S)∈[−γ,γ] ∥(JF)x∥OP where we used the computation of the derivative of ψγ. To lower bound the numerator we have ⟨u,E[(JF)X(JF)T X]−1Σ(JF)X(JF)T Xθ+∆FE[(JF)X(JF)T X]−1u⟩ = (0,1)TΣ(JF)X(JF)T Xθ+∆F(0,1) = E⟨(0,1),(JF)X(JF)T Xθ+ ∆F⟩2 = E ( (∇xF2)T X(JF)T Xθ+ ∆F2 )2 . The integrand is zero except when d(X,∂S) ≤γso it equals Pr[d(X,∂S) ≤γ]EX|d(X,∂S)∈[−γ,γ] ( (∇F2)T X(JF)T Xθ+ ∆F2 )2 and combining gives the result. We now estimate the right hand side of (18) for small γ, using differential geometric techniques. The main idea is that as we take γ smaller, we end up zooming into the manifold ∂S which locally looks closer and closer to being ﬂat. Differential-geometric quantities describing the manifold appear when we make this approximation rigorous. The most involved term to handle ends up to be calculating the expectation EX|d(X,∂S)≤γ ( (∇F2)T X(JF)T Xθ+ ∆F2 )2 . To do this, we ﬁrst argue that the term with the Laplacian dominates as γ →0, then by Stokes theorem, we end up integrating ⟨∇ψ,dN⟩over intersections of S with small spheres of radius γ, where N is a normal to S. Such quantities can be calculated by comparing to the “ﬂat” manifold case — i.e. when N does not change. How far away these quantities are (thus how small γ needs to be) depends on the curvature of S (or more precisely, the condition number of the manifold). Lemma 6 makes rigorous the statement that well-conditioned manifolds are locally ﬂat and then Lemma 7, which is part of the proof of Weyl’s tube formula [Gray, 2003, Weyl, 1939], lets us rigorously say that the tubular neighborhood (that is, a thickening of the manifold) behaves similarly to the ﬂat case. Lemma 5. There exists an absolute constant c> 0 such that the following is true. For any γ >0 satisfying γ <min { cd (1 + ∥θ1∥) supx:d(x,∂S)≤γ∥(JF1)x∥OP ,cτ∂S d } for score matching on the extended family withm+ 1 sufﬁcient statistics and distribution pθ with θ= (θ1,0) we have λmax(ΓSM) ≥ cd γ ∫ ∂S p(x)dA Proof. In the denominator, we can observe by (22) that ∥(JF)x∥2 OP ≤∥JF1∥2 OP + ∥∇F2∥2 2 ≤∥JF1∥2 OP + γ−2B2 d ≤2γ−2B2 d where the last inequality holds assuming γis sufﬁciently small that ∥JF1∥2 OP ≤γ−2B2 d. 26In the numerator we can observe (∇xF2)T X(JF)T Xθ+ ∆F2 = ∫ S ⟨(∇ψγ)((X−y)),(JF)T Xθ⟩+ (∆ψγ)(X−y)dy = ∫ S∩B(X,γ) ⟨(∇ψγ)((X−y)),(JF)T Xθ⟩+ (∆ψγ)(X−y)dy = γd ∫ B(0,1)∩(X−S)/γ ⟨(∇ψγ)(γu),(JF)T Xθ⟩+ (∆ψγ)(γu)du = ∫ B(0,1)∩(X−S)/γ γ−1⟨∇ψ(u),(JF)T Xθ⟩+ γ−2(∆ψ)(u)du = ∫ B(0,1)∩(X−S)/γ γ−1⟨∇ψ(u),(JF)T Xθ⟩+ ∫ ∂(B(0,1)∩(X−S)/γ) γ−2⟨∇ψ,dN⟩ = ∫ B(0,1)∩(X−S)/γ γ−1⟨∇ψ(u),(JF)T Xθ⟩+ ∫ B(0,1)∩(X−∂S)/γ γ−2⟨∇ψ,dN⟩ where the second-to-last expression is a surface integral which we arrived at by applying the divergence theorem, using that the Laplacian is the divergence of the gradient, and in the last step we used that ψand all of its derivatives vanish on the boundary of the unit sphere. Using that θ= (θ1,0) we have ⏐⏐⏐⏐⏐ ∫ B(0,1)∩(X−S)/γ γ−1⟨∇ψ(u),(JF)T Xθ⟩ ⏐⏐⏐⏐⏐≤γ−1 ∫ B(0,1) ∥∇ψ(u)∥∥(JF1)X∥OP∥θ∥ (23) ≤8dγ−1∥(JF1)X∥OP∥θ∥. (24) Let pbe the point in ∂(X −S)/γ which is closest in Euclidean distance to the origin. Let n(q) denote the unit normal vector at point qoriented outwards (Gauss map). Note that by ﬁrst-order optimality conditions for p, we must have n(p) = p/∥p∥. Since dN = n(q)dAwhere dAis the surface area form, we have ∫ B(0,1)∩(X−∂S)/γ ⟨∇ψ,dN⟩= ∫ q∈B(0,1)∩(X−∂S)/γ ⟨∇ψ(q),n(p) + (n(q) −n(p))⟩dA = ∫ q∈B(0,1)∩(X−∂S)/γ −2ψ(q) (1 −∥q∥2)2 ⟨q, p ∥p∥+ (n(q) −n(p))⟩dA. We now show how to lower bounding the integral by showing⟨q, p ∥p∥+ (n(q) −n(p))⟩is lower bounded. Let c(t) be a minimal unit-speed geodesic on M:= (X−∂S)/γ from pto q. Note that τM= τ∂S/γ so if γ is very small, Mis very well-conditioned. By the fundamental theorem of calculus, we have that ⟨p,q⟩= ⟨p,p⟩+ ∫ 1 0 ⟨p,c′(t)⟩dt= ⟨p,p⟩+ ∫ 1 0 ⟨ProjTc(t) p,c′(t)⟩dt where Tc(t) is the tangent space to Mat the point c(t). Hence by the Cauchy-Schwarz inequality we have |⟨p,q⟩≥⟨ p,p⟩− ∫ 1 0 ∥ProjTc(t) p∥∥c′(t)∥dt. By Proposition 6.3 of Niyogi et al. [2008], we have that for φt the angle between the tangent spaces Tp and Tc(t) that cos φt ≥1 − 1 τM dM(p,c(t)) = 1 − t τM dM(p,q). (25) 27Since sin2 φt + cos2 φt = 1 and pis orthogonal to the tangent space at Tp, it follows that ∥ProjTc(t) p∥≤∥ p∥|sin φt|= ∥p∥ √ 1 −cos2 φt ≤∥p∥ √ (2t/τM)dM(p,q) + (t/τM)2dM(p,q)2 ≤∥p∥ √ (2t/τM)dM(p,q) + ∥p∥(t/τM)dM(p,q) hence ∫ 1 0 ∥ProjTc(t) p∥∥c′(t)∥dt≤(2/3)∥p∥ √ (2/τM)dM(p,q)3/2 + ∥p∥(1/2τM)dM(p,q)2. Since ∥p−q∥≤ 2, provided that τM>16 we have by Proposition 6.3 of Niyogi et al. [2008] that dM(p,q) ≤τM(1 − √ 1 −2∥p−q∥/τM) ≤4. Combining, we have for some absolute constant C >0 that ⟨p,q⟩≥⟨ p,p⟩(1 −C √ 1/τM−C/τM). Also, we can compute ∥n(q) −n(p)∥= √ 2 −2 cosφ1 ≤ √ 2 τM dM(p,q) ≤ √ 8 τM so |⟨q,n(q) −n(p)⟩⟩|≤∥ q∥∥n(q) −n(p)∥≤ √ 8 τM . Hence provided τM>C ′for some absolute constant C′>0 and ∥p∥>0.1, we have ⏐⏐⏐⏐⏐ ∫ q∈B(0,1)∩(X−∂S)/γ −2ψ(q) (1 −∥q∥2)2 ⟨q, p ∥p∥+ (n(q) −n(p))⟩dA ⏐⏐⏐⏐⏐ ≥ ∫ q∈B(0,1)∩(X−∂S)/γ ψ(q) (1 −∥q∥2)2 ∥p∥dA using that the integrand on the left is always negative. We can further lower bound the integral by considering the intersection of Mwith a ball of radius r:= 1−∥p∥ 2 centered at p. We have ∫ q∈B(0,1)∩(X−∂S)/γ ψ(q) (1 −∥q∥2)2 ∥p∥dA≥ ∫ q∈B(p,r)∩M ψ(q) (1 −∥q∥2)2 ∥p∥dA ≥∥p∥(cos θ)kvol(Bk(p,r)) inf q∈B(p,r)∩M ψ(q) (1 −∥q∥2)2 = ∥p∥(cos θ)krk inf q∈B(p,r)∩M Bkψ(q) (1 −∥q∥2)2 where k= d−1 is the dimension of Mand θ= arcsin(r/2τ) and we applied Lemma 5.3 of Niyogi et al. [2008]. If ∥p∥∈ (0.1,0.9) this is lower bounded by a constant Ck >0 which is at worst exponentially small in k. Hence recalling (24) we have for any X with d(X,∂S) ∈ (0.1γ,0.9γ) and for γ sufﬁciently small so that γ8k+1∥(JF1)X∥OP∥θ∥<Ck/4 for any such X, we have that ( (∇F2)T X(JF)T Xθ+ ∆F2 )2 ≥γ−4C′ k where C′ k >0 is a constant that is at worst exponentially small in k. Therefore EX|d(X,∂S)∈[−γ,γ] ( (∇F2)T X(JF)T Xθ+ ∆F2 )2 ≥γ−4C′ k Pr(d(X,∂S) ∈(0.1γ,0.9γ)) Pr(d(X,∂S) ≤γ) . 28Combining these estimates, we have for some constant C′′ k > 0 which is at worst exponentially small in k and γ sufﬁciently small (to satisfy the conditions above, including the requirement τM>C ′′) that λmax(ΓSM) ≥C′′ k Pr(d(X,∂S) ∈(0.1γ,0.9γ)) Pr(d(X,∂S) ≤γ)2 . (26) Observe that for any points x,y and θ= (θ1,0) we have by the mean value theorem that pθ(x)/pθ(y) = exp (⟨θ1,F1(x) −F1(y)) ≤exp ( ∥θ∥ sup θ∈[0,1] ∥(JF1)θx+(1−θ)y∥OP∥x−y∥ ) . (27) so the log of the density is Lipschitz. This basically reduces estimatingPr(d(X,∂S) ≤γ) for small γto understanding the volume of tubes around ∂S, which can be done using the same ideas as the proof of Weyl’s tube formula [Weyl, 1939, Gray, 2003]. Lemma 6 (Proposition 6.1 of Niyogi et al. [2008]). Let Mbe a smooth and compact submanifold of dimension qin Rd. At a point p ∈M let B : Tp ×Tp →T⊥ p denote the second fundamental form, and for a unit normal vector u, let Lu be the linear operator deﬁned so that ⟨u,B(v,w)⟩= ⟨v,Luw⟩(this matches the notation from Niyogi et al. [2008]). Then ∥Lu∥OP ≤ 1 τM . Lemma 7 (Lemma 3.14 of Gray [2003]) . Let Mbe a smooth and compact submanifold of dimension q in Rd. Let expp denote the exponential map from the normal bundle at p. The Jacobian determinant of the map M×(−1/τM,1/τM) ×Sd−q−1 →Rd, (p,t,u ) ↦→expp(tu) is det(I−tLu). We can compute Pr(d(X,∂S) ≤r) = ∫ x:d(x,∂S)≤r pθ(x)dx= ∫ p∈∂S ∫ r 0 ∫ S0 det(I−tLu)pθ(expp(tu)) dudtdA where in the second equality we performed a change of variables and obtained the result by applying Lemma 7. We have det(I−tLu) ∈[(1 −t/τ)k,(1 + t/τ)k] and so applying (27) we ﬁnd that if we deﬁne c := γ∥θ∥supx:d(x,∂S)≤γ∥(JF1)x∥OP which can be made arbitrarily small by taking γsufﬁciently small, then Pr(d(X,∂S) ≤r) ∈[2e−cγ(1 −γ/τ)kV,2ecγ(1 + γ/τ)kV] (28) where V := ∫ ∂S p(x)dA. Note that (1+γ/τ)k ≤ekγ/τ and (1−γ/τ)k ≥exp(−O(γk/τ)) provided that γ/τ = O(1/k). Since Pr(d(X,∂S) ∈ (0.1γ,0.9γ)) = Pr( d(X,∂S) < 0.9γ) −Pr(d(X,∂S) ≤0.1γ) and the distribution we consider has a density, by combining (28) and (26) we ﬁnd that for γsufﬁciently small we have λmax(ΓSM) ≥C′′′ k 1 γ ∫ ∂S p(x)dA where C′′′ k is at worst exponentially small in k. 29B.2 Relating Fisher matrices of augmented and original sufﬁcient statistics Next, we show that adding the extra sufﬁcient statistic F2 has a comparatively minor effect on the efﬁciency of MLE. Intuitively, to be able to estimate the coefﬁcient of F2 correctly we just need: (1) the variance of F2 is large, so that a nonzero coefﬁcient of F2 can be observed from samples (e.g. when F2 encodes the cut S, the coefﬁcient can be estimated by looking at the relative weight between S and SC), and (2) there is no redundancy in the sufﬁcient statistics, e.g. F2 ̸= F1 since otherwise different coefﬁcients can encode the same distribution. The proof of this uses that the inverse covariance of the MLE has a simple explicit form (the Fisher information, which is the covariance matrix of (F1,F2)), and conditions (1) and (2) naturally appear when we use this fact. Quantitatively, we show: Lemma 8. Suppose that F = (F1,F2) is a random vector valued inRm+1 with F1 valued in Rm and F2 valued in R. Suppose that F2 is not in the afﬁne of linear combinations of the coordinates of F1, i.e. for all w1 ∈Rm there exists δ >0 such that Cov(⟨w1,F1⟩,F2)2 ≤δVar(⟨w1,F1⟩)Var(F2). Then we have the lower bound ΣF ⪰(1 −δ) [ ΣF1 0 0 Var( F2) ] in the standard PSD (positive semideﬁnite) order. Proof. To show a lower bound on ΣF = [ ΣF1 ΣF1F2 ΣF2F1 ΣF2 ] observe that ⟨w,ΣFw⟩= ⟨w1,ΣF1 w1⟩+ 2w2⟨w1,ΣF1F2 ⟩+ w2 2ΣF2 so under the assumption we have by the AM-GM inequality that ⟨w,ΣFw⟩≥ (1 −δ)[⟨w1,ΣF1 w1⟩+ w2 2ΣF2 ] and hence ΣF is lower bounded in the PSD order as long as ΣF1 is and ΣF2 is. The lower bound on Var(F2) is guaranteed when F2 corresponds to a cut with large mass on both sides since the variance of F2 is lower bounded by its variance conditioned on being away from the boundary of S. B.3 Putting together Finally, given Lemma 5 and 8, we can complete the proof of Theorem 3. Proof of Theorem 3. Deﬁne ρ= Pr(X ∈S) for the purpose of this proof. Observe that by (28) Var(1S −F2) ≤E(1S −F2)2 ≤Pr(d(X,∂S) ≤γ) ≤4γV where V = ∫ ∂S p(x)dA. We have that Cov(⟨w1,F1⟩,F2) = Cov(⟨w1,F1⟩,1S) + Cov(⟨w1,F1⟩,F2 −1S) so if w1 is arbitrary and normalized so that Var(⟨w1,F1⟩) = 1 then we have |Cov(⟨w1,F1⟩,F2)|≤ √ 1 −δ1 √ Var(1S) + √ Var(F2 −1S) ≤ (√ 1 −δ1 + 2 √ γV ρ(1 −ρ) ) √ Var(1S). 30Therefore provided δ >0 we have Σ−1 F ⪯1 δ [ Σ−1 F1 0 0 Var( F2)−1 ] . On the other hand, by Lemma 5 we have λmax(ΓSM) ≥ cd γV . Hence there exists some wsuch that σ2 SM(w) σ2 MLE(w) ≥ δcd max{∥Σ−1 F1 ∥OP,1/ρ(1 −ρ)} 1 γV ≥ δcd 1 + ρ(1 −ρ)∥Σ−1 F1 ∥OP ρ(1 −ρ) γV . Using that min{ρ,1 −ρ}/2 ≤ρ(1 −ρ) ≤1/4 and dividing cby two gives the result. 31",
      "references": [
        "Generative modeling by estimating gradients of the data distribution",
        "Estimation of non-normalized statistical models by score matching",
        "Asymptotic statistics",
        "Metastability in reversible diffusion processes i: Sharp asymptotics for capacities and exit times",
        "Metastability in reversible diffusion processes ii: Precise asymptotics for small eigenvalues",
        "Stochastic analysis on manifolds",
        "An inequality for relative entropy and logarithmic sobolev inequalities in euclidean spaces",
        "Approximate tensorization of entropy at high temperature",
        "Some extensions of score matching",
        "Logarithmic sobolev inequalities",
        "The geometry of markov diffusion generators",
        "Analysis and geometry of Markov diffusion operators",
        "Probability in high dimension",
        "Entropic independence ii: optimal sampling and concentration via restricted modiﬁed log-sobolev inequalities",
        "An isoperimetric inequality on the discrete cube, and an elementary proof of the isoperimetric inequality in gauss space",
        "The analysis of linear partial differential operators I: Distribution theory and Fourier analysis",
        "Markov chains and mixing times",
        "Curvature measures",
        "Finding the homology of submanifolds with high conﬁdence from random samples",
        "Bayesian model comparison with the hyv ¨arinen score: Computation and consistency",
        "Rapid convergence of the unadjusted langevin algorithm: Isoperimetry sufﬁces",
        "Understanding machine learning: From theory to algorithms",
        "Local rademacher complexities",
        "Linear estimating equations for exponential families with application to gaussian linear concentration models",
        "Minimum stein discrepancy estimators",
        "Sliced score matching: A scalable approach to density and score estimation",
        "Probability: theory and examples",
        "Operator reverse monotonicity of the inverse",
        "Dimension-free log-sobolev inequalities for mixture distributions",
        "Statistical analysis of non-lattice data",
        "Efﬁciency of pseudolikelihood estimation for simple gaussian ﬁelds",
        "Interpretation and generalization of score matching",
        "Modiﬁed logarithmic sobolev inequalities in discrete settings",
        "Logarithmic sobolev inequalities for ﬁnite markov chains",
        "Optimal mixing of glauber dynamics: Entropy factorization via high-dimensional expansion",
        "Elements of information theory",
        "Entropic independence in high-dimensional expanders: Modiﬁed log-sobolev inequalities for fractionally log-concave polynomials and the ising model",
        "Sparse logistic regression learns all discrete pairwise graphical models",
        "Optimal structure and parameter learning of ising models",
        "Learning graphical models using multiplicative weights",
        "Learning some popular gaussian graphical models without condition number bounds",
        "The computational hardness of counting in two-spin models on d-regular graphs",
        "A connection between score matching and denoising autoencoders",
        "Fast approximations of the jeffreys divergence between univariate gaussian mixtures via mixture conversions to exponential-polynomial distributions",
        "Skewed jensen—ﬁsher divergence and its bounds",
        "Relative entropy and score function: New information-estimation relationships through arbitrary additive perturbation",
        "Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables",
        "Optimal approximation of signal priors",
        "Identity testing for high-dimensional distributions via entropy tensorization",
        "Generative modeling with denoising auto-encoders and langevin sampling",
        "Convergence for score-based generative modeling with polynomial complexity",
        "Convergence of score-based generative modeling for general data distributions",
        "Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions",
        "Beyond log-concavity: Provable guarantees for sampling multi-modal distributions using simulated tempering langevin monte carlo",
        "Tubes",
        "On the volume of tubes"
      ],
      "meta_data": {
        "arxiv_id": "2210.00726v2",
        "authors": [
          "Frederic Koehler",
          "Alexander Heckett",
          "Andrej Risteski"
        ],
        "published_date": "2022-10-03T06:09:01Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper establishes a tight connection between the statistical efficiency of score matching and the isoperimetric properties (Poincaré, log-Sobolev, and isoperimetric constants) of the distribution being estimated. It demonstrates that score matching is statistically comparable to maximum likelihood estimation (MLE) when the distribution has a small isoperimetric constant, but substantially less efficient when the constant is large (e.g., for multimodal or low-dimensional manifold structures). The work formalizes these results in both finite sample and asymptotic regimes, providing a formal justification and generalization of conjectured challenges for score matching. Additionally, it identifies a direct parallel between score matching in continuous settings and pseudolikelihood estimation in discrete settings.",
        "methodology": "The study primarily employs functional analytic tools, specifically the Poincaré, log-Sobolev, and isoperimetric constants, which characterize the isoperimetric properties of distributions and govern the mixing time of Markov processes like Langevin dynamics. It compares the score matching estimator, which fits the score function ∇x log p(x), with the classical maximum likelihood estimator (MLE). For discrete settings, the paper draws an analogy with pseudolikelihood methods, Glauber dynamics (Gibbs sampler), and approximate tensorization of entropy. Molliﬁers are used to construct additional sufficient statistics for demonstrating inefficiency. Mathematical proofs involve concepts from generalization theory (Rademacher complexity) for finite-sample bounds and differential geometry (reach, condition number of a manifold, divergence theorem, Weyl's tube formula) for asymptotic efficiency lower bounds.",
        "experimental_setup": "The research validates its theoretical findings through several simulations. For exponential family experiments, it investigates fitting bimodal distributions both with and without a 'cut' sufficient statistic, and a unimodal distribution with a rapidly oscillating sufficient statistic. In these experiments, the performance of score matching is compared against MLE by plotting the log of the Euclidean distance to the true parameter and analyzing the distribution of errors. Additionally, the paper demonstrates empirical robustness beyond exponential families by fitting a mixture of two Gaussians using score matching with a one-hidden-layer neural network (with tanh activations), comparing the learned distribution's PDF to the ground truth.",
        "limitations": "The primary limitation highlighted is that score matching can be substantially less statistically efficient than maximum likelihood estimation, particularly for distributions exhibiting large isoperimetric constants (e.g., complex, high-dimensional, multimodal distributions with well-separated modes, or distributions supported over manifolds with negative curvature). The efficiency of score matching also depends on the smoothness of the sufficient statistics; rapidly oscillating functions can lead to poor performance even for unimodal distributions. The theoretical lower bounds on inefficiency (Theorem 3) can have constants that are exponentially small in dimension, although the bounds remain relevant when isoperimetric constants are exponentially large.",
        "future_research_directions": "Future research directions include formally characterizing the improvements offered by annealing strategies, such as those used in denoising score matching (e.g., Song and Ermon [2019]), similar to how it has been done for sampling with Langevin dynamics (Lee et al. [2018]). This would involve understanding how convolving input samples with a sequence of Gaussians with different variances helps mitigate the challenges posed by multimodality and low-dimensional manifold structures for score matching.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Enhancing LLM Reasoning via Vision-Augmented Prompting",
      "full_text": "Efficient Lifelong Model Evaluation in an Era of Rapid Progress Ameya Prabhu∗1,3 Vishaal Udandarao*1,2 Philip H.S. Torr3 Matthias Bethge1† Adel Bibi3† Samuel Albanie2† 1Tübingen AI Center, University of Tübingen 2University of Cambridge 3University of Oxford /githubhttps://github.com/bethgelab/sort-and-search /da◎abasehttps://huggingface.co/datasets/bethgelab/lifelong_benchmarks Abstract Standardized benchmarks drive progress in machine learning. However, with re- peated testing, the risk of overfitting grows as algorithms over-exploit benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by compiling ever- expanding large-scale benchmarks called Lifelong Benchmarks. These benchmarks introduce a major challenge: the high cost of evaluating a growing number of models across very large sample sets. To address this challenge, we introduce an efficient framework for model evaluation,Sort & Search (S&S), which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples. To test our approach at scale, we create Lifelong-CIFAR10 and Lifelong-ImageNet, containing 1.69M and 1.98M test samples for classification. Extensive empirical evaluations across ∼31,000 models demonstrate that S&S achieves highly-efficient approximate accuracy mea- surement, reducing compute cost from 180 GPU days to 5 GPU hours (∼1000x reduction) on a single A100 GPU, with low approximation error and memory cost of <100MB. Our work also highlights issues with current accuracy prediction metrics, suggesting a need to move towards sample-level evaluation metrics. We hope to guide future research by showing our method’s bottleneck lies primarily in generalizing Sort beyond a single rank order and not in improving Search. . . . . . . f1 f2 fm x1 x2 xn Models Samples         m ✕ n 1 0 0 000 1 1 1 . . . . . . . . . . . . . . . . . . . . . Initial Accuracy Predictions Efficient Model Evaluation 1 0 0 0 0 0   1 1 1 . . . . . . . . .    . . . . . . . . . . . .  fm+1  Predictions? New Model {x1 , x2 ,…, xn} 1 0 0 0 0 0   1 1 1 . . . . . . . . .    . . . . . . . . . . . .    Predictions? Sample Pool select subset eval on subset  xn+1 New Sample Existing Models eval on subsetEfficient Insertion {f1 , f2 ,…, fm} select subset of size n' of size m' Figure 1: Efficient Lifelong Model Evaluation. Assume an initial pool of n samples and m models evaluated on these samples (left). Our goal is to efficiently evaluate a new model ( insertM) at sub-linear cost (right top) and efficiently insert a new sample into the lifelong benchmark (insertD) by determining sample difficulty at sub-linear cost (right bottom). See Section 2 for more details. ∗equal contribution, † equal supervising 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2402.19472v2  [cs.LG]  23 Nov 20241 Introduction The primary goal of standard evaluation benchmarks is to assess model performance on some task using data that is representative of the visual world [87]. For instance, the CIFAR10 [54] benchmark tested whether classifiers can distinguish between 10 categories, such as dogs and cats. Subsequent versions like CIFAR10.1 [59], CIFAR10.2 [59], CINIC10 [21], and CIFAR10-W [83] introduced more challenging and diverse samples to evaluate the same objective of classifying 10 categories. As benchmarks become standardized and repeatedly used to evaluate competing methods, they gradually lose their capacity to represent broader tasks effectively. This is because models become increasingly specialized to perform well on these specific benchmarks. This phenomenon, known as overfitting, occurs both in individual models and within the research community as a whole [ 28, 90]. Fresh approaches must compete with a body of methods that have been highly tuned to such benchmarks, incentivising further overfitting if they are to compete [9, 10]. One approach to preventing models from overfitting to biases [87, 3] is to move beyond fixed test sets by creating an ever-expanding pool of test samples. This approach, known as Lifelong Model Evaluation, aims to restore the representativeness of benchmarks to reflect the diversity of the visual world by expanding the coverage of test sets. One can expand the pool by combining datasets or using well-studied techniques like dynamic sampling [ 81, 51, 52], these expanding benchmarks can grow substantially in size as they accumulate samples. This raises the less-explored issue of increasing evaluation costs. As an example, it takes roughly 140 and 40 GPU days respectively to evaluate our current model set on our Lifelong-CIFAR10 and Lifelong-ImageNet datasets (containing 31,000 and 167 models respectively). These issues are only exacerbated in benchmarking foundation models [15]. For instance, evaluating a single large language model (LLM) on MMLU [40] (standard benchmark for evaluating LLMs) takes 24 hours on a consumer-grade GPU [45]. This inevitably will lead to a surge in evaluation costs when benchmarking lots of increasingly expensive models against an ever-growing collection of test samples [78, 22]. Hence, we primarily ask: Can we reduce this evaluation cost while minimising the prediction error? We design algorithms to enable efficient evaluation in lifelong benchmarks, inspired by computerized adaptive testing (CAT) [89]. CAT is a method used to create exams like the GRE and SAT from a continuously growing pool of questions. Unlike traditional tests where all questions must be answered, CAT sub-samples questions based on examinee responses. This approach efficiently gauges proficiency with far fewer questions, while maintaining assessment accuracy. Similarly, we aim to evaluate classification ability of new models without testing on all samples, instead selecting a subset of samples to evaluate models. We propose a method, Sort & Search (S&S), which reuses past model evaluations on a sample set through dynamic programming to enable efficient evaluation of new incoming models. S&S operates by first ranking test samples by their difficulty, done efficiently by leveraging data from previous tests. It then uses these updated rankings to evaluate new models, streamlining the benchmarking process. This strategy enables efficient lifelong benchmarking, reducing the cost dramatically from a collective of 180 GPU days to 5 GPU hours on a single A100 GPU. We achieve a 1000× reduction in inference costs compared to static evaluation on all samples, reducing over 99.9% of computation costs while accurately predicting sample-wise performance. Moreover, with a single algorithm, we address both key challenges: expanding dataset size and evaluating new models given a dataset. Taken together, our main contributions are: 1. We curate two lifelong benchmarks: Lifelong-CIFAR10 and Lifelong-ImageNet, consisting of 1.69M and 1.98M samples respectively. 2. We propose Sort & Search, a novel framework for efficient model evaluation. 3. We show that our simple framework is far more scalable and allows saving 1000x evaluation cost. 4. We provide a novel decomposition of errors in Sort & Search into largely independent sub- components (aleatoric and epistemic errors). 5. We prove and empirically validate that our solution for the Search sub-component reaches the optimal solution and our framework is stable under repeated additions without any degradation. 2 Lifelong Model Evaluation: Formulation and Challenges We first formalise evaluation in lifelong model evaluation and describe the key challenges it raises. 2Formulation. Let D=((x1, y1), . . . ,(xn, yn)) denote an ordered collection of labeled examples, sampled from the underlying task distribution of interest P(X×Y ). Here, xi∈X denotes the ith data sample and yi∈Y denotes the corresponding label. Let M=(f1, . . . , fm) denote an ordered collection of models where each model, f:X→Y , maps data samples to predicted labels. Lifelong benchmark, B=(D, M, insertD, insertM, metrics), augments D and M with three operations: 1 insertD((x′, y′)) inserts a new labeled example (x′, y′) into D. 2 insertM(f′) inserts a new model f′ into M. 3 metrics() returns a |M|-dimensional vector estimating each model’s performance. Key challenges. When new models are proposed, the setM expands over time. Similarly, the sample collection, D expands as new evaluation datasets get proposed to test various aspects of the problem and resist overfitting. The key question becomes: How to efficiently update the benchmark? We can instantiate a “naive” implementation of the metrics() operation ( 3 ) by simply re-evaluating every model on every sample after each call to insertM ( 2 ) or insertD ( 1 ). However, such a strategy exhibits O(|D||M|) runtime complexity for each call to metrics(), rendering lifelong model evaluation practically infeasible as D and M grow. The central question considered by this work is therefore the following: Given a lifelong benchmark B, how can we efficiently compute metrics() each time we insert new labeled samples into D ( 1 ) or new models into M ( 2 )? Inserting ∆m models ( 2 insertM). Suppose that ∆m new models have just been released. We wish to insert these new models into M and efficiently predict performance of these new models. A naive approach would entail evaluating the ∆m models on all |D| samples. Our first challenge is: Can we instead generate the prediction matrix by performing inference only on a small subset of n′ ≪ |D|samples? We want to enable accurate prediction of the remaining entries in the prediction matrix. Inserting ∆n samples ( 1 insertD). Our second challenge arises when we obtain new ∆n labeled data examples. We seek to insert these samples into D and efficiently predict performance of these new samples. A naive approach entails evaluating all |M| models on the ∆n new examples. As above, to substantially reduce cost, we select a small subset of m′ ≪ |M|models with the objective of accurately predicting the remaining entries of the prediction matrix corresponding to the new ∆n samples. Approach. Our approach is characterized by two key ideas. First, we augment B with an instance- level accuracy cache to amortise inference costs across evaluations. The cache is instantiated as a matrix A ∈ {0, 1}|M|×|D| where A(i, j) ≜ I[fi(xj) = yj]. Second, we propose strategies to efficiently generate the prediction matrix Y ∈ {0, 1}|M|×|D|, using a combination of sampling and inference leveraging the accuracy cache. Our methodology is illustrated in Fig. 1. Connections to Existing Literature. The lifelong model evaluation setup, where M and D grow over time, has received limited attention [ 3], the sub-challenge of efficiently evaluating models when new models are released has received more focus. Concretely, this maps to the problem of insertM ( 2 ) within our framework. We comprehensively draw connections across different research directions in Appendix G and briefly present the most similar works here. Model Spider [105] efficiently ranks models from a pre-trained model zoo. LOVM [ 110], Flash-Eval [106] and Flash-HELM [67] similarly rank foundation models efficiently on unseen datasets. However, these approaches predict dataset-level metrics rather than instance-level metrics, and thereby cannot be used in our setup to grow the prediction cache efficiently (see Section 2.1). Concurrent to our work, Anchor Point Sampling [91] and IRT-Clustering [69] both propose efficient instance-level evaluations by creating smaller core-sets from test data. They introduce clustering-based approaches and item response theory [4] to obtain sample-wise accuracy predictions. However, their methods require memory and time complexity quadratic in the number of data samples, i.e., O(|D|2) requiring well over 10TB of RAM for benchmarks having a million samples. The comparisons are infeasible to scale on datasets bigger than a few thousand samples. In contrast, our novel Sort & Search approach, requires memory and time complexity of O(|D| log |D|) with the number of samples, and can scale up to billion-sized test sets (see Section 4 for empirical results). In practice, our method only requiring only two 1D arrays of size of the number of samples, requiring extremely minimal storage overhead, being less than 3GB in absolute terms on billion scale datasets. Furthermore, we motivate why one should adopt sample-wise prediction instead of overall accuracy prediction below. 32.1 Why Adopt Sample-wise Prediction Metrics instead of Overall Accuracy Prediction? Given model predictions ym+1 and ground-truth predictions am+1, current methods typically mea- sures whether one can predict the average accuracy over the full test, measured by mean absolute difference of aggregate accuracies Eagg(ym+1, am+1) = |(|ym+1|−|am+1|)|/n. We argue this is highly unreliable as minimizing the metric only requires predicting the count of 1s in the prediction array rather than correctly predicting on a sample level. For instance, consider a ground-truth prediction array of [0,0,0,1,1,1]. A method that predicts [1,1,1,0,0,0] as the estimated prediction array achieves optimal Eagg of 0 despite not predicting even a single sample prediction correctly! More generally, it is always possible to obtain globally optimal Eagg of 0 while having worst-case mean-absolute error E for any ground truth accuracy am+1. Formally, Theorem 2.1. Given any ground-truth vector am+1, it is possible to construct a prediction vector ym+1 such that Eagg(ym+1, am+1) = 0 and E(am+1, ym+1) = 2.min(1 − |am+1|/n, |am+1|/n) One might wonder whether these worst case bounds ever occur in practice. We empirically test a simple yet optimal array construction, given with oracle ground-truth dataset-level accuracy ofk2, which achieves Eagg = 0, and consistently observe high mean-absolute error E of 0.4−0.5 on a sample level on our lifelong benchmarks (n=∼106), i.e., the model incorrectly predicts 40−50% of the samples in a binary classification task, which is surprisingly high. In comparison, our S&S method, without any oracle access, gets 0.15−0.17 mean-absolute error with just n′=100 samples (at 10, 000x compute saving) on the same benchmarks. Overall, this demonstrates that thoughtful sample-level prediction mechanisms are necessary for efficient lifelong evaluation. 3 Sort & Search: Enabling Efficient Lifelong Model Evaluation Inspired by CAT [89], we propose an efficient lifelong evaluation framework, Sort & Search (S&S), comprising two components: (1) Ranking test samples from the entire dataset pool according to their difficulty3, i.e., Sort and (2) Sampling a subset from the pool to test on, i.e., Search. This framework effectively tackles the two key operations noted in Section 2 ( 1 insertD and 2 insertM). We first describe our Sort and Search method in the case when new models are added ( 2 insertM), and subsequently show that the same procedure applies when we have new incoming samples ( 1 insertD) simply by transposing the cache ( A → AT ). A full schematic of our pipeline is depicted in Fig. 2. 3.1 Ranking by Sort Setup. We recall that our lifelong benchmark pool consists of evaluations of |M| models on |D| samples. For ease of reference, say |M|=m and |D|=n, and we have our cache A ∈ {0, 1}m×n (see Fig. 1 left). We can decompose the cache A row-wise corresponding to each model fi, i ∈ {1, .., m}, obtaining the binary accuracy prediction across the n samples, denoted by ai = [pi1, pi2 . . . , pin]. Here, pij∈{0, 1} represents whether the model fi classified the sample xj correctly. Goal. Given the cache A, we want to obtain a ranked order (from easy to hard) for its columns, which represent the samples. This sorted order ( Sort) can later be used for efficient prediction on new incoming models (Search). We want to find the best global permutation matrix P ∈ {0, 1}n×n, a binary matrix, such that AP permutes the columns of A so that we can rank samples from easy (all 1s across models) to hard (all 0s across all models). We say this has a minimum distance from the optimal ranked accuracy prediction matrix Y ∈ {0, 1}m×n computed by the hamming distance between them, posed as solving the following problem: P∗, Y∗ = argminP,Y∥AP − Y∥1, s.t. P ∈ {0, 1}n×n, P1n = 1n, 1⊤ n P = 1n, if Yij = 1, then Yij′ = 1 ∀j′ ≤ j, if Yij = 0, then Yij′ = 0 ∀j′ ≥ j. (1) 2Given |am+1| = k, one can show the prediction array [1⊤ k , 0⊤ n−k] achieves optimal Eagg = 0 3If a sample xi is more “difficult\" than a sample xj then at least equal number of models predict xj correctly as the number of models predicting xi correctly [6]. 4Figure 2: Full Pipeline of Sort & Search. For efficiently evaluating new models, (Left) we first sort all data samples by difficulty (refer Section 3.1) and (Right) then perform a uniform sampling followed by DP-search and extrapolation for yielding new model predictions (refer Section 3.2). This entire framework can also be transposed to efficiently insert new samples (refer Section 3.3). By definition of a permutation matrix, the constraints P1n = 1n, 1⊤ n P = 1n on binary P enforces by definition that P is a valid permutation matrix. The ranked accuracy prediction matrix Y is a binary matrix created by a row-wise application of a thresholding operator for every row in Y separately. The informal explanation of the optimization problem in Eq. (1) is to find an ordering of samples such that error introduced by thresholding is minimized. We next discuss how to solve this optimization. While the goal is finding the optimal permutationP∗, we still need to jointly solve for P, Y here. We find a solution by alternating between optimizing P keeping Y constant and optimizing Y keeping P constant, with the goal of finding the best P∗, with a coordinate descent algorithm. We now present algorithms for optimizing the two subproblems. 3.1.1 Optimizing P Given Y We know P is binary from Eq. (1). Hence, finding the optimal P∗ is NP-Hard [101]. To simplify the sub-problem, we first present an algorithm to solve the case where we can order samples in a strictly decreasing order of difficulty, measured by how many models classified it correctly ( 1 ). However, samples cannot be arranged as strictly decreasing in practice. Subsequently, we present an alternative which computes soft confidences, enabling the strictly decreasing constraint to hold ( 2 ). A third alternative we explore removes the introduced constraint of a strictly decreasing order ( 3 ). 1 Sorting by Sum. We discuss how to order samples if they follow a strictly decreasing order of difficulty. We can order samples in decreasing order of difficulty by a simple algorithm detailed in Listing 1 (sort_by_sum)—intuitively, this algorithm greedily sorts samples from easy (more 1s) to hard (less 1s) by sorting the sum vector across rows per column (which can trivially be converted to the permutation matrix P∗). However, the assumption of strictly decreasing order of difficulty is unrealistic as the number of samples is usually far larger than the number of models. Hence, it is guaranteed that many samples will have the same level of difficulty by the pigeonhole principle [ 2]. We propose to address this by two methods: (a) Converting the cache (A) to store confidence predictions of ground truth class rather than accuracy (Algorithm 2 ), or (b) Iteratively optimizing rows which are tied in sum values (Algorithm 3 ). Note that we find 1 Sorting by Sum effective in all our tested scenarios, but provide these alternatives in the case where it is insufficient. 2 Sorting by Confidence Sum. One method to have a strictly decreasing order is to relax the constraint on the samples of ai = [ pi1, pi2 . . . , pin] from pij ∈ {0, 1} to pij ∈ [0, 1], and use confidence of the ground truth class. This modification allows all examples to be unique. The procedure is then identical to Sorting by Sum, i.e. algorithm still greedily sorts samples from easy (more 1s) to hard (less 1s) by sorting the sum vector across rows per column. 3 Recursive Sorting by Sum. Another alternative is relaxing the equal difficulty assumption in Algorithm 1 . A natural question is: How does one order samples which have equal number of models predicting them correctly, i.e., two columns ofA with equal number of 1s? 5def sort_by_sum(A): sum_ranking = A.sum(axis=0) order = np.flip(np.argsort(sum_ranking)) return order def two_stage_sort_by_sum(A, idx): #Step 1: Sum order = sort_by_sum(A) #Step 1: Search thresh = dp_search(A[:, order]) #Iterate over bins bins_ordered = sum_bins[order] uniq_bins = np.unique(bins_ordered) for u_bin in uniq_bins: idx = np.nonzero(bins_ordered==u_bin)[0] bin_thresh = np.nonzero(np.all([[bins_ordered >= idx.min()], [bins_ordered <= idx.max()]], axis=0))[1] ,→ ,→ ,→ At = A[thresh][:, order[idx]] #Step 2: Sum new_order = sort_by_sum(At) # Replace current ordering within new in bin,→ order[idx] = order[idx[new_order]] return order def uniform_sampling(query, num_p): # idx -> num_p uniformly sampled points idx = np.arange(0, len(query), len(query)//num_p)[1:] return idx def dp_search(query): # query is 1 x k (from a row of PA) # (k can be assigned := n, n', m, m') query[query==0] = -1 cumsum = np.cumsum(query) idx = np.argmax(cumsum) return idx/len(query) # threshold as % of length, transfers n' -> n size,→ Listing 1: (Left) Algorithm for Optimizing P given Y (Right) Algorithm for Optimizing Y given P We propose an iterative solution: at each step, order samples of equal difficulty by alternatively optimizing P keeping Y constant by applying Algorithm 1 and optimizing Y keeping P constant by DP-Search algorithm (presented in the next Section). We provide the algorithm for two iterations for an illustration in Listing 1 ( two_stage_sort_by_sum). Note that this strictly improves the solution at each recursion depth. Note that ties are broken by preferring the model which minimizes error the most. 3.1.2 Optimizing Y given a P. Optimizing Y given a P, is equivalent to finding a row-wise threshold k ≤ n minimizing the error with the matrix AP for a given P. Intuitively, if the threshold for the ith row is k, then the ith row is of the form [1⊤ k , 0⊤ n−k] where 1k is a vector of all ones of size k and 0n−k is a zero vector of size n −k. In every row, all samples before the row-wise threshold k are predicted to be correctly classified (easy) and those after are incorrectly classified (hard) for the model corresponding to the row. To optimize Y given P, we propose a dynamic programming algorithm, DP-Search which operates on each row yi, detailed in Listing 1 ( dp_search). Given a row in Y, DP-Search computes the difference between number of 1s and number of 0s for each index. By using a prefix sum structure, for an input of size n, the DP approach reduces time complexity from O(n2) to O(n). The optimal threshold k is the index of the maximum value in this vector. The vector yi is simply [1⊤ k , 0⊤ n−k] where 1k is a vector of all ones of size k and 0n−k is a zero vector of size n − k. DP-Search is guaranteed to return the globally optimal solution: Theorem 3.1. Optimality of Y given P. For any given ai ∈ {0, 1}1×n and P, DP-Search returns an ordered prediction vector yi ∈ {0, 1}1×n which is a global minimum of ∥aiP − yi∥1. Applying DP-Search independently row-wise, the algorithm returns the optimal Y given P. Now, we shall 3.1.3 Process Summary We have outlined the process of optimizing (i) P given Y and (ii) Y given P. Note that (i) alone suffices for Sorting Operation when using the 1 Sorting by Sum algorithm, while a combination of (i) and (ii) is primarily needed for 3 Recursive Sorting by Sum. After sorting, we obtain AP∗, which reflects the sample ordering based on difficulty. In the following section, we will reuse (ii) to search for a Y given P to efficiently evaluate new models or add new samples. 63.2 Efficient Selection by Search Goal. After solving Eq. (1), we obtain the optimal P∗ in the sorting phase. We assume that this sample difficulty order generalizes to new models, ∆m. Recall that AP∗ represents the columns of cache A, ordered by sample difficulty (those most often misclassified by models). Given ∆m new models, our goal is to predict accuracy across all n samples for each model, i.e., the accuracy matrix Y∆m ∈ {0, 1}∆m×n. This would be simple if we could evaluate all ∆m models on all n samples, but this approach is costly. The challenge is thus to predict performance on the remaining samples while evaluating only a small subset n′ ≪ n. Hence, we will assume that we can create a smaller ground truth subset a′ m+1 and study: How to find the best accuracy prediction vector ym+1? We use the ground truth vector am+1 for evaluating the efficacy of our method. Recall that evaluation of every new model can be done independently of others, i.e.Y∆m is separable per row. Hence, we describe the problem for the first new model ym+1 ∈ {0, 1}1×n here. (i) How to get the optimal ym+1? Our goal here is to generate the sample-wise prediction vector ym+1 ∈ {0, 1}1×n. We divide it into two subtasks: selection and optimization. The selection task is to select the best n′ observations to sample. The optimization task is, given the n′ observations a′ m+1 ∈ {0, 1}1×n′ how to generate the prediction vector ym+1 ∈ {0, 1}1×n. Subtask 1: How to Select Samples? We want to find the best n′ observations forming a′. Note that any ranked solution we obtain using this vector needs to be interpolated from n′ points to n points— we use this intuition to sample n′ points. Hence, a simple solution is to sample points such that any threshold found minimizes the difference between the actual threshold and a threshold predicted by our set of n′, i.e., sample n′ points uniformly, providing the algorithm in Listing 1 (uniform_sampling). We also compare empirically with a pure random sampling approach in Section 4. Subtask 2: Optimizing ym+1. Given the n′ observations a′ m+1 ∈ {0, 1}1×n′ , how to generate the prediction vector ym+1 ∈ {0, 1}1×n? We use the threshold given byDP-Search (Listing 1) and obtain the threshold, given in terms of fraction of samples in |a′ m+1|. We extrapolate this threshold from n′ to n points, to obtain the threshold for the prediction vector ym+1. ym+1 is simply [1⊤ k , 0⊤ n−k] where 1k is a vector of all ones of size k and 0n−k is a zero vector of size n − k. So far, we have only discussed evaluation of ∆m new models ( 2 insertM). How can we also efficiently extend the benchmark i.e. efficiently adding ∆n new samples ( 1 insertD)? 3.3 Efficient Insertion of New Samples ( insertD) To add new samples into our lifelong benchmark efficiently, we have to estimate their difficulty with respect to the other samples in the cache A. To efficiently determine difficulty by only evaluating m′ ≪ m models, a ranking over models is required to enable optimally sub-sampling a subset of m′ models. This problem is quite similar in structure to the previously discussed addition of new models, where we had to evaluate using a subset of n′ ≪ n samples. How do we connect the two problems? We recast the same optimization objectives as described in Eq. (1), but replaceA with A⊤ and Y with Y⊤. In this case, Eq. (1) would have A⊤P, which would sort models, instead of samples, based on their aggregate sum over samples (i.e., accuracy) optimized using Algorithm 1 to obtain P∗, ordering the models from classifying least samples correctly to most samples correctly. Here, Algorithm 1 is sufficient, without needing to solve the joint optimization ( 3 ) because accuracies (sum across rows) are unique as the number of samples is typically much larger than the number of models. In case of new incoming samples ∆n, we similarly would treat every sample independently and optimize the predicted array y⊤ n+1 using Efficient Selection by Search (Section 3.2). 4 Experiments To validate Sort & Search empirically, we showcase experiments on two tasks: 1 efficient estima- tion of new sample difficulties (insertD) and 2 efficient performance evaluation of new models (insertM). We then comprehensively analyse various design choices within our S&S framework. 74.1 Experimental Details Lifelong-Datasets. We combine 31 domains of different CIFAR10-like datasets comprising samples with various distribution shifts, synthetic samples generated by diffusion models, and samples queried from different search engines to formLifelong-CIFAR10. We deduplicate our dataset and downsample images to 32×32. Our final dataset consists of 1.69M samples. Similarly, we source test samples from ImageNet and corresponding variants to form Lifelong-Imagenet, designed for increased sample diversity (43 unique domains) while operating on the same ImageNet classes. We include samples from different web-engines and generated using diffusion models. Our final Lifelong-ImageNet contains 1.98M samples (see full list of dataset breakdown in Appendix C). Model Space. For Lifelong-CIFAR10, we use 31, 250 CIFAR-10 pre-trained models from the NATS-Bench-Topology-search space [25]. For Lifelong-ImageNet, we use 167 ImageNet-1K and ImageNet-21K pre-trained models, sourced primarily from timm [98] and imagenet-testbed [84]. Sample Addition Split ( 1 insertD). To study efficient estimation of new sample difficulties on Lifelong-CIFAR10, we hold-out CIFAR-10W [83] samples for evaluation (∼500, 000 samples) and use the rest ∼1.2 million samples for sorting. We do not perform experiments for Lifelong-Imagenet since the number of models is quite small (167 in total), directly evaluating all models is relatively efficient, as opposed to the more challenging Lifelong-CIFAR10 where evaluation on 31, 250 models is expensive, practically necessitating reducing the number of models evaluated per new sample. Model Evaluation Split ( 2 insertM). To study efficient evaluation of new models, we split the model set for the Lifelong-CIFAR10 benchmark into a randomly selected subset of 6, 000 models for ordering samples (i.e., Sort) and evaluate metrics on the remaining 25, 250 models (i.e., Search). For Lifelong-Imagenet, we use 50 random models for ordering samples and evaluate on 117 models. Metrics ( 3 metrics()). We measure errors between estimated predictions for each new model ym+1 and ground-truth predictions am+1 using mean-absolute error (MAE): E(am+1, ym+1): E(am+1, ym+1) = ∥am+1P∗−ym+1∥1/n (2) 4.2 Results: Sample-Level Model Performance Estimation ( insertM) We evaluate the predictive power ofS&S for evaluating new models ( 2 ) when subjected to a varying sampling budgets n′ i.e., we run our S&S over 13 different sampling budgets: {8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768} on both Lifelong-ImageNet and Lifelong-CIFAR10. Our main results in Sections 4.2 and 4.3 use Sorting by Sum ( 1 ) for obtaining P∗ and uniform sampling for the sample budget n′. Using this configuration, we now present our main results. Key Result 1: Extreme Cost-Efficiency. From Figs. 3(a) and 3(b), we note our approach converges to a very low mean-absolute error with 1/1000 the number of evaluation samples, leading to extreme cost savings at inference time (from 180 GPU days to 5 GPU hours on one A100-80GB GPU)4. Key Result 2: Mean Absolute Error Decays Exponentially. Upon analysing the observed E vs. n′ relationship, we note that exponentially decreasing curves fit perfectly in Figs. 3(a) and 3(b). The exponential decay takes the form E=ae−bx+c. The fitted curves have large exponential coefficients b of 0.04 and 0.02. This further shows the surprisingly high sample-efficiency obtained by S&S. Key Result 3: Outperforming Baselines by Large Margins. We construct a competitive, scalable version of Vivek et al. [91] as a baseline, called CopyNearest&Expand: It first samples n′ points out of n (similar to S&S without sorting), and then expands the n′-sized prediction array to n samples by copying the rest n−n′ predictions from the nearest neighbor prediction array from the ranking set of models. We note that this baseline is equivalent to removing the Sort component, and only using random sampling. Comparing to the baseline, we see from Fig. 3(c) that our Sort & Search is: 1) More accurate: It achieved 1% lower MAE at a sampling budget of n′=8192 compared to the baseline, meaning that on average, our S&S correctly classifies ∼19k more samples. 4The “compute saved” axis in the plots is computed as n n′ . Effective compute savings are: In Lifelong- CIFAR10, we do 25, 250×1, 697, 682 evaluations in the full evaluation v/s 25, 250×2, 048 in our evaluation. Similarly, for Lifelong-ImageNet, we perform 117×1, 986, 310 v/s 117×2, 048 evaluations. 8E = 0.16 + 0.07 * exp(-0.04n') Exponential-decay fit: (a) Lifelong-CIFAR10 E = 0.13 + 0.04 * exp(-0.02n') Exponential-decay fit:  (b) Lifelong-ImageNet 101 102 103 104 105 Sampling Budget n' 0.13 0.14 0.15 0.16 0.17 0.18Mean Abs. Error CopyNearest&Expand (baseline) Sort&Search (ours) 106x 105x 104x 103x 102x 101x Compute Saved (c) Baseline Comparison Figure 3: Main Results. (a,b) We achieve 99% cost-savings for new model evaluation onLifelong- ImageNet and Lifelong-CIFAR10 showcasing the efficiency (MAE decays exponentially with n′) of Sort&Search. (c) S&S is more efficient and accurate compared to the baseline on Lifelong-ImageNet. 2) Faster convergence: S&S converges much faster than the baseline (at n′=1, 024 vs. n′=32, 768) thereby showcasing the high degree of sample efficiency in converging to the minimal error. 3) Consistent: Fig. 4(b) shows the better consistency of S&S, across wider range of models used for Sort—at n′=512, S&S with only 10 Sort-models still outperforms the baseline using 50 Sort-models. Storage Efficiency. Storage Efficiency. Our method (S&S) achieves high storage efficiency, requiring only two 1D arrays: one to store the sort-sum and another to construct the current search output. This results in minimal storage overhead, amounting to just 0.0166% of the input data or less than 100 MB in absolute terms. Consequently, Sort&Search not only outperforms alternative methods, such as CopyNearest&Expand, but is also far more memory-optimized. 4.3 Results: Sample Difficulty Estimation ( insertD) We next showcase results for the task ( 1 ) where for new samples, the goal is to sub-sample the number of models to evaluate, for accurately determining sample difficulty. We present results on Lifelong-CIFAR10, with two different methods for ranking models5, Sorting by Sum ( 1 ) and Sorting by Confidence Sum ( 2 ). We evaluate over 9 model budgets m′ (number of models evaluated over): {8, 16, 32, 64, 128, 256, 512, 1024, 2048}. From Fig. 4(a), we observe that both methods converge quickly—Sorting by Sum ( 1 ) reaches an MAE < 0.15 by only evaluating on m′=64 models out of 31, 250 (104× computation savings). This demonstrates our method’s ability to efficiently determine sample difficulty, enabling efficient insertion back into the lifelong-benchmark pool. 4.4 Breaking down Sort & Search Varying the Number of Sort-Models Used. In Fig. 4(b), we analyse the effect of the number of models used for computing the initial ranking (i.e., m) on the final performance onLifelong-ImageNet. Using more models improves MAE— using lesser models for ranking (m=10) converges to a higher MAE (2% difference at convergence when using m=50 (blue line) vs. m=10 (red line)). Note that the m used for ranking does not have any effect on the speed of convergence itself (all methods roughly converge at the same sampling budget (n′=2, 048)), but rather only on the MAE achieved. Different Sorting Methods. We compare the three different algorithms on Lifelong-Imagenet: 1 Sorting by Sum, 2 Sorting by Confidence Sum, and 3 Sorting by Recursive Sum. From Fig. 4(c), we note an MAE degradation when using the continual relaxation of the accuracy prediction values as confidence values, signifying no benefits. However, using the multi-step recursive correction of rankings ( 3 ) provides significant boosts (0.5% boost in MAE at all n′>1, 024) due to its ability to locally correct ranking errors that the global sum method ( 1 ) is unable to account for. Different Sampling Methods. In Fig. 4(d), we compare methods used for sub-selecting the data- samples to evaluate—we compare uniform vs. random sampling. Both methods converge very quickly and at similar budgets to their optimal values and start plateauing. However, uniform sampling provides large boosts over random sampling when the sampling budget is small (5% lower MAE at n′=8)—this can be attributed to its “diversity-seeking” behaviour which helps cover samples from all difficulty ranges, better representing the entire benchmark evaluation samples rather than an unrepresentative random set sampled via random sampling. 5Recursive sum ( 3 ) is not applicable here as all sum values are unique, see Section 3.3. 9101 102 103 Sampling Budget m' 0.150 0.175 0.200 0.225 0.250 0.275Mean Abs. Error Sum Confidence-Sum 106 105 104 103 102 101 Compute Saved (a) Sample Difficulty  (b) #Ranking models 101 102 103 104 Sampling Budget n' 0.14 0.16 0.18 0.20Mean Abs. Error Sum Recursive Sum Confidence Sum (c) Ranking methods 101 102 103 104 Sampling Budget n' 0.14 0.16 0.18 0.20 0.22Mean Abs. Error Uniform Random (d) Sampling methods Figure 4: (a) We achieve accurate sample difficulty estimates onLifelong-CIFAR10(<0.15 MAE) at a fraction of the total number of models to be evaluated, thereby enabling cost-efficient sample insertion. (b,c,d), We analyse three design choices for better understanding S&S, using Lifelong-Imagenet. 4.5 Decomposing the Errors of S&S Here, we showcase a decomposition of the errors of Sort & Search. Specifically, the total mean absolute error E(am+1, ym+1) can be decomposed into a component irreducible by further sampling, referred to as the Aleatoric Sampling Error (Ealeatoric), and a component which can be improved by querying larger fraction of samples n′, referred to as the Epistemic Sampling Error (Eepistemic). Aleatoric Error Lifelong-CIFAR10 - - - - Total Error (E) —— Epistemic Error (Eepistemic) Aleatoric Error - - - - Total Error (E) —— Epistemic Error (Eepistemic) Lifelong-ImageNet Figure 5: Error Decomposition Analysis on Lifelong-CIFAR10 (left) and Lifelong-ImageNet (right). We observe that epistemic error (solid line) drops to 0 within only 100 to 1000 samples across both datasets, indicating this error cannot be reduced further by better sampling methods. The total error E is almost entirely irreducible (Aleatoric), induced because new models do not perfectly align with the ranking order P∗. This suggests generalizing beyond a single rank ordering, not better sampling strategies, should be the focus of subsequent research efforts. Aleatoric Sampling Error. Let y∗ m+1 = y′ when n′ = n, i.e., it is the best prediction obtainable across all subsampled thresholds, as we have access to the full am+1 vector. However, some error remains between y∗ and am+1 due to the ordering operation (i.e., Sort). This error, caused by errors in the generalization of the permutation matrix P∗ cannot be reduced by increasing the sample budget n′. More formally, we define this error as: Ealeatoric(am+1, ym+1) = min ym+1 ∥am+1P∗ − ym+1∥ = ∥am+1P∗ − y∗ m+1∥. (3) Epistemic Sampling Error. Contrarily, there is a gap between optimal ranking prediction y∗ m+1 and ym+1 with the current sample size n′. This gap, Epistemic Sampling Error, is formally defined as: Eepistemic(y∗ m+1, ym+1) = ∥y∗ m+1 − ym+1∥. (4) Results. We analyse sampling effectiveness inLifelong CIFAR-10and Lifelong-ImageNet by studying the Epistemic Sampling Error (Eepistemic) and Aleatoric Sampling Error (Ealeatoric) in Figure 5. First, we see that the epistemic error is very low and quickly converges to 0, i.e., we converge to the best achievable performance within sampling just 100 to 1000 samples on both datasets. The remaining error after that is irreducible. We attribute it primarily caused by generalization gaps in the global permutation matrix P∗ as better approximations like Recursive Sum ( 3 ) did not improve performance as shown in Fig. 4(c). This introduces an interesting question: Do models follow a single global ranking order or are they better decomposed into different rank orders? How consistently do models follow one single global ranking order?We present a detailed analysis in Appendix D to verify this. We calculated the cross-correlation matrix for predictions from 167 10models across the entire Lifelong-Imagenet benchmark (1.9M test samples). Surprisingly, all model pairs showed positive correlations to varying degrees, with no pairs being anti-correlated. Models with near-zero correlations had near-random performance, indicating uncorrelated predictions due to their randomness. Top-performing models exhibited slightly higher correlations. Overall, there was no clear evidence of model cliques. This analysis strongly suggests that model predictions are highly correlated, justifying our choice of using a single ranking function, but the ranking is simply noisy. 5 Conclusion In this work, we address the efficient lifelong evaluation of models. To mitigate the rising evaluation costs on large-scale benchmarks, we proposed an efficient framework called Sort & Search, which leverages previous model predictions to rank and selectively evaluate test samples. Our extensive experiments, involving over 31,000 models, demonstrate that our method reduces evaluation costs by 1000x (over 99.9%) with minimal impact on estimated performance on a sample-level. We aim for Sort & Search to inspire the development of more robust and efficient evaluation methods. Our findings show that model predictions are highly correlated, supporting our use of a single ranking function, though the ranking is somewhat noisy. Our analysis of Sort & Search suggests that future research should focus on generalizing beyond a single rank ordering, rather than on better sampling strategies. Overall, we hope Sort & Search enables large reductions in model evaluation cost and provides promising avenues for future work in lifelong model evaluation. 5.1 Limitations and Open Problems Although showcasing very promising results in enhancing the efficiency of evaluating models on our large-scale Lifelong Benchmarks, our investigation with S&S leads to some interesting open problems: (1) Ranking Imprecision: Our error decomposition analysis provides convincing evidence (Section 4.5) that the ordering of samples P∗ while evaluating new models bottlenecks prediction performance. Generalizing from imposing a single sample ordering P∗ to sample ordering structures, such as different clusters of models each with their own orderings or rejection frameworks for models if it does not align with the ordering could dramatically improve the framework. (2) Identifying Difficult Samples: Finding and labeling challenging examples is an essential task for lifelong benchmarks, which is not the focus of our work. Studying hard or adversarial sample selection approaches with lifelong benchmarking is a promising direction. We provide an extensive survey of related approaches in this direction in Appendix G. (3) Scaling up to Foundation Models: Our work mainly tackles lifelong model evaluation under an image classification setting for trained classification models. Despite it being clear that our method should scale to foundation models, since it only relies on the existence of an A matrix, it would be interesting to test it on more benchmarks from the LLM and VLM domain. Follow-up works [3] have built on this direction. Acknowledgements The authors would like to thank (in alphabetic order): Bruno Andreis, Ça ˘gatay Yıldız, Fabio Pizzati, Federico D’Agostino, Ori Press, Shashwat Goel, and Shyamgopal Karthik for helpful feedback. AP is funded by Meta AI Grant No. DFR05540. VU thanks the International Max Planck Research School for Intelligent Systems (IMPRS-IS) and the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD program for support. VU was supported by a Google PhD Fellowship in Machine Intelligence. PT thanks the Royal Academy of Engineering for their support. AB acknowledges the funding from the KAUST Office of Sponsored Research (OSR-CRG2021-4648) and the support from Google Cloud through the Google Gemma 2 Academic Program GCP Credit Award. SA is supported by a Newton Trust Grant. MB acknowledges financial support via the Open Philantropy Foundation funded by the Good Ventures Foundation. This work was supported by the German Research Foundation (DFG): SFB 1233, Robust Vision: Inference Principles and Neural Mechanisms, TP4, project number: 276693517 and the UKRI grant: Turing AI Fellowship EP/W002981/1. MB is a member of the Machine Learning Cluster of Excellence, funded by 11the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC number 2064/1 – Project number 390727645. References [1] Chirag Agarwal, Daniel D’souza, and Sara Hooker. Estimating example difficulty using variance of gradients. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [2] Miklós Ajtai. The complexity of the pigeonhole principle. Combinatorica, 14:417–433, 1994. [3] Anonymous. Democratizing evaluation with infinity-benchmarks: Sample-level heterogeneous test- ing over arbitrary capabilities. In Submitted to The Thirteenth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=Dj1PVLU8fK. under review. [4] Frank B Baker. The basics of item response theory. ERIC, 2001. [5] Eslam Mohamed Bakr, Pengzhan Sun, Xiaogian Shen, Faizan Farooq Khan, Li Erran Li, and Mohamed Elhoseiny. Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models. InInternational Conference on Computer Vision (ICCV), 2023. [6] Robert Baldock, Hartmut Maennel, and Behnam Neyshabur. Deep learning through the lens of example difficulty. Conference on Neural Information Processing Systems (NeurIPS), 2021. [7] Hritik Bansal and Aditya Grover. Leaving reality to imagination: Robust classification via generated datasets. International Conference on Learning Representations Workshop (ICLR-W), 2023. [8] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Conference on Neural Information Processing Systems (NeurIPS), 2019. [9] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Conference on Fairness, Accountability, and Transparency (FAccT), 2021. [10] Lucas Beyer, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we done with imagenet? In Conference on Neural Information Processing Systems (NeurIPS), 2021. [11] Haoyang Bi, Haiping Ma, Zhenya Huang, Yu Yin, Qi Liu, Enhong Chen, Yu Su, and Shijin Wang. Quality meets diversity: A model-agnostic framework for computerized adaptive testing. In International Conference on Data Mining (ICDM), 2020. [12] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schimdt. Visit-bench: A benchmark for vision-language instruction following inspired by real-world use. Conference on Neural Information Processing Systems (NeurIPS), 2023. [13] Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, and Roy Schwartz. Breaking common sense: Whoops! a vision-and-language benchmark of synthetic and compositional images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2616–2627, 2023. [14] Avrim Blum and Moritz Hardt. The ladder: A reliable leaderboard for machine learning competitions. In International Conference on Machine Learning (ICML), 2015. [15] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. [16] Florian Bordes, Shashank Shekhar, Mark Ibrahim, Diane Bouchacourt, Pascal Vincent, and Ari S Morcos. Pug: Photorealistic and semantically controllable synthetic data for representation learning.arXiv preprint arXiv:2308.03977, 2023. [17] Samuel R Bowman and George E Dahl. What will it take to fix benchmarking in natural language understanding? In North American Chapter of the Association for Computational Linguistics (NAACL), 2021. [18] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. [19] Muxi Chen, Yu Li, and Qiang Xu. Hibug: On human-interpretable model debug. InConference on Neural Information Processing Systems (NeurIPS), 2023. [20] Ciprian A Corneanu, Sergio Escalera, and Aleix M Martinez. Computing the testing error without a testing set. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [21] Luke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet or cifar-10. arXiv preprint arXiv:1810.03505, 2018. 12[22] Mostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish Vaswani, and Yi Tay. The efficiency misnomer. arXiv preprint arXiv:2110.12894, 2021. [23] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition (CVPR), 2009. [24] Greg d’Eon, Jason d’Eon, James R Wright, and Kevin Leyton-Brown. The spotlight: A general method for discovering systematic errors in deep learning models. In Conference on Fairness, Accountability, and Transparency (FAccT), 2022. [25] Xuanyi Dong, Lu Liu, Katarzyna Musial, and Bogdan Gabrys. Nats-bench: Benchmarking nas algorithms for architecture topology and size. Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021. [26] Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with v-usable information. In International Conference on Machine Learning (ICML), 2022. [27] Sabri Eyuboglu, Maya Varma, Khaled Saab, Jean-Benoit Delbrouck, Christopher Lee-Messer, Jared Dunnmon, James Zou, and Christopher Ré. Domino: Discovering systematic errors with cross-modal embeddings. International Conference on Learning Representations (ICLR), 2022. [28] Alex Fang, Simon Kornblith, and Ludwig Schmidt. Does progress on imagenet transfer to real-world datasets? In Conference on Neural Information Processing Systems (NeurIPS), 2023. [29] Wanyong Feng, Aritra Ghosh, Stephen Sireci, and Andrew S Lan. Balancing test accuracy and security in computerized adaptive testing. International Conference on Artificial Intelligence in Education (AIED), 2023. [30] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. In Conference on Neural Information Processing Systems (NeurIPS), 2023. [31] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022. [32] Irena Gao, Gabriel Ilharco, Scott Lundberg, and Marco Tulio Ribeiro. Adaptive testing of computer vision models. In International Conference on Computer Vision (ICCV), 2023. [33] Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, et al. Evaluating models’ local decision boundaries via contrast sets. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. [34] Quentin Garrido, Randall Balestriero, Laurent Najman, and Yann Lecun. Rankme: Assessing the downstream performance of pretrained self-supervised representations by their rank. In International Conference on Machine Learning (ICML), 2023. [35] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations (ICLR), 2018. [36] Robert Geirhos, Kristof Meding, and Felix A Wichmann. Beyond accuracy: quantifying trial-by-trial behaviour of cnns and humans by measuring error consistency. Conference on Neural Information Processing Systems (NeurIPS), 2020. [37] Aritra Ghosh and Andrew Lan. Bobcat: Bilevel optimization-based computerized adaptive testing. International Joint Conference on Artificial Intelligence (IJCAI), 2021. [38] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. International Conference on Learning Representations (ICLR), 2019. [39] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In International Conference on Computer Vision (ICCV), 2021. [40] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. International Conference on Learning Representations (ICLR), 2021. [41] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [42] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. arXiv preprint arXiv:2306.14610, 2023. 13[43] Zhenya Huang, Qi Liu, Chengxiang Zhai, Yu Yin, Enhong Chen, Weibo Gao, and Guoping Hu. Exploring multi-objective exercise recommendations in online education systems. In International Conference on Information and Knowledge Management (CIKM), 2019. [44] Ben Hutchinson, Negar Rostamzadeh, Christina Greer, Katherine Heller, and Vinodkumar Prabhakaran. Evaluation gaps in machine learning practice. In Conference on Fairness, Accountability, and Trans- parency (FAccT), 2022. [45] Régis Pierrard Ilyas Moutawwakil. Llm-perf leaderboard. https://huggingface.co/spaces/ optimum/llm-perf-leaderboard, 2023. [46] Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Gold- blum, Jonas Geiping, and Tom Goldstein. Bring your own data! self-supervised evaluation for large language models. arXiv preprint arXiv:2306.13651, 2023. [47] Disi Ji, Robert L Logan, Padhraic Smyth, and Mark Steyvers. Active bayesian assessment of black-box classifiers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 7935–7944, 2021. [48] Amita Kamath, Jack Hessel, and Kai-Wei Chang. Text encoders are performance bottlenecks in contrastive vision-language models. arXiv preprint arXiv:2305.14897, 2023. [49] Gal Kaplun, Nikhil Ghosh, Saurabh Garg, Boaz Barak, and Preetum Nakkiran. Deconstructing distribu- tions: A pointwise framework of learning. International Conference on Learning Representations (ICLR), 2023. [50] Faisal Khan, Bilge Mutlu, and Jerry Zhu. How do humans teach: On curriculum learning and teaching dimension. Advances in neural information processing systems, 24, 2011. [51] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. Dynabench: Rethinking benchmarking in nlp. North American Chapter of the Association for Computational Linguistics (NAACL), 2021. [52] Jannik Kossen, Sebastian Farquhar, Yarin Gal, and Tom Rainforth. Active testing: Sample-efficient model evaluation. In International Conference on Machine Learning (ICML), 2021. [53] Jannik Kossen, Sebastian Farquhar, Yarin Gal, and Thomas Rainforth. Active surrogate estimators: An active learning approach to label-efficient model evaluation. Conference on Neural Information Processing Systems (NeurIPS), 2022. [54] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [55] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International Journal of Computer Vision (IJCV), 128(7):1956–1981, 2020. [56] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Benita Teufel, Marco Bellagente, et al. Holistic evaluation of text-to-image models. Conference on Neural Information Processing Systems (NeurIPS), 2023. [57] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. [58] Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, and Ludwig Schmidt. Are we learning yet? a meta review of evaluation failures across machine learning. In Conference on Neural Information Processing Systems (NeurIPS), 2021. [59] Shangyun Lu, Bradley Nott, Aaron Olson, Alberto Todeschini, Hossein Vahabi, Yair Carmon, and Ludwig Schmidt. Harder or different? a closer look at distribution shift in dataset reproduction. In International Conference on Machine Learning Workshops (ICML-W), 2020. [60] Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. arXiv preprint arXiv:2203.08242, 2022. [61] Horia Mania, John Miller, Ludwig Schmidt, Moritz Hardt, and Benjamin Recht. Model similarity mitigates test set overuse. Conference on Neural Information Processing Systems (NeurIPS), 32, 2019. [62] Dena F Mujtaba and Nihar R Mahapatra. Multi-objective optimization of item selection in computerized adaptive testing. In Proceedings of the Genetic and Evolutionary Computation Conference , pages 1018–1026, 2021. [63] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. Annual Meeting of the Association for Computational Linguistics (ACL), 2020. 14[64] Simon Ott, Adriano Barbosa-Silva, Kathrin Blagec, Jan Brauner, and Matthias Samwald. Mapping global dynamics of benchmark creation and saturation in artificial intelligence. Nature Communications, 13(1): 6793, 2022. [65] Letitia Parcalabescu, Michele Cafagna, Lilitta Muradjan, Anette Frank, Iacer Calixto, and Albert Gatt. Valse: A task-independent benchmark for vision and language models centered on linguistic phenomena. arXiv preprint arXiv:2112.07566, 2021. [66] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022. [67] Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen. Efficient benchmarking (of language models). arXiv preprint arXiv:2308.11696, 2023. [68] Momchil Peychev, Mark Niklas Müller, Marc Fischer, and Martin Vechev. Automated classification of model errors on imagenet. Conference on Neural Information Processing Systems (NeurIPS), 2023. [69] Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin. tinybenchmarks: evaluating llms with fewer examples. arXiv preprint arXiv:2402.14992, 2024. [70] Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. Dynasent: A dynamic benchmark for sentiment analysis. Dynasent: A dynamic benchmark for sentiment analysis, 2021. [71] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. [72] Inioluwa Deborah Raji, Emily M Bender, Amandalynne Paullada, Emily Denton, and Alex Hanna. Ai and the everything in the whole wide world benchmark. Conference on Neural Information Processing Systems (NeurIPS), 2021. [73] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classifiers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. [74] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning (ICML), 2019. [75] Pedro Rodriguez, Joe Barrow, Alexander Miserlis Hoyle, John P Lalor, Robin Jia, and Jordan Boyd- Graber. Evaluation examples are not equally informative: How should that change nlp leaderboards? In Annual Meeting of the Association for Computational Linguistics (ACL), 2021. [76] Rebecca Roelofs, Vaishaal Shankar, Benjamin Recht, Sara Fridovich-Keil, Moritz Hardt, John Miller, and Ludwig Schmidt. A meta-analysis of overfitting in machine learning. Conference on Neural Information Processing Systems (NeurIPS), 2019. [77] Mark Rofin, Vladislav Mikhailov, Mikhail Florinskiy, Andrey Kravchenko, Elena Tutubalina, Tatiana Shavrina, Daniel Karabekyan, and Ekaterina Artemova. V ote’n’rank: Revision of benchmarking with social choice theory. Annual Meeting of the Association for Computational Linguistics (EACL), 2022. [78] Nikhil Sardana and Jonathan Frankle. Beyond chinchilla-optimal: Accounting for inference in language model scaling laws. arXiv preprint arXiv:2401.00448, 2023. [79] Zhelun Shi, Zhipin Wang, Hongxing Fan, Zhenfei Yin, Lu Sheng, Yu Qiao, and Jing Shao. Chef: A comprehensive evaluation framework for standardized assessment of multimodal large language models. arXiv preprint arXiv:2311.02692, 2023. [80] Ali Shirali and Moritz Hardt. What makes imagenet look unlike laion. arXiv preprint arXiv:2306.15769, 2023. [81] Ali Shirali, Rediet Abebe, and Moritz Hardt. A theory of dynamic benchmarks. arXiv preprint arXiv:2210.03165, 2022. [82] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. [83] Xiaoxiao Sun, Xingjian Leng, Zijian Wang, Yang Yang, Zi Huang, and Liang Zheng. Cifar-10-warehouse: Broad and more realistic testbeds in model generalization analysis. arXiv preprint arXiv:2310.04414, 2023. [84] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. Conference on Neural Informa- tion Processing Systems (NeurIPS), 2020. 15[85] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5238– 5248, 2022. [86] Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, and Phillip Isola. Learning vision from models rivals learning vision from data. arXiv preprint arXiv:2312.17742, 2023. [87] Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In Conference on Computer Vision and Pattern Recognition (CVPR), 2011. [88] Vishaal Udandarao, Max F Burg, Samuel Albanie, and Matthias Bethge. Visual data-type understanding does not emerge from scaling vision-language models. arXiv preprint arXiv:2310.08577, 2023. [89] Wim J Van der Linden and Cees AW Glas.Computerized adaptive testing: Theory and practice. Springer, 2000. [90] Kirill Vishniakov, Zhiqiang Shen, and Zhuang Liu. Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy. 2023. [91] Rajan Vivek, Kawin Ethayarajh, Diyi Yang, and Douwe Kiela. Anchor points: Benchmarking models with much fewer examples. arXiv preprint arXiv:2309.08638, 2023. [92] Eric Wallace, Adina Williams, Robin Jia, and Douwe Kiela. Analyzing dynamic adversarial training data in the limit. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 202–217, 2022. [93] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. [94] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Conference on Neural Information Processing Systems (NeurIPS), 2019. [95] Hangyu Wang, Ting Long, Liang Yin, Weinan Zhang, Wei Xia, Qichen Hong, Dingyin Xia, Ruiming Tang, and Yong Yu. Gmocat: A graph-enhanced multi-objective method for computerized adaptive testing. In Conference on Knowledge Discovery and Data Mining (KDD), 2023. [96] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. Conference on Neural Information Processing Systems (NeurIPS), 2019. [97] Zan Wang, Hanmo You, Junjie Chen, Yingyi Zhang, Xuyuan Dong, and Wenbin Zhang. Prioritizing test inputs for deep neural networks via mutation analysis. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 397–409. IEEE, 2021. [98] Ross Wightman. Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019. [99] Olivia Wiles, Isabela Albuquerque, and Sven Gowal. Discovering bugs in vision models using off-the-shelf image generation and captioning. arXiv preprint arXiv:2208.08831, 2022. [100] Jingwei Yu, Mu Zhenyu, Jiayi Lei, Li’Ang Yin, Wei Xia, Yong Yu, and Ting Long. Sacat: Student- adaptive computerized adaptive testing. In The Fifth International Conference on Distributed Artificial Intelligence, 2023. [101] Ganzhao Yuan and Bernard Ghanem. Binary optimization via mathematical programming with equilib- rium constraints. arXiv preprint arXiv:1608.04425, 2016. [102] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. [103] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? InThe Eleventh International Conference on Learning Representations, 2022. [104] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. The visual task adaptation benchmark. 2019. [105] Yi-Kai Zhang, Ting-Ji Huang, Yao-Xiang Ding, De-Chuan Zhan, and Han-Jia Ye. Model spider: Learning to rank pre-trained models efficiently. arXiv preprint arXiv:2306.03900, 2023. [106] Lin Zhao, Tianchen Zhao, Zinan Lin, Xuefei Ning, Guohao Dai, Huazhong Yang, and Yu Wang. Flasheval: Towards fast and accurate evaluation of text-to-image diffusion generative models. arXiv preprint arXiv:2403.16379, 2024. 16[107] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. [108] Wangchunshu Zhou, Yan Zeng, Shizhe Diao, and Xinsong Zhang. Vlue: A multi-task multi-dimension benchmark for evaluating vision-language pre-training. In International Conference on Machine Learning (ICML), 2022. [109] Yan Zhuang, Qi Liu, Zhenya Huang, Zhi Li, Shuanghong Shen, and Haiping Ma. Fully adaptive framework: Neural computerized adaptive testing for online education. In Conference on Artificial Intelligence (AAAI), 2022. [110] Orr Zohar, Shih-Cheng Huang, Kuan-Chieh Wang, and Serena Yeung. Lovm: Language-only vision model selection. arXiv preprint arXiv:2306.08893, 2023. 17NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: We have highlighted the main efficient evaluation claim in the title, abstract, introduction and results section. We back up our main claim with our experiments in Sec- tion 4. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have included a limitations section in Section 5.1. 3. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Yes, we provide our proofs in Appendices H and I. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We transparently include all our experimental settings required to reproduce our findings in the main paper. We also include pseudo-code for the algorithms used in Sort & Search in Listing 1. Further, we release our Sort & Search (anonymized) codebase for ensuring reproducibility, in the supplementary material. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Yes, we provide the code in the supplementary material. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes, we include all the experimental setup details in Section 4.1. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report error bars with standard error of the mean, for our main results in Section 4.2. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? 18Answer: [Yes] Justification: We mention the total number of GPU hours required for our entire model eval- uation using the standard full-evaluation vs. using our Sort & Search method, highlighting the cost savings from our method, in the main paper. 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes, to the best of our knowledge and abilities. 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our paper can be considered as foundational research and not tied to particular applications, let alone deployments. We do not immediately see any negative societal impact. A positive societal impact might be faster and cheaper evaluation available for developing benchmarks. 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We only work with existing datsets and models, and do not release any new datasets or models. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited the original datasets and code for correct credit assignment in Table 1. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code is documented and released under GPL3 license. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. 19Part I Appendix Table of Contents A Domain-Agnosticity of Lifelong Benchmarks 2 B Towards Truly Lifelong Benchmarks: A Conceptual Framework 3 C Lifelong-ImageNet and Lifelong-CIFAR10: Details 4 D Analysis: How Consistently Do Models Follow Global Ranking? 5 E Analysis: Changing the metric from MAE to a Rank Correlation 7 F Does Error Accumulate with Consecutive Additions of New Models/Data? 8 G Extended Related Work 9 H Proof of Theorem 3.1 11 I Proof for Theorem 4.1 12 J 167 Models used for Lifelong-ImageNet experiments 13 1A Domain-Agnosticity of Lifelong Benchmarks Our framework is domain-agnostic. All our framework requires is an A matrix constructed using any binary metric, with rows representing samples and columns representing evaluated models. We discuss several applications of our framework across a range of metrics: • Language Models: Our framework can be directly applied to multiple-choice question evaluations popular for benchmarking language model evaluations. The metric here is exact match or near-exact match, a binary metric that perfectly aligns with our framework requirements. • Dense Prediction Tasks or Multi-label Classification: For pixel-wise prediction tasks or multi-label classification, our framework can be adapted by flattening the predictions of each sample. In this approach, each sample contributes an array of binary values to the A matrix instead of a single value. Extending the search algorithm is straightforward: if a point is sampled, all associated values are sampled and annotated. • Tasks with Real-valued Predictions: For tasks such as regression or BLEU score evalua- tions, our framework can be used after applying a thresholding operation, which converts predictions into binary values (above or below the threshold). While this adaptation allows the framework to function, it restricts the output predictions to the binary threshold level. Followup work [3] does extend lifelong benchmarks to evaluating language models and multimodal language models and tackles the unique challenges faced in those cases. 2B Towards Truly Lifelong Benchmarks: A Conceptual Framework In the main paper, we introduced the concept of lifelong model evaluation through the idea of ever-expanding large-scale benchmarks, termed Lifelong Benchmarks. Although Lifelong-ImageNet and Lifelong-CIFAR10 are large-scale, they are not truly lifelong as they do not expand over time. These benchmarks primarily test the efficacy of our Sort & Search method due to their large size. To achieve true lifelong benchmarks, we need continuous acquisition of samples and models, allowing for continual growth (as detailed in Section 2). In Fig. 6, we illustrate how lifelong benchmarking differs from the standard benchmarking approaches currently used in machine learning research. A great example of lifelong benchmark is [3] for language models, and multimodal foundation models. New incoming samples Traditional Approach: Static Benchmarks D1: Cats-Dogs D2: Cats-Dogs-V2 D3: Cats-Dogs-Toon Proposed Approach: Lifelong Benchmarks Model Score 1 0.94 2 0.87 3 0.58 Model Score 1 0.86 2 0.75 3 0.89 Model Score 1 0.71 2 0.95 3 0.83 Lifelong Cats-Dogs time update pool Model Score 1 0.63 2 0.49 3 0.82 … … N 0.76 model evaluation sample rankingModel  1 Model  2 Models to  evaluate Model  N … Model  1 Model  2 Model  3 Models to  evaluate Model  N … time M1 “state-of-the-art”   M3 best “robust generalization” M2 best “distributional generalization” Figure 6: Static vs Lifelong Benchmarking. (Top) Static benchmarks incentivise machine learning practitioners to overfit models to specific datasets, weakening their ability to assess generalisation. (Bottom) We conceptualise Lifelong Benchmarks as an alternative paradigm—ever-expanding pools of test samples that resist overfitting while retaining computational tractability. 3C Lifelong-ImageNet and Lifelong-CIFAR10: Details In this section, we detail the creation of our two lifelong benchmarks. Considerations. We aim to establish lifelong benchmarking as a standard evaluation protocol in computer vision. To demonstrate this, we considered two popular datasets as our basis: CIFAR10 [54] and ImageNet [23]. We chose them due to (1) their widespread adoption in prior art, (2) the diverse set of models trained on them, and (3) the presence of numerous dataset variants with the same set of labels, encompassing distribution shifts [8], temporal variations [80], and adversarial samples [41]. Note that while our current lifelong benchmarks are based on two datasets, our framework can generally be applied to any broader range of datasets. We describe the precise construction of our datasets below. See Table 1 for key statistics and a detailed breakdown. Lifelong-CIFAR10. We combine 31 domains of different CIFAR10-like datasets comprising samples applied with various synthetic distribution shifts, synthetic samples generated by diffusion models, and samples queried from different search engines using different colors and domains. We deduplicate our dataset to ensure uniqueness and downsample all images to the standard CIFAR10 resolution of 32 × 32. Our final dataset consists of 1.69 million samples. Lifelong-ImageNet. We source our test samples from ImageNet and its corresponding variants. Similar to Lifelong-CIFAR10, our benchmark is designed for increased sample diversity (43 unique domains) while operating on the same ImageNet class set. We include samples sourced from different web-engines and generated using diffusion models. Our final Lifelong-ImageNet contains 1.98 million samples. Table 1: Overview of our Lifelong Benchmarks. We list the constituent source datasets (dedupli- cated) and their statistics for constructing our lifelong benchmarks here. Our benchmarks encompass a wide-range of natural and synthetic domains, sources and distribution shifts, making for a compre- hensive lifelong testbed. Dataset #Test Samples#Domains#Unique SourcesSynthetic/NaturalCorrupted/Clean License Lifelong-CIFAR10 1,697,682 31 9 Both BothCIFAR10.1 [73] 2,000 1 1 Natural Clean MIT LicenseCIFAR10 [54] 10,000 1 1 Natural Clean UnknownCIFAR10.2 [59] 12,000 1 1 Natural Clean UnknownCINIC10 [21] 210,000 1 1 Natural Clean MIT LicenseCIFAR10-W [83] 513,682 11 8 Both Clean MIT LicenseCIFAR10-C [40] 950,000 19 1 Natural Corrupted Apache-2.0 License Lifelong-ImageNet 1,986,310 43 9 Both BothImageNet-A [41] 7,500 1 3 Natural Clean MIT LicenseObjectNet [8] 18,514 1 1 Natural Clean Custom LicenseOpenImagesNet [55]23,104 1 1 Natural Clean MIT LicenseImageNet-V2 [74] 30,000 1 1 Natural Clean MIT LicenseImageNet-R [39] 30,000 13 1 Natural Clean MIT LicenseImageNet [23] 50,000 1 1 Natural Clean Custom Non-CommercialGreyscale-ImageNet [84]50,000 1 1 Natural Clean MIT LicenseStylizedImageNet [35]50,000 1 1 Synthetic Corrupted MIT LicenseImageNet-Sketch [96]50,889 1 1 Natural Clean MIT LicenseSDNet [7] 98,706 19 1 Synthetic Clean MIT LicenseLaionNet [80] 677,597 1 1 Natural Clean UnknownImageNet-C [38] 900,000 19 1 Natural Corrupted Apache-2.0 License 4D Analysis: How Consistently Do Models Follow Global Ranking? In all our main results using Sort & Search, we use a single ranking order for all new models. A natural question arises: Are all models consistent in their agreement of what is considered a difficult sample, and what is easy? Perhaps, there could be a clique of models that all agree that certain samples are hard, whereas other models that do not—is this the case or is one ranking order truly sufficient? To justify this choice of considering a single ranking order, we run a simple experiment. We compute the cross-correlation matrix between each of the 167 models with each other on the predictions across the entire Lifelong-Imagenet benchmark (1,986,310 test samples) where models are sorted in descending order of accuracy i.e. the highest accuracy model is plotted in the first row/column and the least accurate model is plotted last. Note that the 167 models are extremely distinct in architecture, backbone, training datasets, data augmentation, normalization, and loss functions (see full list in Appendix J). The cross-correlation matrix plot is depicted in Fig. 7(b). Reading the plot. The colorbar is important here, it ranges from 0 to 1—we implicitly only look at positively correlated models. We verified that all the correlation values were positive by plotting the distribution of correlation values in Fig. 7(a)—hence, there are no models that are totally anti-correlated with each other. Now, in the correlation matrix, if there exist certain “model cliques”—certain sets of models that are highly correlated with each other and anti-correlated with all others—we would observe disconnected components, systematically isolated squares. Result. From the correlation plot, we do not find any clear evidence of model cliques. The only anomalous entries we could find are low performing models, whose predictions are uncorrelated with all other models as they are random. We observe slightly higher correlations between the top performing models, but note that this is confounded by their high accuracy—if models are highly accurate, their correlations are likely to be higher by chance alone (since there are more ones in the prediction arrays and hence higher chance of intersecting predictions). However, no distinct cliques were found. Therefore, this analysis further gives us a strong indication that model predictions are highly correlated, hence justifying our choice of using a single ranking function. Brief Discussion. While our analysis suggests that model predictions are highly correlated, we point out that this analysis is done for a varied set of models purely for the task of image classification. We do acknowledge that other tasks like retrieval or captioning might yield different correlation structures, such that there might be different model cliques emerging. Such a structure would then potentially impact our Sort algorithm. Hence, while our current results suggest that the sorted order of difficulty generalizes to new incoming models holds fairly robustly, our method might still be sensitive to task deviations, labeling errors etc. We leave a further exploration of this for future work. 5(a) Spearman Correlations are all positive (b) Spearman Correlation Matrix Figure 7: Correlation Analysis between Model Predictions onLifelong-ImageNet. (a) We note that all correlations between model predictions are positive, signifying the similarities between all models despite their diverse sizes, architectures, and inductive biases. (b) We show the cross-correlation matrix between all model predictions—the x and y axes showcase models, sorted by their accuracies. The floating point numbers on the x and y axes are the model accuracies—the highest accuracy models (70% accuracy) appear at the top and left, while the lowest accuracy models appear at the bottom and right (10% − 30%). 6E Analysis: Changing the metric from MAE to a Rank Correlation In all our main results using Sort & Search, we use the mean-absolute-error (MAE) to evaluate the effectiveness of our framework. While MAE serves as a useful proxy metric for algorithm development, it is not a necessary requirement to provide practical applications. In particular, for many use-cases, it is the ranking of the models, rather than their absolute metrics, that are of primary importance for informing downstream decisions about which model to use. Figure 8: We change the metric for evaluating the efficacy of Sort & Search from MAE to Spearman correlation—we observe consistently high correlations of 0.5 or greater. To illustrate a practical application, we examine whether Sort & Search preserves the ranking of models at high sampling efficiency. Specifically, we conducted an experiment by changing the evaluation metric from MAE to Spearman correlation between the rankings of 25, 250 models using Sort & Search and the rankings obtained after full sample evaluation on Lifelong-CIFAR10. The results, presented in Fig. 8, show a consistently high correlation of 0.5. We believe this demonstrates the framework’s applicability for practical use-cases. 7F Does Error Accumulate with Consecutive Additions of New Models/Data? In this section, we argue that the errors should not accumulate with consecutive addition of new models or data. The core intuition lies in the fact that sequential updates to P∗ t when made with the predicted vector yt+1 will necessarily preserve the same permutation, i.e. P∗ t+1 = P∗ t as yt+1 strictly follows P∗ t itself, adding an error of 0. Detailed Explanation . Considering the case where a new model is presented in which A ∈ {0, 1}|M×|D| where |M| is the number of models and |D| the number of data samples. We solve Equation 1 by alternating the solution between solving fory given the permutation P and P given the prediction y. For ease, and without loss of generality, consider the problem when solving Equation 1 repetitively for a sequence of new samples. A natural question is: Do we need to re-optimize for Pt and update A with the new ranked prediction vectors yt for every timestep? Our algorithm Sort & Search, while might not be achieving global optimality in both P and y, however, we have a guarantee that if P∗ t and y∗ t are the solutions of Sort & Search at step t, then P∗ t = P∗ t+1 at every step and we do not require recomputing P∗ t+1 optimizing [At|y∗ t ]Pt+1 after every addition where [At|Y∗ t ] is the concatenation of At with the new sample Yt+1. This is since Sort & Search only requires access to the sum over columns of [At|Y∗ t ] (see Algorithm 1 ). The core intuition underlying this result is that at the new step t + 1 the vector y∗ t+1 has a structure of ones followed by zeros ordered according to the optimal permutation P∗ t+1 that orders samples from “easiest” to “hardest” following the structure in AP∗ t . Hence, adding it to the sum preserves the ordering of elements (if ties are broken in the manner of the old ordering). Empirical Backing. We conducted experiments by adding new models serially and using the Sort & Search predictions as ground truth for further model additions on Lifelong-ImageNet dataset. The results are presented in the Appendix F. We observe the errors do not accumulate with consecutive additions, exactly the same model order is preserved – confirming our insight empirically. 8G Extended Related Work In this section, we expand on the brief literature review from Section 2 for a more expansive coverage of related topics. Comprehensive Benchmarks. Benchmarking has become ubiquitous in the machine learning world in the last few years [ 72]. It has gained further traction in the recent past with the release of foundation models like GPT-4 [ 18] and CLIP [ 71]. A popular direction taken by efforts like GLUE [93], BigBench [82], HELM [57] etc., is to have a benchmark of benchmarks, reporting the average accuracy over the constituent datasets. This approach now spans across several domains including fact-based question-answering [40], language understanding [94], zero-shot classification of vision-language models [ 30], large-scale vision model evaluation [ 104], multi-modal model evaluation [102, 108], and text-to-image generation [5, 56]. Despite these benchmarks having vast coverage of testing concepts, the obvious downsides are two-fold: (1) they are static in nature and hence can always be susceptible to test-set contamination [60], and (2) their large sizes renders them very expensive to run full model evaluations on. Adversarial Dynamic Benchmarks. One necessary aspect essential for lifelong benchmarks is collecting harder samples, which has been pursued by two strands of works. Adversarial methods to augment benchmarks [ 92, 63, 51, 70, 81] aim to automatically curate samples that all tested models reliably fail on. These methods usually involve an iterative optimisation procedure to find such adversarial samples. The second strand of work in curating adversarial samples are efforts revolving around red-teaming [31, 66] that aim to explicitly elicit certain sets of behaviours from foundation models; primarily these approaches look at the problem of adversarial benchmarking from a safety perspective. Further, a host of benchmarks that aim to stress-test models are making their way on the horizon—their primary goal is to create test sets for manually discovered failure modes [103, 65, 85, 88, 42, 48, 13, 16]. However, while they are sample efficient, they are criticized as unfair. To mitigate this, a strand of automatic error discovery [ 19, 27, 99, 68] or their human- in-the-loop variants [97, 24, 32] have been developed. This is complementary to our work, as we primarily explore model testing. Active Testing. Efforts such as [47, 52, 53, 106] aim to identify “high-quality”, representative test instances from a large amount of unlabeled data, which can reveal more model failures with less labeling effort. The key assumption underlying these works is that they assume access to a host of unlabeled data at a relatively cheap cost. However, they assume that the cost of label acquisition is a bottleneck. However, these assumptions can break down when doing multiple forward passes on a single batch of data with a large-scale foundation model is necessitated. Albeit similar in spirit to the task of actively acquiring a subset of samples for testing models, an important distinction of our method is that we want to minimise the number of forward-passes through a model—we believe that the cost of running a model on several test samples is substantial, and hence needs to be reduced for efficient evaluation in terms of time, resources and capital. Ideas for Replacing Benchmarks. Recently, there have been a surge of methods introducing creative ways of benchmarking models [58, 76, 49, 33, 75, 77, 61, 44, 17, 86, 64, 34, 76, 75] including hosted competitions [14], self-supervised evaluation [46] and newer metrics [36]. Further, recently ELO style methods have been gaining a lot of attention [12, 107] due to their scalability of deployment to millions of users in a peer-to-peer manner. The ELO algorithm is used to compute ranks for different models based on human-in-the-loop preferences. However, despite its utility ELO is heavily dependent on the choice of user inputs and can be a very biased estimator of model rankings [79]. Another interesting idea proposed by [20] is to assume access to the pre-training data of models and compute topological maps to give predictions of test error; this however requires running expensive forward passes over the training data or modifying the training protocol, which might be not be scalable to pre-trained models. Computerized Adaptive Testing. Computerized Adaptive Testing (CAT) is a framework that allows for efficient testing of human examinees. The idea is to lower the burden of students taking tests by only asking them a subset of questions from the entire pool. There have been few main directions of solutions: model-agnostic strategies for selection [11], bi-level optimization [37, 109, 29], multi- objective optimization [62, 43, 95], retrieval-augmented adaptive search [100]. One key challenge in CAT is the lack of a stable ground-truth. Since the goal in CAT is to estimate the proficiency of an examinee, and the examinee’s true ground-truth proficiency is not provided, how would one evaluate the true proficiency of an examinee? Thereby, existing CAT methods cannot explicitly optimise for 9predicting ability directly i.e. they cannot do exact ability estimation. Hence, CAT methods are not usually guaranteed to converge to the true examinee abilities under certain conditions. The biggest distinction of our work from CAT is the access to the ground-truth targets for the tasks we consider. In both Lifelong-ImageNet and Lifelong-CIFAR10, we have access to the ground-truth and hence can compute grounded metrics that can be optimised towards, unlike in CAT, where every method has to inherently be label-free. Curriculum Learning. This refers to the problem of finding a curriculum of input samples such that the optimisation objective of an algorithm becomes easier. The most intuitive explanation from curriculum learning comes from how humans learn [50]. In the context of machine learning, the idea behind curriculum learning is to find the “difficulty” of samples, where difficulty is usually defined in terms of the ease of classifying that sample correctly. Some recent works in this direction utilise estimating variance of gradients [1] and other information theoretic properties [26] to estimate sample difficulty. These approaches are complementary to our Sum component in S&S since these can be easily integrated into our framework directly. 10H Proof of Theorem 3.1 Theorem. Optimality of Y given P. For any given ai ∈ {0, 1}1×n and P, DP-Search returns an ordered prediction vector yi ∈ {0, 1}1×n which is a global minimum of ∥aiP − yi∥1, where being an ordered prediction vector implies that if yj = 1 then yj′ = 1∀j′ ≤ j. Moreover, if yj = 0, then yj′ = 0 ∀j′ ≥ j. Proof. First, we reduce the problem from Eq. (1) to the following: y′∗ = argminy′∥a′P∗ − y′∥ if y′ j = 1, then y′ j′ = 1 ∀j′ ≤ j, and if y′ j = 0, then y′ j′ = 0 ∀j′ ≥ j. (5) Note that y′ essentially constructs a vector, y′ i, of all ones up to some index i with the rest being zero . Let b = a′P∗ be the sorted vector according to the permutation matrix. Thus, the objective function has the following error: e(y′ i) =   i − iX k=1 bk ! + nX k=i+1 bk. (6) Observe that the first term is the number of zeros to the left of index i (inclusive) in b, while the second term is the number of 1s in b to the right of index i. Proposition H.1. If y′ i is a minimizer to Theorem 4.2, then, the following holds: nX k=i+1 bk ≤ (n − i) − nX k=i+1 bj. Proof. Let j < iand that y′ i and y′ j are feasible solutions for Theorem 4.2. However, let that y′ i be such that the inequality in Proposition H.1 while it is not the case for y′ j. Then, we compare the differences in the objective functions e(y′ i) and e(y′ j). We have that: e(y′ j) − e(y′ i) =     j − jX k=1 bj ! + nX k=j+1 bk   − \"  i − iX k=1 bk ! + nX k=i+1 bk # = 2 iX k=j+1 bk − (i − j). However, we know from the assumptions that 2 Pn i+1 bk ≤ n − i and that 2 Pn j+1 bk ≥ n − j. Subtracting the two inequalities we have 2 Pn k=j+1 bk ≥ i − j which implies that y′(sj) ≥ e(y′ i) which implies that y′ i is a better solution to any other y′ j not satisfying the inequality in Proposition H.1. The inequality condition in proposition H.1 implies that for the choice of index i, the number of zeros in a to the right of index i is more than the number of 1s to the right of index i. Since any solution y′ i either satisfies property in Proposition H.1 or not. Moreover, since Proposition H.1 demonstrated that the set of indices that satisfy this property are better, in objective value, than all those that do not satisfy it, then this condition achieves optimality. 11I Proof for Theorem 4.1 Theorem. Given any ground-truth vector am+1, it is possible to construct a prediction vector ym+1 such that Eagg(ym+1, am+1) = 0 and E(am+1, ym+1) = 2min(1 − |am+1|/n, |am+1|/n) Proof. Given am+1, construct a the prediction vector ym+1, such that Eagg(ym+1, am+1) = 0 and E(am+1, ym+1) = 2.min(1 − |am+1|/n, |am+1|/n) Construction: We first design construction for the prediction vector ym+1. Let us consider three cases: (i) |am+1| < 0.5, (ii) |am+1| > 0.5 and (iii) |am+1| = 0.5. Case 1 (|am+1| < 0.5): We construct the prediction vector by first flipping all the indexes with value 1 in am+1 to 0, resulting in MAE of |am+1|/n. Since, we are constrained to maintain the same |am+1|, we can flip any |am+1| other indexes with values 0 to 1. This is possible in this case as there are more 0s than 1s in am+1. This results in MAE of |am+1|/n. Taken together, they achieve the total MAE of E = 2|am+1|/n. Case 2 (|am+1| > 0.5): We construct the prediction vector by first flipping all the indexes with value 0 in am+1 to 1, resulting in an MAE of 1 − |am+1|/n. Since, we are constrained to maintain the same |am+1|, we can flip any other index 1 − |am+1| with values 1 to 0. This is possible in this case as there are more 1s than 0s in am+1. This results in an MAE of 1 − |am+1|/n. Taken together, they achieve the total MAE of E = 2.(1 − |am+1|/n). Case 3 (|am+1| = 0.5): We construct the prediction vector by flipping all the indexes with value 0 in am+1 to 1 and flipping all the indexes with value 1 in am+1 to 0. This achieves the total MAE of E = 1 = 2|am+1|/n = 2.(1 − |am+1|/n). This concludes the construction of the prediction vector ym+1. 12J 167 Models used for Lifelong-ImageNet experiments We use the following models (as named in thetimm [98] and imagenet-testbed [84] repositories): 1. BiT-M-R101x3-ILSVRC2012 2. BiT-M-R50x1-ILSVRC2012 3. BiT-M-R50x3-ILSVRC2012 4. FixPNASNet 5. FixResNet50 6. FixResNet50CutMix 7. FixResNet50CutMix_v2 8. FixResNet50_no_adaptation 9. FixResNet50_v2 10. alexnet 11. alexnet_lpf2 12. alexnet_lpf3 13. alexnet_lpf5 14. bninception 15. bninception-imagenet21k 16. cafferesnet101 17. densenet121 18. densenet121_lpf2 19. densenet121_lpf3 20. densenet121_lpf5 21. densenet161 22. densenet169 23. densenet201 24. dpn107 25. dpn131 26. dpn68 27. dpn68b 28. dpn92 29. dpn98 30. efficientnet-b0 31. efficientnet-b0-autoaug 32. efficientnet-b1 33. efficientnet-b1-advprop-autoaug 34. efficientnet-b1-autoaug 35. efficientnet-b2 36. efficientnet-b2-advprop-autoaug 37. efficientnet-b2-autoaug 38. efficientnet-b3 39. efficientnet-b3-advprop-autoaug 40. efficientnet-b3-autoaug 41. efficientnet-b4 42. efficientnet-b4-advprop-autoaug 43. efficientnet-b4-autoaug 44. efficientnet-b5 45. efficientnet-b5-advprop-autoaug 46. efficientnet-b5-autoaug 47. efficientnet-b5-randaug 48. efficientnet-b6-advprop-autoaug 49. efficientnet-b6-autoaug 50. efficientnet-b7-advprop-autoaug 51. efficientnet-b7-autoaug 52. efficientnet-b7-randaug 53. efficientnet-b8-advprop-autoaug 54. fbresnet152 55. inceptionresnetv2 56. inceptionv3 57. inceptionv4 58. instagram-resnext101_32x16d 59. instagram-resnext101_32x32d 60. instagram-resnext101_32x8d 61. mnasnet0_5 62. mnasnet1_0 63. mobilenet_v2 64. mobilenet_v2_lpf3 65. mobilenet_v2_lpf5 66. nasnetalarge 67. nasnetamobile 68. polynet 69. resnet101 70. resnet101_cutmix 71. resnet101_lpf2 72. resnet101_lpf3 73. resnet101_lpf5 74. resnet152 75. resnet18 76. resnet18-rotation-nocrop_40 77. resnet18-rotation-random_30 78. resnet18-rotation-random_40 79. resnet18-rotation-standard_40 80. resnet18-rotation-worst10_30 81. resnet18-rotation-worst10_40 82. resnet18_lpf2 83. resnet18_lpf3 84. resnet18_lpf5 85. resnet18_ssl 86. resnet18_swsl 87. resnet34 88. resnet34_lpf2 89. resnet34_lpf3 90. resnet34_lpf5 91. resnet50 92. resnet50_adv-train-free 93. resnet50_augmix 94. resnet50_aws_baseline 95. resnet50_cutmix 96. resnet50_cutout 97. resnet50_deepaugment 98. resnet50_deepaugment_augmix 99. resnet50_feature_cutmix 100. resnet50_l2_eps3_robust 101. resnet50_linf_eps4_robust 102. resnet50_linf_eps8_robust 103. resnet50_lpf2 104. resnet50_lpf3 105. resnet50_lpf5 106. resnet50_mixup 107. resnet50_ssl 108. resnet50_swsl 109. resnet50_trained_on_SIN 110. resnet50_trained_on_SIN_and_IN 111. resnet50_with_brightness_aws 112. resnet50_with_contrast_aws 113. resnet50_with_defocus_blur_aws 114. resnet50_with_fog_aws 115. resnet50_with_frost_aws 116. resnet50_with_gaussian_noise_aws 117. resnet50_with_greyscale_aws 118. resnet50_with_jpeg_compression_aws 119. resnet50_with_motion_blur_aws 120. resnet50_with_pixelate_aws 121. resnet50_with_saturate_aws 122. resnet50_with_spatter_aws 123. resnet50_with_zoom_blur_aws 124. resnext101_32x16d_ssl 125. resnext101_32x4d 126. resnext101_32x4d_ssl 127. resnext101_32x4d_swsl 128. resnext101_32x8d 129. resnext101_32x8d_ssl 130. resnext101_32x8d_swsl 131. resnext101_64x4d 132. resnext50_32x4d 133. resnext50_32x4d_ssl 134. resnext50_32x4d_swsl 135. se_resnet101 136. se_resnet152 137. se_resnet50 138. se_resnext101_32x4d 139. se_resnext50_32x4d 140. senet154 141. shufflenet_v2_x0_5 142. shufflenet_v2_x1_0 143. squeezenet1_0 144. squeezenet1_1 145. vgg11 146. vgg11_bn 147. vgg13 148. vgg13_bn 149. vgg16 150. vgg16_bn 151. vgg16_bn_lpf2 152. vgg16_bn_lpf3 153. vgg16_bn_lpf5 154. vgg16_lpf2 155. vgg16_lpf3 156. vgg16_lpf5 157. vgg19 158. vgg19_bn 159. wide_resnet101_2 160. xception 161. resnet50_trained_on_SIN_and_IN_then_finetuned_on_IN 162. resnet50_imagenet_subsample_1_of_16_batch64_original_images 163. resnet50_imagenet_subsample_1_of_2_batch64_original_images 164. resnet50_imagenet_subsample_1_of_32_batch64_original_images 165. resnet50_imagenet_subsample_1_of_8_batch64_original_images 166. resnet50_with_gaussian_noise_contrast_motion_blur_jpeg_compression_aws 167. resnet50_imagenet_100percent_batch64_original_images 1314",
      "references": [
        "Estimating example difficulty using variance of gradients",
        "The complexity of the pigeonhole principle",
        "Democratizing evaluation with infinity-benchmarks: Sample-level heterogeneous testing over arbitrary capabilities",
        "The basics of item response theory",
        "Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models",
        "Deep learning through the lens of example difficulty",
        "Leaving reality to imagination: Robust classification via generated datasets",
        "Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models",
        "On the dangers of stochastic parrots: Can language models be too big?",
        "Are we done with imagenet?",
        "Quality meets diversity: A model-agnostic framework for computerized adaptive testing",
        "Visit-bench: A benchmark for vision-language instruction following inspired by real-world use",
        "Breaking common sense: Whoops! a vision-and-language benchmark of synthetic and compositional images",
        "The ladder: A reliable leaderboard for machine learning competitions",
        "On the opportunities and risks of foundation models",
        "Pug: Photorealistic and semantically controllable synthetic data for representation learning",
        "What will it take to fix benchmarking in natural language understanding?",
        "Sparks of artificial general intelligence: Early experiments with gpt-4",
        "Hibug: On human-interpretable model debug",
        "Computing the testing error without a testing set",
        "Cinic-10 is not imagenet or cifar-10",
        "The efficiency misnomer",
        "Imagenet: A large-scale hierarchical image database",
        "The spotlight: A general method for discovering systematic errors in deep learning models",
        "Nats-bench: Benchmarking nas algorithms for architecture topology and size",
        "Understanding dataset difficulty with v-usable information",
        "Domino: Discovering systematic errors with cross-modal embeddings",
        "Does progress on imagenet transfer to real-world datasets?",
        "Balancing test accuracy and security in computerized adaptive testing",
        "Datacomp: In search of the next generation of multimodal datasets",
        "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
        "Adaptive testing of computer vision models",
        "Evaluating models’ local decision boundaries via contrast sets",
        "Rankme: Assessing the downstream performance of pretrained self-supervised representations by their rank",
        "Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness",
        "Beyond accuracy: quantifying trial-by-trial behaviour of cnns and humans by measuring error consistency",
        "Bobcat: Bilevel optimization-based computerized adaptive testing",
        "Benchmarking neural network robustness to common corruptions and perturbations",
        "The many faces of robustness: A critical analysis of out-of-distribution generalization",
        "Measuring massive multitask language understanding",
        "Natural adversarial examples",
        "Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality",
        "Exploring multi-objective exercise recommendations in online education systems",
        "Evaluation gaps in machine learning practice",
        "Llm-perf leaderboard",
        "Bring your own data! self-supervised evaluation for large language models",
        "Active bayesian assessment of black-box classifiers",
        "Text encoders are performance bottlenecks in contrastive vision-language models",
        "Deconstructing distributions: A pointwise framework of learning",
        "How do humans teach: On curriculum learning and teaching dimension",
        "Dynabench: Rethinking benchmarking in nlp",
        "Active testing: Sample-efficient model evaluation",
        "Active surrogate estimators: An active learning approach to label-efficient model evaluation",
        "Learning multiple layers of features from tiny images",
        "The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale",
        "Holistic evaluation of text-to-image models",
        "Holistic evaluation of language models",
        "Are we learning yet? a meta review of evaluation failures across machine learning",
        "Harder or different? a closer look at distribution shift in dataset reproduction",
        "Data contamination: From memorization to exploitation",
        "Model similarity mitigates test set overuse",
        "Multi-objective optimization of item selection in computerized adaptive testing",
        "Adversarial nli: A new benchmark for natural language understanding",
        "Mapping global dynamics of benchmark creation and saturation in artificial intelligence",
        "Valse: A task-independent benchmark for vision and language models centered on linguistic phenomena",
        "Red teaming language models with language models",
        "Efficient benchmarking (of language models)",
        "Automated classification of model errors on imagenet",
        "tinybenchmarks: evaluating llms with fewer examples",
        "Dynasent: A dynamic benchmark for sentiment analysis",
        "Learning transferable visual models from natural language supervision",
        "Ai and the everything in the whole wide world benchmark",
        "Do cifar-10 classifiers generalize to cifar-10?",
        "Do imagenet classifiers generalize to imagenet?",
        "Evaluation examples are not equally informative: How should that change nlp leaderboards?",
        "A meta-analysis of overfitting in machine learning",
        "Vote’n’rank: Revision of benchmarking with social choice theory",
        "Beyond chinchilla-optimal: Accounting for inference in language model scaling laws",
        "Chef: A comprehensive evaluation framework for standardized assessment of multimodal large language models",
        "What makes imagenet look unlike laion",
        "A theory of dynamic benchmarks",
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
        "Cifar-10-warehouse: Broad and more realistic testbeds in model generalization analysis",
        "Measuring robustness to natural distribution shifts in image classification",
        "Winoground: Probing vision and language models for visio-linguistic compositionality",
        "Learning vision from models rivals learning vision from data",
        "Unbiased look at dataset bias",
        "Visual data-type understanding does not emerge from scaling vision-language models",
        "Computerized adaptive testing: Theory and practice",
        "Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy",
        "Anchor points: Benchmarking models with much fewer examples",
        "Analyzing dynamic adversarial training data in the limit",
        "Glue: A multi-task benchmark and analysis platform for natural language understanding",
        "Superglue: A stickier benchmark for general-purpose language understanding systems",
        "Gmocat: A graph-enhanced multi-objective method for computerized adaptive testing",
        "Learning robust global representations by penalizing local predictive power",
        "Prioritizing test inputs for deep neural networks via mutation analysis",
        "Pytorch image models",
        "Discovering bugs in vision models using off-the-shelf image generation and captioning",
        "Sacat: Student-adaptive computerized adaptive testing",
        "Binary optimization via mathematical programming with equilibrium constraints",
        "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi",
        "When and why vision-language models behave like bags-of-words, and what to do about it?",
        "The visual task adaptation benchmark",
        "Model spider: Learning to rank pre-trained models efficiently",
        "Flasheval: Towards fast and accurate evaluation of text-to-image diffusion generative models",
        "Judging llm-as-a-judge with mt-bench and chatbot arena",
        "Vlue: A multi-task multi-dimension benchmark for evaluating vision-language pre-training",
        "Fully adaptive framework: Neural computerized adaptive testing for online education",
        "Lovm: Language-only vision model selection"
      ],
      "meta_data": {
        "arxiv_id": "2402.19472v2",
        "authors": [
          "Ameya Prabhu",
          "Vishaal Udandarao",
          "Philip Torr",
          "Matthias Bethge",
          "Adel Bibi",
          "Samuel Albanie"
        ],
        "published_date": "2024-02-29T18:58:26Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The main research problem addresses the high computational cost of evaluating a growing number of machine learning models on ever-expanding large-scale benchmarks, which aim to mitigate overfitting to static test sets. The paper introduces Sort & Search (S&S), an efficient framework that reuses previous model evaluations to significantly reduce evaluation costs. Key contributions include: 1) Curating Lifelong-CIFAR10 (1.69M samples) and Lifelong-ImageNet (1.98M samples) benchmarks. 2) Proposing the S&S framework for efficient model evaluation. 3) Demonstrating a 1000x reduction in evaluation cost (from 180 GPU days to 5 GPU hours) with low approximation error and minimal memory usage (<100MB). 4) Providing a novel decomposition of S&S errors into aleatoric and epistemic components. 5) Proving optimality of the Search sub-component and stability of the framework under repeated additions.",
        "methodology": "The Sort & Search (S&S) framework is inspired by computerized adaptive testing (CAT) and comprises two main components: Sort and Search. The **Sort** component ranks test samples by difficulty. This is achieved by finding an optimal permutation matrix P and a ranked accuracy prediction matrix Y, solved using a coordinate descent algorithm. Methods for optimizing P include 'Sorting by Sum' (greedy approach by summing correct classifications per sample), 'Sorting by Confidence Sum' (relaxing binary accuracy to confidence values for unique ordering), and 'Recursive Sorting by Sum' (iteratively optimizing tied sum values). The **Search** component efficiently selects a subset of samples for evaluation. For new models, it uniformly samples a small subset (n' << n) of test samples. A dynamic programming algorithm, DP-Search, is then used to determine an optimal row-wise threshold based on these n' observations, which is extrapolated to predict performance across all 'n' samples. To efficiently insert new samples, the framework transposes the accuracy cache (A to A^T) and applies the same Sort & Search procedure, effectively ranking models instead of samples.",
        "experimental_setup": "The framework was empirically validated on two large-scale lifelong benchmarks: **Lifelong-CIFAR10** (1.69 million 32x32 samples, compiled from 31 CIFAR10-like domains) and **Lifelong-ImageNet** (1.98 million samples, from 43 ImageNet variants). For Lifelong-CIFAR10, 31,250 pre-trained models from the NATS-Bench-Topology-search space were used. For Lifelong-ImageNet, 167 ImageNet-1K and ImageNet-21K pre-trained models were sourced. Experiments for new model evaluation (insertM) on Lifelong-CIFAR10 used 6,000 models for sorting and 25,250 for evaluation; on Lifelong-ImageNet, 50 models for sorting and 117 for evaluation. Sample difficulty estimation (insertD) was tested on Lifelong-CIFAR10 using ~1.2 million samples for sorting and ~500,000 CIFAR-10W samples for evaluation. Performance was measured using Mean-Absolute Error (MAE) between estimated and ground-truth predictions, and also Spearman correlation for ranking preservation. Various sampling budgets (from 8 to 32768 samples) and model budgets (from 8 to 2048 models) were explored. The reported computational savings were a reduction from 180 GPU days to 5 GPU hours on a single A100 GPU for Lifelong-CIFAR10, with memory cost less than 100MB.",
        "limitations": "The paper identifies several limitations and open problems. Firstly, **Ranking Imprecision** is a bottleneck, as the reliance on a single global sample ordering (P*) limits prediction performance. This suggests that a more nuanced approach, such as model clusters with their own orderings or rejection frameworks for misaligned models, could improve the framework. Secondly, the work does not focus on **Identifying Difficult Samples**, which is crucial for lifelong benchmarks. Finally, while the method is expected to scale, its primary validation is in image classification, and further testing on **Scaling up to Foundation Models** in LLM and VLM domains is needed to confirm its generalizability.",
        "future_research_directions": "Future research directions include: 1) Generalizing the ranking mechanism beyond a single global sample ordering, exploring structures like different clusters of models each with their own optimal orderings, or developing rejection frameworks for models that do not align with the learned ordering. 2) Integrating approaches for identifying and labeling challenging or adversarial examples into the lifelong benchmarking framework. 3) Extending and validating the Sort & Search method to foundation models in different domains, specifically Large Language Models (LLMs) and Vision-Language Models (VLMs). 4) Inspiring the development of more robust and efficient evaluation methods for machine learning models in general. The analysis suggests focusing on improving the 'Sort' component rather than just sampling strategies for 'Search'.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "CoT Information: Improved Sample Complexity under Chain-of-Thought Supervision",
      "full_text": "Front. Comput. Sci., 2024, 0(0): 1–36 https://doi.org/10.1007/sxxxxx-yyy-zzzz-1 REVIEW ARTICLE The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends Mengqi Chen1, Bin Guo(B)1, Hao Wang1, Haoyu Li1, Qian Zhao1, Jingqi Liu1, Yasan Ding1, Yan Pan2, Zhiwen Yu1 1 Northwestern Polytechnical University, Xi’an, China 2 National University of Defense Technology, Changsha, China © Higher Education Press 2024 Abstract Persuasion, as one of the crucial abili- ties in human communication, has garnered exten- sive attention from researchers within the field of intelligent dialogue systems. Developing dialogue agents that can persuade others to accept certain standpoints is essential to achieving truly intelli- gent and anthropomorphic dialogue systems. Ben- efiting from the substantial progress of Large Lan- guage Models (LLMs), dialogue agents have ac- quired an exceptional capability in context under- standing and response generation. However, as a typical and complicated cognitive psychological sy- stem, persuasive dialogue agents also require knowl- edge from the domain of cognitive psychology to attain a level of human-like persuasion. Conse- quently, the cognitive strategy-enhanced persuasive dialogue agent (defined as CogAgent), which in- corporates cognitive strategies to achieve persua- sive targets through conversation, has become a pre- dominant research paradigm. To depict the research trends of CogAgent, in this paper, we first present several fundamental cognitive psychology theories and give the formalized definition of three typical cognitive strategies, including the persuasion strat- egy, the topic path planning strategy, and the argu- ment structure prediction strategy. Then we pro- pose a new system architecture by incorporating the formalized definition to lay the foundation of CogAgent. Representative works are detailed and investigated according to the combined cognitive strategy, followed by the summary of authoritative benchmarks and evaluation metrics. Finally, we summarize our insights on open issues and future directions of CogAgent for upcoming researchers. Keywords Persuasive dialogue, cognitive strat- egy, cognitive psychology, persuasion strategy Received month dd, yyyy; accepted month dd, yyyy E-mail: guob@nwpu.edu.cn arXiv:2402.04631v1  [cs.CL]  7 Feb 20242 Front. Comput. Sci., 2024, 0(0): 1–36 1 Introduction Dialog agents can engage in chitchat with humans to establish certain emotional connections or help us complete tasks through long-form conversations (e.g., restaurant reservation, travel time arrange- ment). Building intelligent human-machine dia- logue agents that can conduct natural and engag- ing conversations with humans is the long-standing goal of artificial intelligence (AI) [1, 2]. Moreover, the persuasive ability of dialogue agents has gar- nered extensive attention from researchers. Persua- sion is one of the crucial abilities in human commu- nication. The Elaboration Likelihood Model (ELM) theory [3] suggests that people tend to engage with persuasive messages when communicating with oth- ers. It is a prevalent phenomenon for individu- als to hold diverse perspectives on a given topic and endeavor to influence others in altering their viewpoints, attitudes, or behaviors through conver- sational interactions [4, 5]. There are massive per- suasive scenarios in the real world, such as bargain- ing prices of goods, debating on specific topics, and arguing in online comment sections [6, 7], and active online communities, such as Debate 1) and ChangeMyView2), for people to communicate and influence other people’s opinions by posting their views [8]. A persuasive conversion includes two distinct parties, corresponding to persuader and per- suadee, respectively [9]. The goal of the persuader is to change the persuadee’s viewpoint on a spe- cific topic by combining cognitive strategies, the personality of the persuadee, and other context fea- tures [10, 11]. The development of intelligent per- suasive dialogue agents that can persuade users to accept certain standpoints is emerging as a promis- 1)https://www.debate.org/ 2)https://www.reddit.com/r/changemyview/ ing research field [12, 13]. Modern dialogue agents have arrived at the era characterized by large language models (LLMs) [14, 15]. Driven by an immense scale of parameters and an abundance of training data, dialogue agents (e.g., ChatGPT3), LLaMA [16], Claude [17], Chat- GLM4)) have acquired an exceptional capability of context understanding and response generation [18, 19], reaching a satisfactory level of fluency, logic, emotional expression and personalization when con- versing with humans [20,21]. In addition to engag- ing in casual conversations with humans, existing dialogue agents, represented by ChatGPT, can as- sist humans in accomplishing intricate tasks, such as writing codes [22, 23], writing long and coher- ent academic papers [24, 25] and aiding in o ffice works (e.g., Microsoft Copilot5)), thereby substan- tially augmenting productivity and the quality of life. However, the persuasion process is an activity that involves human psychological cognition [10, 26, 27]. The design of persuasive dialogue agents needs to incorporate cognitive strategies to orga- nize content, logic, and presentation of dialogue re- sponse reasonably from the cognitive psychology perspective. There have been considerable works of persuasive dialogue systems, which mainly en- hance persuasiveness from three aspects, namely, integrating persuasion strategies, planning topic pa- ths, and extracting argument structures. For ex- ample, Wang et al.[11] provide an in-depth anal- ysis of the impact of persuasion strategies on the persuasive power of dialogue systems in the con- text of persuasion for donation scenarios. To en- able persuasion e fficiently, Qin et al. [28] drive a 3)https://openai.com/blog/chatgpt/ 4)https://github.com/THUDM/ChatGLM3 5)https://adoption.microsoft.com/en-us/ copilot/Mengqi Chen et al. 3 Fig. 1 The example of persuasive dialogue, where the dia- logue agent persuades the user to relieve for job crisis using various persuasion strategies. conversation towards a specified persuasive target with explicit topics /keywords planning over kno- wledge graphs. Arguments are also an important source of e ffective persuasive content. Prakken et al. [29] develop a dialogue system wherein an argu- ment graph serves as the persuasive knowledge for persuasive response generation. Different from ex- isting works that focus on the persuasiveness of di- alogue systems, in this paper, we argue that persua- sion is a cognitive psychology activity and that the persuasion strategy, the topic path planning strat- egy, and the argument structure prediction strategy can all be categorized as cognitive strategies. We define cognitive strategy-enhanced persuasive dia- logue agent as CogAgent. CogAgent aims to integrate a variety of cogni- tive strategies to ensure that the generated dialogue contents can effectively influence the persuadee in terms of their perceptions, opinions, or attitudes [11, 30]. CogAgent has great potential in many scenarios, such as counseling depressed children [31], persuasion for social good [11], winning de- bates [32], and recommending items to users [33, 34]. Fig. 1 depicts a persuasive dialogue example. The dialogue agent persuades the user to reduce anxiety from job crises using various persuasion strategies. The social and communicative dynam- ics behind persuasive dialogue contexts are com- plex. E ffective and successful persuasive dialogue does not mechanically convey target viewpoints to persuadees but rather empathetically addresses per- suadees through social and emotional communi- cations [35]. Thus, persuasive dialogues are not strictly task-oriented but are carried around tasks with additional cognitive strategies to build trust and empathy with persuadees, leading to a smooth persuasive process. As an emerging research area, an in-depth sur- vey of the existing academic e fforts is necessary. Duerr et al.[36] broadly reviews the works that use natural language generation to automatically de- tect and generate persuasive texts. Zhan et al.[37] concentrates on the negotiation dialogue system, a typical type of persuasive dialogue system, and comprehensively summarizes benchmarks, evalu- ations, and methodologies of negotiation dialogue systems. Deng et al.[38] provide an overview of the prominent problems and advanced designs in proactive dialogue systems, which treats persua- sive dialogue as the subset of the proactive dia- logue. Compared with these surveys, we provide a comprehensive review of concepts, challenges, methodologies, and applications in the field of cog- nitive strategy-enhanced persuasive dialogue. We formalize the definition of cognitive strategies ex- tended from cognitive psychology theory. Based on the formalized concept model and generic sys- tem architecture, we summarize representative re- search in the field of CogAgent from a systematic perspective. Furthermore, benchmarks, evaluation metrics, and thoughts on promising research trends are analyzed to promote the research progress. To sum up, our contributions are summarized as fol- lows.4 Front. Comput. Sci., 2024, 0(0): 1–36 • Drawing from cognitive psychology theories, we formalize the definition of cognitive strat- egies, and present the concept model and gen- eric system architecture of CogAgent, to pro- vide an overall picture for the summary of research works. • We make a profound investigation of the de- velopment in CogAgent by presenting the co- re contributions of each work, according to the addressed challenges. Besides, we also comprehensively summarize available datase- ts and evaluation metrics. • We further discuss some open issues and pro- mising research trends in CogAgent, includ- ing model adaptivity/generality of CogAgent, multi-party CogAgent, multimodal CogAge- nt, etc., to promote the development of the research community. The rest of the paper is organized as follows. In Section 2, we first summarize the typical cog- nitive psychology theories and present the defini- tion of cognitive strategies. Then we formalize the concept model of CogAgent and design a generic system architecture, followed by typical applica- tion scenarios of CogAgent. In Section 3, we first introduce the challenges faced by CogAgent and then summarize the key techniques to achieve Co- gAgent based on the user cognitive strategies. In Section 4, we summarize the available datasets and evaluation metrics, followed by open issues and promising research trends in Section 5. 2 Formalized Concept Model and Sys- tem Architecture for CogAgent In this section, we first summarise the typical cog- nitive psychology theories involved in human con- versations, as the theoretical foundation for the de- sign of CogAgent. Then we formalize the concept model for CogAgent and present the generic sys- tem architecture to visualize the overall picture in CogAgent. 2.1 The Cognitive Psychology Theory As a typical cognitive-psychological activity, the persuasion process requires the support of cogni- tive psychology theories to e ffectively model the mental changes that people experience during con- versations, thus promoting the design of CogAgent. This section summarises typical cognitive psychol- ogy theories to inspire subsequent CogAgent re- searchers. 2.1.1 Pre-suasion The concept of Pre-suasion [39], proposed by the renowned authority on persuasion, Robert Cialdini, is a prominent theory in the persuasion field. Pre- suasion means that the success rate of persuasion can be significantly enhanced by attracting the at- tention of the persuadee through appropriate choic- es of words and actions before communication or requests are conducted. Pre-suasion emphasizes that the timing of persuasion is as important as per- suasive content. When we intend to persuade oth- ers to accept our points, we need to consider others’ perspectives and organize our conversational argu- ments at the appropriate time to e ffectively com- plete the persuasion process. 2.1.2 Principle of Consistency The principle of consistency suggests that people usually try to maintain consistency based on what they have expressed and the commitments they have made in the past [40]. By planning topic paths, one can think about and define one’s opinions and arguments in advance to maintain consistency andMengqi Chen et al. 5 increase persuasiveness when communicating with others. The principle of consistency plays an im- portant role in persuading others. Through consis- tency of statements, the persuadees will recognize that the points raised are consistent with their be- liefs or opinions and will e ffectively increase the effectiveness of persuasion. 2.1.3 Theory of Mind The theory of mind (ToM) [41] suggests that effec- tive questions and answers in communications are based on a shared world of experiences and refer- ents between interlocutors. To communicate effec- tively, people model both the mental states of their listeners and the e ffects of their behavior on the world, and then react to and predict the behavior of others. This ability to understand and infer human intentions is defined as a ToM. One way to imitate ToM is to observe others’ perspectives in various situations and to derive a set of rules that a ffect their perspectives and emotions. When the same or highly similar scenarios reoccur, we can make reasonable behavioral or emotional predictions ac- cordingly. Many researchers explicitly model ToM as a concrete cognitive process to ensure that dia- logue agents can access potential human psycho- logical states and cognitive processes [42–44]. 2.1.4 Rhetoric Aristotle, one of the earliest masters of the art of persuasion, proposes three basic elements of per- suasion: ethos (credibility), pathos (emotions), and logos (logic) in his work, The Philosophy of Rheto- ric [45]. These principles serve as a guide to ef- fective persuasive communication. By establish- ing credibility, appealing to emotions, and applying logical reasoning, one can effectively persuade oth- ers to accept his propositions. Aristotle’s insights in Rhetoric remain highly influential not only in the field of persuasive dialogue but also in shaping our understanding of aesthetics and related concepts. Credibility represents the identification of per- suaders, including their identity and moral char- acter, which influences the persuasiveness of the speaker. Aristotle in his Rhetoric explains in detail the three elements that a ffect credibility, namely wisdom, virtue, and goodwill. Wisdom includes elements such as breadth of knowledge, expertise, and authority. Virtue includes elements such as fairness, honesty, and dignity. By demonstrating wisdom, virtue, and goodwill, persuaders can en- hance their persuasiveness and foster the trust and reliability of persuadees. Combining essential ele- ments of credibility can greatly enhance the e ffec- tiveness of CogAgent. Emotion refers to the ex- pression of sentiments during the persuasion pro- cess, thus lowering people’s psychological defenses in accepting persuasive content. Aristotle stated that we cannot persuade others through rationality, but can achieve it with emotion. Emotional expres- sions play an important role in changing the cog- nitive decisions of others. The use of emotionally charged content and expressions can be more ef- fective in eliciting agreement and empathy from the persuadee. Logic refers to the use of inher- ent factual logic, causality, or other rational fac- tors in expressions to gain persons’ trust and per- suade them to change their perceptions. By pre- senting coherent logical arguments, supported by factual data and authoritative sources, persuaders can establish credibility, gain persuadees’ percep- tions, and change their opinions. The cognitive psychology theories, which can be used to model the dynamics of human cogni- tive psychological status, provide a solid founda- tion for CogAgent. Under the guidance of cogni-6 Front. Comput. Sci., 2024, 0(0): 1–36 Table 1 Part of definitions and examples of persuasion strategies. Strategy Definition Example Present of Facts Using factual evidence (e.g., official news reports, statistics) and a credible reasoning process to persuade others In recent months, the demand for residential properties has become extremely high. The price of residential property has risen almost twenty percent. Challenges and Inquiries Expressing disbelief or opposition to the other side’s viewpoints and providing strong rebuttal evidence to enhance persuasiveness Really? I don’t agree. This Star Wars episode was incredible! Emotional Resonance Eliciting specific emotions to influence others’ attitudes State-of-the-art special effects are the main reason for the success of previous episodes, so audiences have high expectations for this one, and I don’t think they will be disappointed Eliciting Anger If that’s the case, there’s not much point in further discussion. We might as well call the whole deal off. Eliciting Guilt Come on, you can at least try a little, besides your cigarette. Self- modeling Indicating one’s intention to act and choosing to act as a role model for the persuadee to follow That still leaves a gap of 20 dollars to be covered. Let’s meet each other halfway once more, then the gap will be closed and our business completed. Building Trust Building rapport and psychological trust through a harmonious conversation I’m glad we’ve agreed on price. We’ll go on to the other terms and conditions at our next meeting. Courtesy Tips Expressing gratitude, approval, praise, etc. to lower the other party’s psychological defensesI know exactly what you mean. Hearing that song gives me a nostalgic feeling. Compromise Expressing concessions on time to avoid being too intense in the guidance process and causing the other party to end the conversation I think it unwise for either of us to insist on his price. How about meeting each other halfway so that business can be concluded? Attachment of Views Expressing kindness and concern through active listening and to some extent seconding the other person’s point of view Better late than never. tive psychology, we can comprehensively investi- gate and model explicit cognitive factors and strate- gies that can change users’ cognitive psychological states, such as logical expressions and emotional appeals. These cognitive strategies can facilitate CogAgent to understand the psychological state of the persuadee and enhance the persuasiveness of responses from multiple perspectives to achieve mo- re efficient persuasion processes. 2.2 Cognitive Strategy To achieve efficient persuasion, it is necessary to integrate various cognitive strategies for precisely responding to the psychological changes of the per- suadee. Evolved from cognitive psychological the- ories, we categorize cognitive strategies into three aspects, persuasion strategy, topic path planning strategy, and argument structure prediction stra- tegy, detailed as follows. 2.2.1 Persuasion Strategy The persuasion strategy aims to influence or change the perceptions, opinions, attitudes, or behaviors of persuadees from a psychological standpoint, thro- ugh the use of linguistic techniques of expression, such as logical appeal, foot-in-the-door, and self- disclosure [4, 11, 46]. Based on existing research, we construct a comprehensive and e ffective set of persuasion strategies that can achieve persuasive goals, inspired by the theory of mind, the rhetoric, and other psychology theories. We formalize the definitions and examples of expressions of persua- sion strategies, as shown in Table 1 and Table 2. Numerous studies [11,47–49] have demonstrated that persuasion strategies can e ffectively enhance the persuasiveness of the dialogue content. How to reasonably select the appropriate strategies accord- ing to the dialogue context and the perusadee’s psy- chological state to generate a persuasive dialogue response is crucial to achieving high-quality Co-Mengqi Chen et al. 7 Table 2 Part of definitions and examples of persuasion strategies. Strategy Definition Example Problem De- composition Decomposing the ultimate persuasion goal into sub-issues and stepping through the persuasion process Let me get down some information about your apartment first. what is your property’s address? Social Identity Gaining psychological support from the other person by emphasizing group and identity belonging I know. I have been a subscriber for the past two years. Herd Mentality Presenting a viewpoint that is recognized or accepted by the majority of people and persuading the other side to accept it There was always a good round of applause every time she sang. Expression of Disgust Expressing a particular point of view or emotion to emphasize the persuasive content Oh, my god! I look so old. I look as if I were 40. I think it’s time for some plastic surgeries. Expression of Empathy I know, dear. I am too. But we’ve just been too busy to look for a house. Expression of Views That means the apartment has furniture in it. Logical Appeal Enhancing the credibility of persuasive content through the logical and reasoning process It certainly is. But to tell you the truth, the room is so large that I can share it with someone else, and that will decrease the total amount of the rent. Task Inquiry Asking questions related to persuasive goals That might be going overboard a bit. How about just that scarf with a bracelet? Personal Story Using narrative examples to illustrate the positive outcomes of your actions to inspire others to follow suit Yes, I’m sure I’ve done a lot of house painting in my life. If I got even a tiny drop of paint on her furniture, she would get furious. So I learned to be very picky. Refutation of Objections Directly refuting the other side’s point of view Not necessary. If we use a realtor to find a house, it will be more expensive. Greeting Greeting at the beginning of a dialogue Hi there! How are you doing today? gAgent. 2.2.2 Topic Path Planning Strategy The topic path planning strategy aims to plan the topic transition sequence during the persuasive di- alogue process, to ensure the dialogue coherence and the progress of the dialogue towards the per- suasive target. The persuasive dialogue agent shou- ld smoothly navigate between topics to reduce ir- relevant associations of the persuadee and the dif- ficulty of the persuasion process [50,51]. The topic path planning strategy is widely employed in target- guided persuasive dialogue systems [52,53]. Start- ing from the topic of interest to the persuadee, the persuasive dialogue agent needs to gradually and smoothly transfer the conversation topic to the per- suasive target to improve the persuadee’s psycho- logical acceptability and ensure the persuasive ef- fect. How to plan the reasonable topic path and generate an in-depth multi-turn persuasive conver- sation according to the corresponding topics is to be explored. 2.2.3 Argument Structure Prediction Strategy Argument structure prediction strategy is designe- d to predict persuasive and authoritative argument surrounding the discussed topic, thereby enhancing the credibility of persuasive dialogue contents and convincing the persuadee of the plausibility of the proposed claims [54–56]. Persuasive dialogue age- nts need to be equipped with a large-scale library of arguments and counter-arguments. By predicting reasonable argument structures based on specific persuasive topics, dialogue agents can incorporate coherent argumentation skills, such as citing au- thorities and providing convincing arguments and evidence, to effectively enhance the plausibility of dialogue contents and the credibility of the persua- sion process. The argument structure prediction strategy has been extensively explored in the field of debate dialogue, where debaters often consider argument structures to express viewpoints with clar-8 Front. Comput. Sci., 2024, 0(0): 1–36 ity, logical coherence, and compelling evidence [32, 57,58]. With the argumentative structure, the whole persuasive process can be progressed incrementally, and the overall organization, logical coherence, and credibility of the persuasive process can be signifi- cantly increased. How to mine the supporting argu- ment structures based on the dialogue context and reasonably integrate the argument structures into dialogue contents to enhance the credibility of per- suasive dialogue is to be investigated. 2.3 Formalized Concept Model for CogAgent Based on the definitions of cognitive strategies, we define the dialogue system that is incorporated with cognitive strategies to accomplish persuasive tasks through smooth and accessible conversations as C- ognitive Strategy-enhanced Persuasive Dialogue (- CogAgent). We introduce the formalized concept model of CogAgent as follows. Typically, given the dialogue context sequence H = {(Q1, A1), ...,(QS −1, AS −1)}with S -1 turns, wh- ere Qi and Ai are the dialogue query and response at the i-th dialogue turn, and the current dialogue query QS = (q1, ...,qm) with m words, the objec- tive of general dialogue system is to generate the dialogue response AS = (a1, ...,an) with n words. The modern dialogue systems usually follow the encoder-decoder architecture [2,59] or decoder-on- ly architecture [16,60]. For the encoder-decoder ar- chitecture, the encoder aims to transform input text sequence into vector representations using LSTM [61], Transformer [62] or other advanced neural models, as shown in Eq. 1. QS, H = Encoder(QS , H) (1) Based on the semantic vectors of dialogue con- text and input query, the decoder generates the dia- logue response word by word in an auto-regressive manner, as shown in Eq. 2, where at is the t-th words in the response. P(A|QS, H) = nY t=1 p(at|QS, H, a<t) (2) For the decoder-only architecture, all input text sequences will concatenated into a uniform sequen- ce with special tokens, and then the decoder also generates the response in a word-by-word manner. The general dialogue system can generate smooth and fluent responses based on the dialogue context. To generate persuasive dialogue content, it is es- sential to combine three kinds of cognitive strate- gies. Based on the definitions of three cognitive strate- gies, we give the formalized definition of CogA- gent. Given the dialogue context and the current query, CogAgent needs to first predict the persua- sion strategy Per, conversation topic Top , and the argument content Arg based on the current dialogue content, as follows. Per, Top , Arg = S trPre(QS, H) (3) where S trPre refers to the cognitive strategy pre- dictor. Then the dialogue decoder generates the di- alogue response word by word conditioned on ad- ditional cognitive strategies, as shown in Eq. 4. P(A|QS, H, Per, Top , Arg) = nY t=1 p(at|QS, H, Per, Top , Arg, a<t) (4) 2.4 Generic System Architecture After the concept model of CogAgent, we present the generic system architecture of CogAgent, asMengqi Chen et al. 9 Fig. 2 The generic system architecture of CogAgent. shown in Fig 2. The overall process of CogAgent starts from the semantic understanding of dialogue context and the persuasive target, powered by LLMs (e.g., ChatGPT, LLaMa, Claude, ChatGLM). The input text will be encoded into semantic embed- dings for subsequent processes. TheCognitive Stra- tegy Mining part is responsible for mining cogni- tive strategies, including persuasion strategies, topic paths over knowledge graph, and argument struc- ture of topics. The Cognitive Strategy Prediction for Dialogue Modelling part predicts appropriate cognitive strategies based on dialogue context and enhances the linguistic expression, logical struc- ture, and other persuasive aspects of responses. The persuasion strategy mining process first min- es various kinds of persuasion strategies through crowd strategy emergence based on cognitive psy- chology theories. According to the dialogue con- text, the persuasion strategies to be used in subse- quent rounds of dialogue will be predicted. The topic graph construction process constructs topic graphs or topic paths and then plans the wander- ing paths of topics for persuasion according to the dialogue context and persuasion strategies. The ar- gument mining process first constructs a complete argument structure from credible data sources and then predicts the arguments needed for persuasion based on the above cognitive strategies. Finally, the cognitive strategies-enhanced dialogue context will be fed into LLMs to generate persuasive dialogue responses, for numerous applications, such as psy- chological counseling, bargaining, and persuasion for social good.10 Front. Comput. Sci., 2024, 0(0): 1–36 2.5 Application Scenarios Persuasive dialogue system has widespread appli- cations in daily life. It is an ongoing e ffort of the academic/industry researchers to conduct persua- sive dialogue with users to achieve persuasive tar- gets, summarized as follows. Persuasion for social good. Persuasion for so- cial good is a typical persuasive dialogue scenario where people are persuaded to donate money or goods to charities for social good purposes, such as children’s aid and natural disaster relief. Many researchers have explored how to combine persua- sion strategies to promote users’ donation behavior. For example, Wanget al.[11] provide an insightful analysis of what persuasion strategies are effective for what types of personal characteristics of users. Mishra et al.[63] propose a Reinforcement Learn- ing (RL) based persuasive dialogue system with an efficient reward function consisting of five di ffer- ent sub rewards, Persuasion, Emotion, Politeness- Strategy Consistency, Dialogue-Coherence, and N- on-repetitiveness. Chen et al.[35] produce a mod- ular persuasive dialogue system that seamlessly in- tegrates factual information and persuasive content into generated dialogue response using the condi- tional language model. Persuasion for psychological counseling. The frequent occurrence of mental diseases, such as de- pression, makes mental health gradually receive ex- tensive attention from society [64–66]. Psycho- logical counseling aims at reducing people’s emo- tional distress and helping them understand and wo- rk through the challenges that they face. Relieving the psychological pressure of the persuaded through conversation holds profound significance for the pe- rsuasive dialogue system. Extensive studies have explored the possibility of using persuasive dialogue systems to provide psychological counseling. For example, Liu et al. [46] collect the Emotion Sup- port Conversation dataset (ESConv) with well-desi- gned persuasion strategy annotation to train dia- logue system to provide emotional support through dialogue interactions. Zhou et al.[67] build a com- monsense cognition graph and an emotional con- cept graph based on commonsense knowledge from COMET [68] and concept knowledge from Con- ceptNet [69]. The two kinds of knowledge are alig- ned to generate dialogue responses for emotional support. Persuasion for negotiation. Negotiation is a common real-life persuasion scenario in which two parties negotiate through ongoing conversations to persuade the other party to accept the terms or de- mands they make to maximize their interests. Ne- gotiation is a necessary means of facilitating agree- ments among people and improving the e fficiency of society. There have been several studies using persuasive dialogue systems to achieve negotiation. For instance, Joshi et al. propose DIALOGRAPH [13], a negotiation dialogue system that explicitly incorporates dependencies between sequences of strategies into graph neural networks. Nortio et al. [70] embark on an exploration of persuasive tech- niques in international negotiations, emphasizing the significance of persuasive strategies during the negotiation process. Persuasion for debate. Debate is a professional persuasive scenario in which debaters persuade the opponent and the audience to accept their view- points by planning their arguments wisely and ar- guing their points from multiple perspectives. Many researchers have explored the automatic generation of persuasive arguments from online discussions or debate competitions [57, 71, 72]. Slonimet al.[32] introduce Project Debater, an autonomous debat-Mengqi Chen et al. 11 ing system that can engage in a competitive debate with humans. Persuasion for recommendation. Engaging in dialogue-based recommendations for movies, prod- ucts, and other such aspects proves to be a highly practical application of a persuasive dialogue sys- tem. To achieve successful recommendations, it is crucial to employ a persuasion strategy to fa- cilitate rapid user comprehension and acceptance of the recommendations. For example, Gupta et al. [73] propose to decompose the recommenda- tion response generation process into first generat- ing explicit commonsense paths between the source and persuasive target followed by generating re- sponses conditioned on the generated paths. 3 Research Challenges and Key Tech- niques Due to the complexity of modeling the psycholog- ical changes in the persuasive conversation, many critical challenges in CogAgent need to be address- ed. In this section, we first detail these challenges faced by CogAgent, and then conduct a compre- hensive investigation of representative works of Co- gAgent according to the adopted cognitive strate- gies, i.e., the persuasion strategy, the topic path planning strategy, and the argument structure pre- diction strategy. 3.1 Research Challenges in CogAgent Exhaustive mining of cognitive strategies . Ps- ychology defines human cognition as the process by which a person encounters, perceives, and un- derstands things [74, 75]. The formation and evo- lution of human cognition is an extremely complex process involving knowledge, personality, emotion, and many other aspects. E ffective persuasive di- alogue changes people’s feelings and perceptions about things through persuasive strategies that con- vince people to change their opinions and behav- iors [11, 76]. Therefore, it is a great challenge to build a complete set of cognitive strategies from the perspective of cognitive psychology by mining cognitive strategies that can effectively change the way human beings perceive and understand things. Several researchers have defined some persuasion strategies based on cognitive psychology theories (e.g., logical appealand emotion appealfrom [11], self-disclosure from [46]). However, most of these strategies are task-specific and not exhaustive enough to cope with generalized persuasion scenarios. How to construct well-defined cognitive strategies from multiple perspectives needs to be explored in depth. Modeling and selecting of cognitive strategies. In persuasive dialogues, people usually dynamicall- y choose different persuasive strategies depending on different persuasive goals and the evolving con- versational contexts. Persuasive strategies contain complex semantic patterns, rather than mere names or descriptions [77, 78]. How to model the implicit associations between strategy definitions and lin- guistic expressions, and precisely select cognitive strategies according to the dialog context to facili- tate the smooth flow of the persuasive dialog pro- cess is a serious challenge. Some researches have explored how to select appropriate cognitive strate- gies based on the dialogue context [13,49]. The ap- propriate selection of cognitive strategies is a crit- ical step for CogAgent to simulate humans in per- suasive conversations and is essential for achieving high-quality persuasive conversations. Integrating cognitive strategies into models. As defined at the cognitive psychology level, cog- nitive strategies are more abstract semantic con- cepts. Data-driven neural network models (DNNs), even LLMs, remain superficial in the understand-12 Front. Comput. Sci., 2024, 0(0): 1–36 ing of cognitive strategies. How to facilitate DNNs to learn the profound semantics of cognitive strate- gies, to rationally integrate cognitive strategies into the generation of persuasive dialogues, and to im- prove the persuasiveness of CogAgent, is quite cha- llenging. Graph-based, [13], reinforcement lear- ning-based [79] and other advanced methods are investigated to integrate cognitive strategies into per- suasive dialog generation. It is promising to inte- grate cognitive strategies with the outstanding lan- guage comprehension ability of LLMs. Absence of evaluation metrics. To improve the quality of persuasive dialogue, the performance of CogAgent needs to be evaluated accurately and co- mprehensively. However, existing evaluation met- rics for dialog systems (e.g., BLEU [80], METEOR [81], ROUGE-L [82]) are usually evaluated at the level of word similarity or semantic similarity be- tween generated responses and ground truth, with- out taking into account the effectiveness of persua- sive strategies, the rationality of persuasive path planning, and the richness of argument structure. It is a challenge to develop comprehensive and rea- sonable evaluation metrics to accurately evaluate the quality of CogAgent, incorporating the charac- teristics of persuasive dialog systems. 3.2 Persuasion Strategy-based CogAgent Incorporating persuasion strategies to enhance the persuasiveness of dialog responses is an important research direction in CogAgent. By using specific persuasive strategies, CogAgent can express the pe- rsuasive content in a way that is more acceptable to the persuadees, thus accomplishing the persua- sive goals more smoothly. As abstract psychologi- cal concepts, how to select appropriate persuasion strategies according to the dialogue context and gui- de the generation of responses is an important re- search question. In this section, we make an inves- tigation of the employment of persuasion strategies in CogAgent, summarized in Table 3. 3.2.1 Strategy Classification based on Dialogue Context A straightforward approach to fusing persuasion strategies in persuasive conversations is to predict a strategy label (e.g., Present of Facts) based on the dialogue context and feed the strategy into the decoder with the dialogue context to generate the dialogue response. For example, Wang et al. [11] propose a per- suasion strategy classifier to predict 10 persuasion strategies based on the dialogue context informa- tion and sentence-level features. The authors also analyze the impact that different people’s backgrou- nds on strategy prediction, laying the groundwork for research on personalized persuasive dialogue agents. He et al. [47] decouple strategy selection and response generation in CogAgent. The dia- logue manager predicts a persuasion strategy ba- sed on the persuasion strategies in dialogue history by a sequence-to-sequence model and the response generator produces a response conditioned on the strategy and dialogue history. 3.2.2 Persuasion Strategy Planning Persuasive dialogue is usually a process that lasts multiple turns, supported by successive strategies [87, 88]. Consequently, strategy planning within a long planning horizon in CogAgent is quite im- portant, rather than predicting a specific strategy based on the dialogue history. Several studies fo- cus on long-term planning of persuasion strategies, making CogAgent more e fficient in reaching per- suasion goals.Mengqi Chen et al. 13 Table 3 Representative works of persuasion strategy-based CogAgent. Solution Work Description Strategy classifying based on dialogue context Wang et al. [11] Proposing a classifier to predict persuasion strategies in dialogue using context and sentence features. He et al. [47] Decoupling strategy selection and response generation in CogAgent for predicting strategy and generating responses based on dialogue history. Persuasion strategy planning Cheng et al. [49] Proposing lookahead heuristics to estimate future user feedback after using the specific strategy. Yu et al. [83] Using Monte Carlo Tree Search for persuasion strategy planning without model training. Graph-based strategy incorporation Joshi et al. [48] Using GNNs to model strategies, dialogue acts, and dependencies in graph structures for response generation. Zhou et al. [84] Modeling both dialogue context semantic and persuasion strategy history with finite state transducers. Knowledge-enhanced strategy modeling Jia et al. [85] Introducing a knowledge-enriched encoder and memory-enhanced strategy module for dynamic emotion and semantic pattern modeling. Chen et al. [35] Designing RAP for dynamic factual and persuasive responses based on knowledge and individual persuasion strategies. Novel integration mechanism Mishra et al. [63] Creating an RL reward function to enhance consistency in politeness strategy, persuasiveness, and emotion acknowledgment in persuasive dialogue. Tu et al. [86] Proposing a novel model MISC, which firstly infers the user’s fine-grained emotional status, and then responds skillfully using a mixture of strategies. For instance, Cheng et al. [49] firstly adopt an A* search algorithm for persuasion strategy plan- ning. When predicting the appropriate strategy in each dialogue turn, look-ahead heuristics are pro- posed to estimate future user feedback after using the specific strategy, thus considering the long-term effect of persuasion strategies. The proposed looka- head method requires abundant annotated data, af- fecting the application to broader persuasive dia- logue scenarios. To overcome this bottleneck, Yu et al. [83] prompts LLMs to perform persuasion strategy planning by simulating future dialogue in- teractions using the Monte Carlo Tree Search (MC- TS) algorithm. This method requires no model trai- ning and can therefore be adapted to any persua- sion scenario. 3.2.3 Graph-based Strategy Incorporation Graph Neural Networks (GNNs) [89–91] can com- bine the benefits of interpretability and expressiv- ity, benefiting from encoding graph-structured data through message propagation. Due to the human brain’s reasoning process to capture semantic as- sociations, graph-based methods have been widely used in various tasks [92,93]. Numerous researche- rs have embarked on exploring the potential of gra- ph-based methods for incorporating persuasion str- ategies in CogAgent. For example, Joshi et al. [48] introduce DIAL- OGRAPH, as shown in Fig 3, a persuasive dialogue system that incorporates persuasion strategies and dialogue acts using GNNs. DIALOGRAPH mod- els persuasion strategies in multi-turn dialogue con- text and their dependencies as graph structures and incorporating strategies into response generation us- ing hierarchical graph pooling-based approaches. Zhou et al. [84] propose to model both dialogue context semantic and persuasion strategy history fi- nite state transducers (FSTs). To model the per- suasion factors affecting the persuasive content of dialogues, Liu et al.[94] present persuasion-factor graph convolutional layers to encode and learn rep- resentations of the persuasion-aware interaction data.14 Front. Comput. Sci., 2024, 0(0): 1–36 Fig. 3 Overview architecture of DIALOGRAPH which models persuasion strategies as graph structure. 3.2.4 Knowledge-enhanced Strategy Modeling As concepts in cognitive psychology, persuasion strategies encompass complex semantic informa- tion and various intricate linguistic features [88, 95]. To comprehensively represent the complex semantics embedded within persuasion strategies, researches investigate combining external knowl- edge to model and mimic the the intricate patterns in strategies. For example, Jiaet al.[85] propose a knowledge- enriched dialogue context encoder to model the dy- namic emotion state and a memory-enhanced strat- egy modeling module to model the semantic pat- terns of persuasion strategies. The same-strategy responses are stored in the memory bank to pro- vide more specific guidance for the strategy- con- strained response generation. Chen et al.[35] de- sign the Response-Agenda Pushing Framework (R- AP) to dynamically produce factual responses bas- ed on knowledge facts and persuasive responses conditioned on individual persuasion strategies. 3.2.5 Novel Integration Mechanisms In addition to the above studies to model and in- tegrate persuasion strategies, researchers propose some novel integration mechanisms to improve the performance of CogAgent, summarized as follows. Combined with RL, Yanget al.[96] propose two variants of ToM-based persuasive dialog agent, wh- ere the explicit version that outputs the opponent type as an intermediate prediction, and an implicit version that models the opponent type as a latent variable. Both models are optimized using rein- forcement learning. Similarly, Mishra et al. [63] design an e fficient reward function in RL to im- prove the politeness-strategy consistency, persua- siveness, and emotional acknowledgement in per- suasive dialogue. To increase the expressed empathy and learn the gradual transition in the long response, Tu [86] in- troduce a MIxed Srategy-aware model (MISC) in- tegrating COMET, a pre-trained generative com- monsense reasoning model, for emotional persua- sive dialogue. The COMET knowledge tuples are adopted to enhance the fine-grained emotional un- derstanding of users. Then MISC formulates per-Mengqi Chen et al. 15 Table 4 Representative works of topic path planning strategy-based CogAgent. Solution Work Description Reinforcement learning based planning Xu et al. [98] Presenting KnowHRL, a three-layer Knowledge-aware hierarchical RL-based model for coherent topic path planning and multi-turn persuasive dialogue responses. Liu et al. [99] Hierarchical RL for conversation topic path planning, using high-level strategies and low-level responses. Lei et al. [100] Introducing four persuasion-related factors in the reward function to achieve persuasive goals efficiently. Graph-based planning Zhong et al. [101] Using commonsense knowledge graphs and GNN to enhance semantic relations between topic keywords, improving keyword-augmented response retrieval. Zou et al. [102] Employing a concept graph for topic planning, utilizing an Insertion Transformer for persuasive response generation based on multi-concept paths. Wang et al. [103] Introducing a Transformer-based network for target-driven topic path planning with knowledge-target mutual attention and set-search decoding. Novel planning mechanism Tang et al. [53] Combining various planning algorithms for robust and smooth topic path planning, incorporating a sampling strategy, flow generator, and global planner. Wang et al. [104] Introducing a consistency-driven dialogue planning approach that utilizes stochastic processes to model the temporal evolution of the conversation path dynamically. suasion strategy as a probability distribution over a strategy codebook to use a mixture of strategies for persuasive response generation. To investigate the potential of LLMs in persua- sive conversations, Zheng et al.[97] first construc- t a large-scale persuasive dialogue dataset in the emotional support domain, leveraging the genera- tive capabilities of LLMs. Then several advanced tuning techniques (fine-tuning, adapter-tuning, Lo- RA-tuning) are employed to to showcase the supe- riority of LLMs in persuasive dialogue generation. 3.3 Topic Path Planning Strategy-based CogAgent In persuasive dialogues, generating engaging re- sponses through effective topic path planning is crit- ical to achieving persuasive targets. Topic path plan- ning strategy is a navigation tool that enhances the coherence of the persuasion process by continu- ously leading users to discuss di fferent points and topics until reaching persuasive targets. This sec- tion deepens into the intricate details of the topic path planning strategy, summarized in Table 4. 3.3.1 Reinforcement Learning-based Planning In the context of topic paths planning strategy, Re- inforcement Learning serves as a dynamic framew- ork for guiding persuasive dialogue systems in a goal-oriented manner. The core of RL is to learn the optimal sequence of actions according to the reward function and is therefore ideally suited for planning coherent topic paths in CogAgent. For example, to achieve coherent topic path plan- ning, Xu et al.[98] introduce a three-layer Knowl- edge aware hierarchical RL-based model (KnowH- RL). The upper layer of KnowHRL plans a high- level topic sequence to track user interests toward persuasive targets. The lower layers are responsi- ble for generating multi-turn persuasive dialogue responses. similarly, Liu et al.[99] propose a hier- archical RL method, GoChat, for topic path plan- ning, as shown in Fig 4. The high-level strategies in GoChat determine sub-goals that guide the conver- sation towards the ultimate target and the low-level strategy generates the corresponding responses to achieve those sub-goals. To plan topic paths from a global perspective, Yanget al.[105] introduce the global planning met- hod integrated with a commonsense knowledge gr-16 Front. Comput. Sci., 2024, 0(0): 1–36 Fig. 4 The overall framework of GoChat with hierarchical reinforcement learning. aph (KG). The key advancement is the introduction of a global RL framework that utilizes topic path planning on KG to guide the local response gener- ation model toward persuasive targets, resulting in more coherent conversations. To achieve persua- sive goals more e ffectively, Lei et al. [100] con- sider four factors (dialogue turn, goal completion difficulty, user satisfaction estimation, and cooper- ative degree) in the reward function. The targets of achieving persuasive targets quickly and maintain- ing the engagingness of users. 3.3.2 Graph-based Planning Knowledge is essential to the cognitive reasoning processes of human beings. We humans usually perform common reason during persuasive conver- sation to enhance the logic and persuasiveness of dialog contents. Therefore, relying on common- sense knowledge graphs for topic path planning can produce more persuasive target-related topic paths for CogAgent, thus reaching persuasive targets mo- re efficiently. Initially, the semantic knowledge relations amo- ng topic keywords are captured to perform next- turn topic prediction during conversation [28,106]. Then the predicted topic keywords are used to re- trieve appropriate candidate responses for persua- sive targets. Furthermore, Zhong et al.[101] intro- duce commonsense knowledge graphs and Graph Neural Networks (GNN) to model the semantic re- lations between topic keywords and enhance the keyword-augmented response retrieval, To plan topic paths more reasonably, Zou et al. [102] introduces a concept graph based on the dia- logue data, where the vertices represent concepts and edges are concept transitions between utter- ances. The topic sequence containing multiple con- cepts is obtained by the multi-concept planning mo- dule and an Insertion Transformer generates a per- suasive response according to the planned topic pat- hs. Wang et al.[107] propose a target-driven plan- ning network (TPNet), which models the topic path planning as a sequence generation task using Trans- former, as shown in Fig 5. A knowledge-target mu- tual attention mechanism and a set-search decod- ing (SSD) strategy are developed to generate topic paths based on the dialogue context. 3.3.3 Novel Planning Mechanism In addition to the above research to plan topic paths in CogAgent, there are some novel planning mech- anisms to be explored, summarized as follows. Combining the strengths of multiple topic plan- ning algorithms, the Tang et al. [53] propose anMengqi Chen et al. 17 Fig. 5 The overview framework of TPNet. EAGLE model for topic path planning. Compris- ing a topic path sampling strategy, topic flow gener- ator, and global planner, EAGLE achieves robust- ness to unseen target topics and smooth transitions. The model demonstrates enhanced global planning ability through its integrated approach, addressing limitations in existing topic-planning conversation models. To ensure the smooth and coherent progression toward persuasive goals across different turns, Wan- g et al. [108] introduce a consistency-driven di- alogue planning approach that utilizes stochastic processes to model the temporal evolution of the conversation path dynamically. Firstly, a latent spa- ce is defined, and Brownian bridge processes are employed to capture the continuity of goal-oriented behavior, allowing for more flexible integration of user feedback into dialogue planning, and explic- itly generating conversation paths. Ultimately, these paths are employed as natural language prompts to guide the generation of persuasive dialogue. 3.4 Argument Structure Prediction Strategy-based CogAgent CogAgent entails an ongoing conversation between a dialogue agent and a user at the cognitive level, where the dialogue agent proactively steers the con- versation. As the conversation progresses, the con- tents presented by the dialogue agent to support its perspectives undergo dynamic transformations. Consequently, the reasonable selection and appli- cation of arguments and evidence play a pivotal role in the persuasiveness of the dialogue. The uti- lization of arguments and evidence is imperative in the process of persuasion. Firstly, employing argu- ments and evidence allows for the gradual decom- position and progressive reasoning of persuasive targets, thereby facilitating a logical and sequential flow of the conversation that enhances the users’ acceptance of viewpoints [109]. Secondly, the pro- vision of factual support elevates the credibility of persuasive discourse, thereby augmenting the per- suasiveness of the conversation. In this section, we provide an investigation of the crucial techniques for argument mining and argument structure pre- diction in CogAgent, as summarized in Table 5.18 Front. Comput. Sci., 2024, 0(0): 1–36 Table 5 Representative works of argument structure prediction strategy-based cogAgent. Solution Work Description Argument mining Khatib et al. [110] Classifying and structurally modeling arguments from online debate portals based on diverse vocabulary, grammar, and metric features. Hua et al. [111] Proposing an argument generation framework with retrieval modules and a sentence-level LSTM for generating viewpoints. Srivastava et al. [112] Using attention-based link prediction and Transformer encoder to model hierarchical causal relationships and discover associations in online argument structures. Niculae et al. [113] Introducing factor graph model for argument mining, concurrently learning fundamental unit types classification and argument relationship prediction. Argument structure prediction Rach et al. [57] Proposing argument search technique using supervised learning-based relation classification to retrieve arguments for debate dialogue system Sakai et al. [71] Introducing an approach to consider the human agreement and disagreement, resulting in a persuasive argument with a hierarchical argumentation structure. Prakken et al. [29] Enhancing argument modeling with a five-layer graph, serving as a knowledge base for a chatbot to identify user focal points and select rebuttal points. Li et al. [114] Using factor graphs to extract online debate features, incorporating them into an LSTM model to predict persuasive arguments. 3.4.1 Argument Mining To integrate the argument structure into CogAgent, it is first necessary to perform argument mining ac- cording to conversation topics. Researchers em- bark on mining argumentative text from dialogues for CogAgent. Debate involves the explicit use of argumenta- tive content for dialogue expression, making it an important source of argument mining. For exam- ple, Khatib et al.[110] utilize online debate portals to acquire both controversial and non-controversial text snippets related to several contentious topics. These snippets are organized in a semi-structured format. Eventually, by employing a diverse set of vocabulary, grammar, and metric feature types, the arguments are classified and structurally modeled. Hua et al. [111] propose a framework for gener- ating arguments to opposing viewpoints. The re- trieval module of this framework comprises Query Formulation, Keyphrase Extraction, and Passage Ranking and Filtering. Subsequently, a sentence- level LSTM is trained to generate a sequence of sentences. In online discussion platforms, people also use argumentative texts to enhance their expressions. For instance, Tran et al.[115] and others [104,116, 117] employ multi-task learning to unearth argu- ments and evidence at both the micro and macro levels, enhancing persuasive power in online dis- cussions. Srivastava et al.[112] employs an attenti- on-based link prediction embedding model to mod- el the hierarchical causal relationships within com- mon argument structures in online discussions. Th- ey then utilize Transformer encoder layers to dis- cover the associations and boundaries between ar- guments. Furthermore, they employ AMPERSAN- D et al. [56] and SMOTE et al. [118] to address data imbalance issues, thereby improving model accuracy. Furthermore, Niculae et al.[113] intro- duce a factor graph model for argument mining, wherein the model concurrently learns the classifi- cation of fundamental unit types and prediction of argument relationships. Furthermore, the parame- ter structures of structured SVM and RNN can en- force structural constraints (e.g., transitivity), while also representing dependencies between adjacent relationships and propositions.Mengqi Chen et al. 19 Fig. 6 The overall framework of the model for predicting which side makes more convincing arguments [114]. 3.4.2 Argument Structure Prediction Dialogue systems of persuasive tasks commonly rely on structured knowledge concerning arguments and their relationships. Numerous researchers have demonstrated that predicting argument structures and integrating them into CogAgent can signifi- cantly enhance topic consistency, content coher- ence, and persuasiveness of persuasive dialogue co- ntents [32, 71, 72]. For example, Rach et al. [57] propose an argu- ment search technique for a debate dialogue sys- tem, which utilizes supervised learning-based re- lation classification to retrieve arguments mapped to a generic tree structure for the dialogue model. Sakai et al.[71] introduce an approach to consider human agreement and disagreement, resulting in a persuasive argument with a hierarchical argumen- tation structure. The dialogue agent selects the next action based on the user’s agreement or disagree- ment and sends the chosen action to the response generation module to generate logically consistent and persuasive dialogue. For more intensive argument modeling, Prakken et al. [29] equip dialogue agents with a five-layer argument graph, consisting of 1288 nodes, with an average of three counterarguments per node. This graph serves as the knowledge base for the pro- posed chatbot, allowing it to dynamically identify and annotate the user’s focal points on the param- eters, enabling the selection of appropriate rebuttal points. Li et al. [114] utilized factor graph mod- els to extract features of argument structures from online debate platforms. These features were then incorporated into an LSTM model to predict the most persuasive arguments, as shown in Fig 6. This study proves that the consideration of argument str- ucture plays a vital role in producing persuasive di- alogue content. 4 Datasets and Evaluation Metrics for CogAgent 4.1 Datasets for CogAgent Massive data is undeniably indispensable for train- ing high-quality CogAgent. To foster advancement in this field, numerous large-scale and high-quality datasets have been released. In this section, we cat- egorize existing datasets by application scenarios, including psychological counseling, debate, price negotiation, persuasion for donation, and product recommendation, summarized as Table 6.20 Front. Comput. Sci., 2024, 0(0): 1–36 Table 6 A review of available datasets for CogAgent. Scenario Dataset Description Psychological counseling ESConv [46] The first dataset for psychological counseling, annotated with persuasive strategies. AUGESC [119] The enhanced dataset from ESConv using LLMs with a broader range of topics. PsyQA [120] A Chinese mental health support dataset featuring annotated persuasive strategies. Debate IAC [121] Argumentative dialog dataset with curated threads, posts, and annotations. Winning Arguments [7] A metadata-rich subset of r/ChangeMyView subreddit conversations includes data on the success of user utterances in persuading the poster. DebateSum [122] A dataset for the competitive formal debate with corresponding argument and extractive summaries. Price negotiation Craigslist- Bargain [47] A human-human dialogue dataset for price negotiation where the buyer and seller are encouraged to reach an agreement to get a better deal. Negotiation- Coach [123] An additional negotiation coach based on CraigslistBargain, which monitors the exchange between two annotators and provides real-time negotiation strategy. Persuasion for donation Persuasion For Good [11] A collection of online conversations where one participant (the persuader) tries to convince the other (the persuadee) to donate to a charity. EPP4G and ETP4G [63] Datasets extending Persuasion For Good by annotating it with the emotion and politeness-strategy labels. FaceAct [124] A dataset extending Persuasion For Good by adding the utterance-level annotations that change the positive and/or the negative face of the participants in a conversation Product recommen- dation TG-ReDial [33] A dataset consisting of dialogues between a seeker and a recommender. DuRecDial [125] A human-to-human Chinese dialog dataset, which contains multiple sequential dialogues for every pair of a recommendation seeker and a recommender. INSPIRED [126] A movie recommendation dataset, consisting of human-human dialogues with an annotation scheme for persuasive strategies. 4.1.1 Datasets for Psychological Counseling Psychological counseling is a typical field of per- suasive dialogue, where CogAgent reduces users’ psychological anxiety and encourages positive emo- tions through the persuasive dialogue process. Re- searchers have released several datasets for psy- chological counseling. ESConv. ESConv6) [46] is a well-designed and rich, e ffective corpora for psychological counsel- ing, consisting of 1,053 dialogue pairs and a to- tal of 31,410 sentences. Each dialogue pair in- cludes information about the initial emotional state of the seeker, the persuasive strategies employed by the supporter during each interaction, and the con- tent of the conversation. The dataset encompasses seven distinct emotional states and eight supportive strategies, with the labeling of these strategies be- ing inspired by Hill’s Helping Skills Theory [88]. 6)https://github.com/thu-coai/ Emotional-Support-Conversation AUGESC. The limitations imposed by crowd- sourcing platforms on data themes and collection methods, along with the substantial regulatory costs, have hindered the extension of downstream dia- logue models to open-domain topics. In response, Zheng et al. augment ESConv to AUGESC7) [119] using LLMs, which comprises 65,000 dialogue ses- sions and a total of 1,738,000 utterances. It sub- stantially expands the scale of ESConv and encom- passes a broader range of topics. PsyQA. PsyQA8) [120] is a Chinese mental heal- th support dataset collected from a Chinese mental health service platform, including 22,000 questions and 56,000 lengthy, well-structured answers. In line with psychological counseling theory, PsyQA annotates some of the answer texts with persuasive strategies and further conducts in-depth analyses of the lexical features and strategic patterns within 7)https://github.com/thu-coai/AugESC 8)https://github.com/thu-coai/PsyQAMengqi Chen et al. 21 counseling responses. 4.1.2 Datasets for Debate Debates are typically persuasive scenarios in which each party of the debate organizes arguments to persuade the other party to accept his or her side’s viewpoints. Existing datasets for debate are listed as follows. Internet Argument Corpus (IAC).IAC9) [121] is a scriptless argumentative dialog dataset, com- prising 390,704 posts extracted from 11,800 dis- cussions on the online debate platform 4forums. com. Within this corpus, a manually curated subset of 2,866 threads and 130,206 posts is formed, cate- gorized based on discussion topics. Extended from IAC, IAC 210) [127] is a corpus for research in po- litical debate on Internet forums, consists of three data sets: 4forums (414K posts), ConvinceMe (65K posts), and a sample from CreateDebate (3K posts). Winning Arguments. To delve deeper into the mechanisms of changing others’ viewpoints in so- cial interactions, Tan et al. [7] introduce the Win- ing Arguments (ChangeMyView) Corpus. Wining ArgumentsCorpus is a metadata-rich subset of con- versations made in the r/ChangeMyview subreddit between 1 Jan 2013 - 7 May 2015, with informa- tion on the delta (success) of a user’s utterance in convincing the poster. There are 34911 Speakers, 293297 Utterances, and 3051 Conversations. DebateSum. DebateSum11) [122] is a dataset for the competitive formal debate, including 187,386 unique pieces of evidence with corresponding ar- gument and extractive summaries. The argument data is collected from the National Speech and De- bate Association over 7 years. 9)https://nlds.soe.ucsc.edu/iac 10)https://nlds.soe.ucsc.edu/iac2 11)https://debate.cards/ 4.1.3 Datasets for Price Negotiation Price negotiation is an everyday persuasive scenario where buyers and sellers reach their desired price through the persuasive dialog process. Datasets for price negotiation are summarized as follows. CraigslistBargain. CraigslistBargain12) [47] is a human-human dialogue dataset for price negoti- ation, which consists of 6682 dialogues, collected using Amazon Mechanical Turk (AMT) in a ne- gotiation setting where two workers were assigned the roles of buyer and seller, respectively. The buyer is additionally given a target price and both parties are encouraged to reach an agreement while each of the workers tries to get a better deal. Negotiation-Coach. Negotiation-Coach13) [123] introduce an additional negotiation coach based on CraigslistBargain, which monitors the exchange be- tween two annotators and provides real-time nego- tiation strategy recommendations to the seller for achieving better deals. 4.1.4 Datasets for Persuasion for Donation Persuasion for donation is very common in life, where the persuader persuades others to donate pro- perty or labor to charities for a public good pur- pose. Datasets for persuasion for donation are listed as follows. Persuasion for Social Good. Persuasion for So- cial Good14) [11] is a collection of online conversa- tions generated by AMT workers, where one par- ticipant (the persuader) tries to convince the other (the persuadee) to donate to a charity. This dataset 12)https://worksheets.codalab.org/worksheets/ 0x453913e76b65495d8b9730d41c7e0a0c/ 13)https://github.com/zhouyiheng11/ Negotiation-Coach 14)https://gitlab.com/ucdavisnlp/ persuasionforgood22 Front. Comput. Sci., 2024, 0(0): 1–36 contains 1017 conversations, along with demograp- hic data and responses to psychological surveys fro- m users. 300 conversations also have per-sentence human annotations of dialogue acts that pertain to the persuasion setting, and sentiment. EPP4G and ETP4G.EPP4G and ETP4G15) [63] extend Persuasion For Good by annotating it with the emotion and politeness-strategy labels. FaceAct. FaceAct16) [124] further extend Per- suasion For Good by adding the utterance-level an- notations that change the positive and /or the neg- ative face of the participants in a conversation. A face act can either raise or attack the positive face or negative face of either the speaker or the listener in the conversation. 4.1.5 Datasets for Product Recommendation Product recommendation intends to induce the rec- ommended person to accept or buy a particular pro- duct through persuasive dialogues. Datasets for product recommendation are listed as follows. TG-ReDial. TG-ReDial17) [33] consists of 10,0- 00 two-party dialogues between a seeker and a rec- ommender in the movie domain. DuRecDial. DuRecDial18) [125] is a human-to- human Chinese dialog dataset (about 10k dialogs, 156k utterances), which contains multiple sequen- tial dialogues for every pair of a recommendation seeker (user) and a recommender (bot). In each di- alogue, the recommender proactively leads a multi- type dialogue to approach recommendation targets and then makes multiple recommendations with ri- ch interaction behavior. 15)https://github.com/Mishrakshitij/PEPDS 16)https://github.com/ShoRit/face-acts 17)https://github.com/RUCAIBox/TG-ReDial 18)https://github.com/PaddlePaddle/Research/ tree/master/NLP/ACL2020-DuRecDial INSPIRED. INSPIRED19) [126] is a movie rec- ommendation dataset, consisting of 1,001 human- human dialogues with an annotation scheme for persuasive strategies based on social science the- ories. 4.2 Evaluation metrics Toward CogAgent The reasonable evaluation of the quality of CogA- gent is a challenging dilemma. Di fferent from the open-domain dialog system, the evaluation of Co- gAgent needs to be performed under di fferent per- suasion scenarios and multifaceted persuasive goal- s. This requires judging the quality of dialogue response while emphasizing the persuasive e ffects in specific persuasive contexts and assessing the adaptability and persuasiveness of the system’s cog- nitive strategies in di fferent domains. Up to now, there is no unified theory on how to effectively eval- uate CogAgent, and researchers predominantly em- ploy two kinds of evaluation methods: automatic evaluation metrics and human evaluation. We sum- marized commonly used automatic evaluation and human evaluation metrics in table 7. Notably, eval- uating CogAgent based on LLMs has also recently received significant attention. 4.2.1 Automatic Evaluation Metrics Automatic evaluation metrics evaluate the perfor- mance of CogAgent by calculating the similarity between the responses generated by CogAgent and ground truths. There are typical categories of auto- matic evaluation metrics: overlap-based methods, embedding-based methods, and learning-based tec- hniques. Overlap-based metrics. Overlap-based meth- ods measure the degree of text overlap between 19)https://github.com/sweetpeach/InspiredMengqi Chen et al. 23 Table 7 Evaluation metrics for CogAgent. Evaluation Method Category Description Metrics Automaticevaluation Overlap- based Measuring the degree of text overlap between generated responses and golden responses BLEU [80], ROUGE [82], METEOR [128], CIDEr [129] Embedding- based Evaluating the semantic similarity of embedding vectors between generated responses and reference ones Greedy Matching [130], Embedding averaging [131], Vector Extreme [132] Learning- based Employing machine learning models to predict the quality scores of generated responses, relying not only on given references ADEM [133] Human evaluation Scoring by human annotators to evaluate the quality of the generated responses with subjective judgment Fluency, Coherence, Contextualization, Emotional expression, Diversity, Persuasiveness generated responses and golden responses, with pa- rticular emphasis on the number of the same n- grams. These methods quantify the similarity of the text, especially the local structural similarity, to measure the quality of generated responses. Clas- sical Overlap-based methods include BLEU [80], ROUGE [82], METEOR [128] and CIDEr [129]. Among these, BLEU evaluates response quality by comparing the harmonic mean of n-gram overlaps between generated responses and the golden ones. BLEU is a straightforward and intuitive metric, yet it is constrained by surface features and may ex- hibit a weak capture of semantic relevance. ROUG- E calculates the length of the longest common sub- sequences between generated and golden responses and considers the precision and recall to evaluate the quality. METEOR integrates multiple aspects of information, including precision, recall, and syn- tactic structure, providing a more comprehensive evaluation. CIDEr evaluates the semantic similar- ity between generated responses and ground truths using n-gram level cosine similarity. These metrics have been widely applied in the evaluation of open- domain dialog systems, but they focus mainly on surface features of the response and may not cap- ture semantic relevance. In addition, relying solely on n-gram overlap to measure similarity may not always accurately evaluate the quality of long texts. Embedding-based metrics. Embedding-based metrics evaluate the semantic similarity of embed- ding vectors between generated responses and ref- erence ones. These methods utilize pre-trained wo- rd embedding models (e.g., BERT [134]) to map textual responses into embedding vectors, thus cap- turing the semantic relationships between the texts more accurately. Specifically, Greedy Matching [130] computes the cosine similarity of word em- beddings between each word in generated response and golden ones. Embedding averaging [131] av- erages all words in the sentence to calculate the sentence-level similarity. Vector Extrema [132] takes the most extreme value in the embedding vec- tor to represent the response to be evaluated. In essence, embedding-based metrics emphasize the semantic quality of CogAgent more than overlap- based metrics and better capture the semantic cor- relations between generated responses and refer- ences. Learning-based metrics. Learning-based met- rics employ machine learning models to predict the quality scores of generated responses, relying not only on given references but aiming to better cor- relate with human judgment. ADEM [133] is a deep model-based evaluation metric for dialogue systems. A hierarchical RNN model is trained in a semi-supervised manner to capture semantic infor- mation and contextual associations and align with the human preferences for dialogue responses.24 Front. Comput. Sci., 2024, 0(0): 1–36 In summary, automatic evaluation metrics o ffer advantages in terms of e fficiency and consistency. However, they face challenges in terms of semantic understanding, manual annotation costs, and model complexity. When selecting and applying automat- ed evaluation metrics, it is important to balance their advantages and disadvantages according to sp- ecific persuasive tasks and scenarios. 4.2.2 Human Evaluation Human evaluation involves subjective judgment an- d scoring by human annotators to evaluate the qual- ity of the generated responses. The annotators are usually domain experts and crowd workers who sub- jectively evaluate the generated responses based on specified criteria and task requirements. Compared to automatic evaluation metrics, human evaluation captures the subjectivity, emotion, and use of per- suasive strategies expressed by CogAgent. There- fore, the flexibility and highly customizable nature of human evaluation becomes a reliable means to ensure that the quality of CogAgent is robustly eval- uated. The human evaluation mainly evaluates CogA- gent in the following main aspects: fluency, coher- ence, contextualization, emotional expression, di- versity, and persuasiveness. In summary, human evaluation has advantages in terms of insightful and accurate evaluation of the quality of CogAgent. Ho- wever, it also has limitations in terms of cost and efficiency, due to the requirement of human labor and time resources. Therefore, in practical applica- tions, researchers need to strike a balance between human and automatic evaluation and choose the evaluation metrics that best suit the task require- ments. 5 Open Issues and Future Trends Though researchers have made considerable efforts to address the above challenges in CogAgent, there are still open issues to be resolved. In this section, we present some open issues and future develop- ment trends for CogAgent to promote the advance- ment of the research community. 5.1 Comprehensive Modeling of Cognitive Psy- chology Theory for CogAgent Although we have summarized some of the cogni- tive psychology theories, a comprehensive investi- gation of the cognitive mechanisms of persuasive dialogues from a cognitive psychology perspective is essential for understanding users’ cognitive weak- nesses and generating engaging persuasive dialogu- es. Many researchers have demonstrated the in- dispensability of employing specific strategies to achieve persuasive effects based on different cogni- tive psychology theories. Utilizing cognitive strate- gies, CogAgent can avoid cognitive dissonance in users and e fficiently persuade them to accept spe- cific viewpoints [11, 96, 135]. Prakken et al.[136] argue that psychological dissonance occurs when individuals are confronted with multiple conflict- ing cognitions. To alleviate this dissonance, three approaches can be used: changing cognitively rel- evant factors in the environment, introducing new cognitive elements, and changing cognitive elemen- ts in behavior. CogAgent should be aware of cogni- tive dissonance to mitigate the obstacles it creates in the persuasion process. In addition, researchers utilize the dual process theory of persuasion and guide the persuasive process with the Elaboration Likelihood Model (ELM) [27], a theory that fo- cuses on cognitive and affective appeals in persua- sion. Another noteworthy aspect is modeling theMengqi Chen et al. 25 user’s cognition. Proposing agreements or mak- ing concessions promptly facilitates the perception of the user’s cognitive state, enabling CogAgent to adapt to changes in the user’s cognition on time and avoiding the failure of the persuasive process [137]. Besides using data analysis to study the mech- anisms of persuasive dialog, we can also explore this phenomenon from the perspective of the cog- nitive functions of the human brain. Advances in neuroscience have provided valuable methods for studying the cognitive mechanisms of persuasive dialogue. As Poldrack et al.state [138], the use of electroencephalography (EEG), magnetoencephal- ography (MEG), functional magnetic resonance im- aging (fMRI), and other brain-imaging tools can deepen our understanding of how the human brain produces social behavior. Arapakis et al.[139] use brainwave recordings to measure users’ interest in news articles, and the experimental results suggest that frontal asymmetry (FFA) can objectively as- sess users’ receptive preferences for content. Ex- ploring the changes in neural signals in the brain of the persuadee during persuasive conversations to model which persuasive factors are effective in be- ing accepted by users and convincing them to adopt persuasive targets is a promising research direction. 5.2 Model Adaptivity /Generality of CogAgent Equipping CogAgent with cross-domain understan- ding and generation capabilities is a promising re- search direction. Existing CogAgent usually fo- cuses on one specific persuasion scenario, such as persuasion for social good, bargaining, and debat- ing. However, it is crucial to develop the ability of CogAgent to understand and transfer through mul- tiple domains, which enables CogAgent to dynam- ically optimize cognitive strategies based on differ- ent persuasive targets and e fficiently perform per- suasive tasks. For example, Wolfet al.[140] utilize transfer learning to jointly fine-tune multiple unsu- pervised response prediction tasks. They demon- strate the e ffectiveness of language model trans- fer learning on the PERSONA-CHAT dataset, es- pecially on the dialogue response generation task. Qian et al.[141] propose a meta-learning-based ap- proach to domain adaptive dialogue generation that learns from multiple resource-rich tasks. They uti- lize multiple resource-rich single-domain dialog dat- asets to train the dialogue system so that it can adapt to new domains with minimal training sam- ples. Therefore, improving the transferability of CogAgent across di fferent domains using transfer learning and other advanced approaches is an im- portant step towards the universal CogAgent. 5.3 Multi-party CogAgent Existing research of CogAgent has demonstrated remarkable performance in two-party conversation- al scenarios. However, in real world, multi-party conversations (MPCs) are more prevalent and re- quire CogAgent to persuade multiple participants simultaneously. Unlike existing persuasive dialog systems, multi-party dialogue scenarios require the collaboration of multiple CogAgent to e fficiently achieve persuasion targets [142–144]. Specifically, a single CogAgent is prone to be overly purpose- ful when interacting with users, which can cause the users’ resentment and resistance and hinder the realization of persuasion targets. In contrast, mul- tiple CogAgents can assume di fferent persuasive roles, cooperate, and persuade from di fferent per- spectives, thus winning users’ trust and realizing persuasion targets more effectively. Existing stud- ies have explored MPCs in open-domain dialogue systems. For instance, Ito et al. [145] construct a multi-modal and multi-party model based on GRU26 Front. Comput. Sci., 2024, 0(0): 1–36 to predict the persuasiveness of multiple members within a group during multi-party conversations, thereby providing a model paradigm for the study of multi-party dialogues. Gu et al.[146] propose a Speaker-Aware BERT (SABERT) model to select appropriate speaking targets from multiple users based on dialogue contexts. Gu et al. [147] ex- plore the problem of ”who says what to whom” in MPCs and propose a plug-and-play graphically- induced fine-tuning (GIFT) module for tuning a va- riety of PLMs for generalized multi-party conver- sation understanding. Inspired by multi-party dia- logue research, it is promising to utilize multiple CogAgents to collaborate on persuasive tasks to enhance the credibility and efficiency of the persua- sion process. Multiple CogAgents utilize persua- sive roles with complementary capabilities, strate- gies, and trust-building to enhance persuasion and effectiveness, thereby facilitating more persuasive and successful persuasion results. 5.4 Interpretability of Persuasive Process Interpretability of models can improve their credi- bility. Improving the interpretability of the persua- sion process is essential to ensure that persuasive dialogue contents produced by CogAgent are ac- cepted and adopted. In recent years, the field of Natural Language Processing (NLP) has increas- ingly focused on improving the interpretability of deep models [148, 149]. For example, Gaur et al. [150] argue that domain-specific knowledge helps to understand how deep models work. They demon- strate the utility of incorporating knowledge-infuse- d learning in knowledge graph format into complex neural networks to achieve model interpretability. Similarly, Yasunagaet al.[151] demonstrate model interpretability and structure inference by combin- ing a pre-trained language model a knowledge grap- h, and a quality assurance context into a unified graph. Currently, research on the interpretability of the persuasion process still lacks an overall frame- work. For the interpretability of the persuasion pro- cess, the e ffectiveness of the persuasion strategy can be verified from the cognitive theory, combined with the knowledge graph reasoning, and the per- suasion behavior can be analyzed interactively. 5.5 Multimodal CogAgent Multimodal perception and comprehension capa- bilities are essential for human beings in daily con- versations. By understanding the multimodal sur- roundings around them, including visual, textual, auditory, and other modal information, we humans can produce engaging dialogues to communicate messages, emotions, and attitudes with others [152, 153]. Despite the outstanding natural language un- derstanding and generation capabilities, perceiving and understanding multimodal context information is essential for natural and harmonious human-mac- hine conversation systems [154, 155]. To persuade people to change their thoughts, opinions, or at- titudes, it is crucial to understand the multimodal surroundings of users. Different environments may lead users to develop different attitudes towards thi- ngs. Combining multimodal contextual informa- tion, persuasive dialogue systems can comprehen- sively understand users’ mental states to generate more specific persuasive dialogue content. There has been extensive research on multimodal dialogue systems that enable the understanding of image or video content through dialogue [156–158]. For ex- ample, Murahari et al. adapt ViLBERT [159] to achieve multi-turn image-based dialogue, which un- derstands the image information through image-text pre-trained on multimodal datasets. Visual Chat- GPT [158] integrates ChatGPT with visual foun-Mengqi Chen et al. 27 dation models to achieve visual dialogue. Different kinds of visual information, such as images, depth images, and mask matrices, are converted into lan- guage formats based on visual foundation models and the prompt manager. Then ChatGPT takes the information from visual and textual modalities to generate dialogue responses. These e fforts have laid a solid foundation for multimodal persuasive dialogue systems. The integration of multimodal information to generate more persuasive conversa- tional content is a highly promising research direc- tion. 5.6 Data and Model Co-Optimization for CogA- gent The huge impact of LLMs (e.g., ChatGPT) in the field of dialog systems has sparked the enthusiasm of researchers and has been widely used in many domains [160–162]. For example, Lianget al.[161] rewrite the policy code for controlling a robot us- ing LLMs. The policy code can receive and un- derstand commands and then outputs the execution code to the API to achieve coherent control of the robot’s actions through the classical chain logic. Similarly, Wen et al.[162] combine the common- sense knowledge implicit in LLMs with the domai- n-specific knowledge of mobile applications to re- alize hands-free speech-based interaction between users and smartphones. LLM can be surprisingly useful in a variety of domains. To develop a high- quality CogAgent, we can utilize LLMs to gener- ate large-scale persuasive dialogue data to quickly validate the algorithm at an early stage. Since the capability of LLMs stems from massive amounts of data, retraining this data is hugely expensive. Therefore, the persuasion process also needs to be modeled to e fficiently and accurately perform the persuasion task. The combination of data-driven LLMs and model-driven persuasion process is the most efficient way to develop intelligent CogAgent. Future research directions for combining LLMs and model-driven persuasion processes include issues such as when to employ the generation abilities of LLMs, when model constraints are needed, and the rules and timing of collaboration between LLMs and the persuasion process. 5.7 Construction of standardized datasets and be- nchmarks Despite the significant progress researchers have made in CogAgent, datasets, and benchmarks for the study of CogAgent are still scarce. The rel- atively small size of many existing datasets (e.g., Persuasion for good [11]) limits the performance of the model in a wider range of applications. The limited amount of data hinders the ability to cap- ture the full complexity and diversity of persuasive dialogue. Moreover, the lack of detailed annota- tions about cognitive strategies in existing datasets creates challenges for training persuasive dialogue agents. Building large-scale, high-quality datasets of persuasive dialogues with rich cognitive strategy annotations is indispensable for the development of CogAgent. Combining the superior text genera- tion capabilities of LLMs [163, 164] is a potential way to build large-scale and high-quality datasets for CogAgent. 6 Conclusion Persuasion is an essential ability in human social communication, and people often skillfully persua- de others to accept their standpoints, views, or per- spectives for various purposes. Consequently, per- suasive dialogue systems have become an engaging research direction. In this paper, we have made a systematic survey of CogAgent. We first present28 Front. Comput. Sci., 2024, 0(0): 1–36 some representative cognitive psychology theories to guide the design of CogAgent at the principle level and formalize the necessary cognitive strate- gies for generating highly persuasive dialogue con- tents, including the persuasion strategy, the topic path planning strategy, and the argument structure prediction strategy. Based on the formalized def- inition and generic architecture of CogAgent, we comprehensively investigate representative works by categorizing cognitive strategies. The available datasets and evaluation metrics for CogAgent are also summarized. Despite significant progress, the research of CogAgent is still in the early stage and massive open issues and prospective future trends to be explored, such as model adaptivity/generality of CogAgent, multi-party CogAgent, and multimo- dal CogAgent. Acknowledgements This work was partially supported by the National Science Fund for Distinguished Young Schol- ars(62025205), and the National Natural Science Foundation of China (No. 62032020). References 1. Guo B, Wang H, Ding Y , Wu W, Hao S, Sun Y , Yu Z. Conditional text generation for harmonious human- machine interaction. ACM Transactions on Intelligent Systems and Technology (TIST), 2021, 12(2): 1–50 2. Huang M, Zhu X, Gao J. Challenges in building in- telligent open-domain dialog systems. ACM Trans- actions on Information Systems (TOIS), 2020, 38(3): 1–32 3. Petty R E, Cacioppo J T, Petty R E, Cacioppo J T. The elaboration likelihood model of persuasion. Springer, 1986 4. Fogg B J. Persuasive technology: using computers to change what we think and do. Ubiquity, 2002, 2002(December): 2 5. IJsselsteijn W, De Kort Y , Midden C, Eggen B, Van Den Hoven E. Persuasive technology for human well- being: setting the scene. In: Persuasive Technology: First International Conference on Persuasive Tech- nology for Human Well-Being, PERSUASIVE 2006, Eindhoven, The Netherlands, May 18-19, 2006. Pro- ceedings 1. 2006, 1–5 6. Fogg B J. Mass interpersonal persuasion: An early view of a new phenomenon. In: Persuasive Technol- ogy: Third International Conference, PERSUASIVE 2008, Oulu, Finland, June 4-6, 2008. Proceedings 3. 2008, 23–34 7. Tan C, Niculae V , Danescu-Niculescu-Mizil C, Lee L. Winning arguments: Interaction dynamics and persua- sion strategies in good-faith online discussions. In: Proceedings of the 25th international conference on world wide web. 2016, 613–624 8. Hidey C, Musi E, Hwang A, Muresan S, McKeown K. Analyzing the semantic types of claims and premises in an online persuasive forum. In: Proceedings of the 4th Workshop on Argument Mining. 2017, 11–21 9. Torning K, Oinas-Kukkonen H. Persuasive system de- sign: state of the art and future directions. In: Proceed- ings of the 4th international conference on persuasive technology. 2009, 1–8 10. Eagly A H, Chaiken S. Cognitive theories of persua- sion. In: Advances in experimental social psychology, volume 17, 267–359. Elsevier, 1984 11. Wang X, Shi W, Kim R, Oh Y , Yang S, Zhang J, Yu Z. Persuasion for good: Towards a personalized persua- sive dialogue system for social good. In: Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019, 5635–5649 12. Shi W, Wang X, Oh Y J, Zhang J, Sahay S, Yu Z. Ef- fects of persuasive dialogues: testing bot identities and inquiry strategies. In: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 2020, 1–13 13. Joshi R, Balachandran V , Vashishth S, Black A, Tsvetkov Y . Dialograph: Incorporating interpretable strategy-graph networks into negotiation dialogues. In: International Conference on Learning Represen- tations (ICLR). 2021 14. Min B, Ross H, Sulem E, Veyseh A P B, Nguyen T H, Sainz O, Agirre E, Heintz I, Roth D. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Sur- veys, 2021 15. Zhao W X, Zhou K, Li J, Tang T, Wang X, Hou Y , Min Y , Zhang B, Zhang J, Dong Z, others . A survey of large language models. arXiv preprint arXiv:2303.18223, 2023 16. Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y , Bashlykov N, Batra S, Bhargava P, Bhosale S, others . Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023 17. Bai Y , Kadavath S, Kundu S, Askell A, Kernion J, Jones A, Chen A, Goldie A, Mirhoseini A, McKin-Mengqi Chen et al. 29 non C, others . Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022 18. Zhou C, Li Q, Li C, Yu J, Liu Y , Wang G, Zhang K, Ji C, Yan Q, He L, others . A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. arXiv preprint arXiv:2302.09419, 2023 19. Ray P P. Chatgpt: A comprehensive review on back- ground, applications, key challenges, bias, ethics, lim- itations and future scope. Internet of Things and Cyber-Physical Systems, 2023 20. Li J, Han D, Guo Z, Qiao B, Wu G. Generating empathetic responses through emotion tracking and constraint guidance. Frontiers of Computer Science, 2024, 18(2) 21. Wang W, Feng S, Song K, Wang D, Li S. Infor- mative and diverse emotional conversation generation with variational recurrent pointer-generator. Frontiers of Computer Science, 2022, 16: 1–3 22. Vaithilingam P, Zhang T, Glassman E L. Expectation vs. experience: Evaluating the usability of code gen- eration tools powered by large language models. In: Chi conference on human factors in computing sys- tems extended abstracts. 2022, 1–7 23. Ni A, Iyer S, Radev D, Stoyanov V , Yih W t, Wang S, Lin X V . Lever: Learning to verify language-to-code generation with execution. In: International Confer- ence on Machine Learning. 2023, 26106–26128 24. Yuan A, Coenen A, Reif E, Ippolito D. Wordcraft: story writing with large language models. In: 27th In- ternational Conference on Intelligent User Interfaces. 2022, 841–852 25. Dergaa I, Chamari K, Zmijewski P, Saad H B. From human writing to artificial intelligence generated text: examining the prospects and potential threats of chat- gpt in academic writing. Biology of Sport, 2023, 40(2): 615–622 26. Bless H, Bohner G, Schwarz N, Strack F. Mood and persuasion: A cognitive response analysis. Personality and social psychology bulletin, 1990, 16(2): 331–345 27. Petty R E, Bri ˜nol P. Emotion and persuasion: Cog- nitive and meta-cognitive processes impact attitudes. Cognition and Emotion, 2015, 29(1): 1–26 28. Qin J, Ye Z, Tang J, Liang X. Dynamic knowledge routing network for target-guided open-domain con- versation. In: Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 8657–8664 29. Prakken H, others . A persuasive chatbot using a crowd-sourced argument graph and concerns. Com- putational Models of Argument, 2020, 326: 9 30. Dijkstra A. The psychology of tailoring-ingredients in computer-tailored persuasion. Social and personality psychology compass, 2008, 2(2): 765–784 31. Kolenik T, Gams M. Intelligent cognitive assistants for attitude and behavior change support in mental health: state-of-the-art technical review. Electronics, 2021, 10(11): 1250 32. Slonim N, Bilu Y , Alzate C, Bar-Haim R, Bogin B, Bonin F, Choshen L, Cohen-Karlik E, Dankin L, Edel- stein L, others . An autonomous debating system. Na- ture, 2021, 591(7850): 379–384 33. Zhou K, Zhou Y , Zhao W X, Wang X, Wen J R. To- wards topic-guided conversational recommender sys- tem. In: Proceedings of the 28th International Confer- ence on Computational Linguistics. 2020, 4128–4139 34. Kang D, Balakrishnan A, Shah P, Crook P, Boureau Y L, Weston J. Recommendation as a communica- tion game: Self-supervised bot-play for goal-oriented dialogue. In: 2019 Conference on Empirical Meth- ods in Natural Language Processing and 9th Interna- tional Joint Conference on Natural Language Process- ing, EMNLP-IJCNLP 2019. 2020, 1951–1961 35. Chen M, Shi W, Yan F, Hou R, Zhang J, Sahay S, Yu Z. Seamlessly integrating factual information and social content with persuasive dialogue. In: Proceed- ings of the 2nd Conference of the Asia-Pacific Chap- ter of the Association for Computational Linguistics and the 12th International Joint Conference on Natu- ral Language Processing. 2022, 399–413 36. Duerr S, Gloor P A. Persuasive natural lan- guage generation–a literature review. arXiv preprint arXiv:2101.05786, 2021 37. Zhan H, Wang Y , Feng T, Hua Y , Sharma S, Li Z, Qu L, Ha ffari G. Let’s negotiate! a sur- vey of negotiation dialogue systems. arXiv preprint arXiv:2212.09072, 2022 38. Deng Y , Lei W, Lam W, Chua T S. A survey on proactive dialogue systems: Problems, methods, and prospects. arXiv preprint arXiv:2305.02750, 2023 39. Cialdini R. Pre-suasion: A revolutionary way to influ- ence and persuade. Simon and Schuster, 2016 40. Bilu Y , Gera A, Hershcovich D, Sznajder B, Lahav D, Moshkowich G, Malet A, Gavron A, Slonim N. Ar- gument invention from first principles. In: Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019, 1013–1026 41. Premack D, Woodru ff G. Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1978, 1(4): 515–526 42. Wu J, Chen Z, Deng J, Sabour S, Huang M. Coke: A cognitive knowledge graph for machine theory of mind. arXiv preprint arXiv:2305.05390, 2023 43. Sap M, Le Bras R, Fried D, Choi Y . Neural theory-of-30 Front. Comput. Sci., 2024, 0(0): 1–36 mind? on the limits of social intelligence in large lms. In: Proceedings of the 2022 Conference on Empiri- cal Methods in Natural Language Processing. 2022, 3762–3780 44. Roman H R, Bisk Y , Thomason J, Celikyilmaz A, Gao J. Rmm: A recursive mental model for dialogue navi- gation. In: Findings of the Association for Computa- tional Linguistics: EMNLP 2020. 2020, 1732–1745 45. Campbell G. The philosophy of rhetoric. SIU Press, 1988 46. Liu S, Zheng C, Demasi O, Sabour S, Li Y , Yu Z, Jiang Y , Huang M. Towards emotional support dialog sys- tems. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan- guage Processing (V olume 1: Long Papers). 2021, 3469–3483 47. He H, Chen D, Balakrishnan A, Liang P. Decou- pling strategy and generation in negotiation dialogues. In: 2018 Conference on Empirical Methods in Natu- ral Language Processing, EMNLP 2018. 2018, 2333– 2343 48. Joshi R, Balachandran V , Vashishth S, Black A, Tsvetkov Y . Dialograph: Incorporating interpretable strategy-graph networks into negotiation dialogues. In: International Conference on Learning Represen- tations. 2020 49. Cheng Y , Liu W, Li W, Wang J, Zhao R, Liu B, Liang X, Zheng Y . Improving multi-turn emotional support dialogue generation with lookahead strategy planning. In: Proceedings of the 2022 Conference on Empiri- cal Methods in Natural Language Processing. 2022, 3014–3026 50. Cacioppo J T, Petty R E. E ffects of message repetition and position on cognitive response, recall, and persua- sion. Journal of personality and Social Psychology, 1979, 37(1): 97 51. Cialdini R B, Cialdini R B. Influence: The psychology of persuasion. volume 55. Collins New York, 2007 52. Ni J, Pandelea V , Young T, Zhou H, Cambria E. Hitkg: Towards goal-oriented conversations via multi- hierarchy learning. In: Proceedings of the AAAI con- ference on artificial intelligence. 2022, 11112–11120 53. Tang Z H, Yeh M Y . Eagle: Enhance target-oriented dialogs by global planning and topic flow integration. In: Proceedings of the 32nd ACM International Con- ference on Information and Knowledge Management. 2023, 2402–2411 54. Petty R E, Cacioppo J T. Communication and persua- sion: Central and peripheral routes to attitude change. Springer Science & Business Media, 2012 55. Swanson R, Ecker B, Walker M. Argument mining: Extracting arguments from online dialogue. In: Pro- ceedings of the 16th annual meeting of the special in- terest group on discourse and dialogue. 2015, 217–226 56. Chakrabarty T, Hidey C, Muresan S, Mckeown K, Hwang A. Ampersand: Argument mining for persua- sive online discussions. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Lan- guage Processing and the 9th International Joint Con- ference on Natural Language Processing (EMNLP- IJCNLP). 2019, 2933–2943 57. Rach N, Schindler C, Feustel I, Daxenberger J, Minker W, Ultes S. From argument search to argumentative dialogue: A topic-independent approach to argument acquisition for dialogue systems. In: Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue. 2021, 368–379 58. Wambsganss T, Kueng T, Soellner M, Leimeister J M. Arguetutor: An adaptive dialog-based learning system for argumentation skills. In: Proceedings of the 2021 CHI conference on human factors in computing sys- tems. 2021, 1–13 59. Ni J, Young T, Pandelea V , Xue F, Cambria E. Recent advances in deep learning based dialogue systems: A systematic survey. Artificial intelligence review, 2023, 56(4): 3055–3155 60. Bubeck S, Chandrasekaran V , Eldan R, Gehrke J, Horvitz E, Kamar E, Lee P, Lee Y T, Li Y , Lund- berg S, others . Sparks of artificial general intelli- gence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023 61. Hochreiter S, Schmidhuber J. Long short-term mem- ory. Neural computation, 1997, 9(8): 1735–1780 62. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez A N, Kaiser Ł, Polosukhin I. Attention is all you need. Advances in neural information processing systems, 2017, 30 63. Mishra K, Samad A M, Totala P, Ekbal A. Pepds: A polite and empathetic persuasive dialogue system for charity donation. In: Proceedings of the 29th In- ternational Conference on Computational Linguistics. 2022, 424–440 64. Walker E R, McGee R E, Druss B G. Mortality in mental disorders and global disease burden implica- tions: a systematic review and meta-analysis. JAMA psychiatry, 2015, 72(4): 334–341 65. Xu B, Zhuang Z. Survey on psychotherapy chatbots. Concurrency and Computation: Practice and Experi- ence, 2022, 34(7): e6170 66. Liang Y , Liu L, Ji Y , Huangfu L, Zeng D D. Identify- ing emotional causes of mental disorders from socialMengqi Chen et al. 31 media for effective intervention. Information Process- ing & Management, 2023, 60(4): 103407 67. Zhou J, Zheng C, Wang B, Zhang Z, Huang M. Case: Aligning coarse-to-fine cognition and a ffection for empathetic response generation. arXiv preprint arXiv:2208.08845, 2022 68. Bosselut A, Rashkin H, Sap M, Malaviya C, Celikyil- maz A, Choi Y . Comet: Commonsense transformers for automatic knowledge graph construction. In: Pro- ceedings of the 57th Annual Meeting of the Associa- tion for Computational Linguistics. 2019 69. Speer R, Chin J, Havasi C. Conceptnet 5.5: An open multilingual graph of general knowledge. In: Proceed- ings of the AAAI conference on artificial intelligence. 2017 70. Nortio E, Jasinskaja-Lahti I, H ¨am¨al¨ainen M, Pakkasvirta J. Fear of the russian bear? negoti- ating finnish national identity online. Nations and Nationalism, 2022, 28(3): 861–876 71. Sakai K, Higashinaka R, Yoshikawa Y , Ishiguro H, Tomita J. Hierarchical argumentation structure for persuasive argumentative dialogue generation. IE- ICE TRANSACTIONS on Information and Systems, 2020, 103(2): 424–434 72. Rach N, Minker W, Ultes S. Increasing the natural- ness of an argumentative dialogue system through ar- gument chains. In: Computational Models of Argu- ment, 331–338. IOS Press, 2020 73. Gupta P, Jhamtani H, Bigham J P. Target-guided di- alogue response generation using commonsense and data augmentation. In: Findings of the Association for Computational Linguistics: NAACL 2022. 2022, 1301–1317 74. Mondal P. A unifying perspective on perception and cognition through linguistic representations of emo- tion. Frontiers in Psychology, 2022, 13: 768170 75. Shettleworth S J. Cognition, evolution, and behavior. Oxford university press, 2009 76. Nguyen H, Mastho ff J. Designing persuasive dialogue systems: Using argumentation with care. In: Per- suasive Technology: Third International Conference, PERSUASIVE 2008, Oulu, Finland, June 4-6, 2008. Proceedings 3. 2008, 201–212 77. Orji R. Why are persuasive strategies e ffective? exploring the strengths and weaknesses of socially- oriented persuasive strategies. In: Persuasive Technol- ogy: Development and Implementation of Personal- ized Technologies to Change Attitudes and Behaviors: 12th International Conference, PERSUASIVE 2017, Amsterdam, The Netherlands, April 4–6, 2017, Pro- ceedings 12. 2017, 253–266 78. Ham J, Bokhorst R, Cuijpers R, Van Der Pol D, Cabibihan J J. Making robots persuasive: the influence of combining persuasive strategies (gazing and ges- tures) by a storytelling robot on its persuasive power. In: Social Robotics: Third International Conference, ICSR 2011, Amsterdam, The Netherlands, November 24-25, 2011. Proceedings 3. 2011, 71–83 79. Samad A M, Mishra K, Firdaus M, Ekbal A. Em- pathetic persuasion: reinforcing empathy and persua- siveness in dialogue systems. In: Findings of the Association for Computational Linguistics: NAACL 2022. 2022, 844–856 80. Papineni K, Roukos S, Ward T, Zhu W J. Bleu: a method for automatic evaluation of machine transla- tion. In: Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002, 311–318 81. Banerjee S, Lavie A. Meteor: An automatic metric for mt evaluation with improved correlation with hu- man judgments. In: Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for ma- chine translation and/or summarization. 2005, 65–72 82. Lin C Y , Hovy E. Automatic evaluation of summaries using n-gram co-occurrence statistics. In: Proceedings of the 2003 human language technology conference of the North American chapter of the association for computational linguistics. 2003, 150–157 83. Yu X, Chen M, Yu Z. Prompt-based monte-carlo tree search for goal-oriented dialogue policy planning. arXiv preprint arXiv:2305.13660, 2023 84. Zhou Y , Tsvetkov Y , Black A W, Yu Z. Augmenting non-collaborative dialog systems with explicit seman- tic and strategic dialog history. In: International Con- ference on Learning Representations. 2019 85. Jia M, Chen Q, Jing L, Fu D, Li R. Knowledge- enhanced memory model for emotional support con- versation. arXiv preprint arXiv:2310.07700, 2023 86. Tu Q, Li Y , Cui J, Wang B, Wen J R, Yan R. Misc: A mixed strategy-aware model integrating comet for emotional support conversation. In: Proceedings of the 60th Annual Meeting of the Association for Com- putational Linguistics (V olume 1: Long Papers). 2022, 308–319 87. Greene J O, Burleson B R. Handbook of communi- cation and social interaction skills. Psychology Press, 2003 88. Hill C E. Helping skills: Facilitating, exploration, in- sight, and action. American Psychological Associa- tion, 2009 89. Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks. In: International32 Front. Comput. Sci., 2024, 0(0): 1–36 Conference on Learning Representations. 2016 90. Veli ˇckovi´c P, Cucurull G, Casanova A, Romero A, Li`o P, Bengio Y . Graph attention networks. In: Interna- tional Conference on Learning Representations. 2018 91. Wu Z, Pan S, Chen F, Long G, Zhang C, Philip S Y . A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 2020, 32(1): 4–24 92. Wu L, Chen Y , Shen K, Guo X, Gao H, Li S, Pei J, Long B, others . Graph neural networks for natu- ral language processing: A survey. Foundations and Trends® in Machine Learning, 2023, 16(2): 119–328 93. Wang H, Guo B, Liu J, Ding Y , Yu Z. Towards infor- mative and diverse dialogue systems over hierarchi- cal crowd intelligence knowledge graph. ACM Trans- actions on Knowledge Discovery from Data, 2023, 17(7): 1–25 94. Liu C, Gao C, Yuan Y , Bai C, Luo L, Du X, Shi X, Luo H, Jin D, Li Y . Modeling persuasion factor of user decision for recommendation. In: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022, 3366–3376 95. Zheng C, Liu Y , Chen W, Leng Y , Huang M. Comae: A multi-factor hierarchical framework for empathetic response generation. In: Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021, 813–824 96. Yang R, Chen J, Narasimhan K. Improving dialog sys- tems for negotiation with personality modeling. In: Proceedings of the 59th Annual Meeting of the As- sociation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (V olume 1: Long Papers). 2021, 681–693 97. Zheng Z, Liao L, Deng Y , Nie L. Building emotional support chatbots in the era of llms. arXiv preprint arXiv:2308.11584, 2023 98. Xu J, Wang H, Niu Z, Wu H, Che W. Knowledge graph grounded goal planning for open-domain conversation generation. In: Proceedings of the AAAI conference on artificial intelligence. 2020, 9338–9345 99. Liu J, Pan F, Luo L. Gochat: Goal-oriented chat- bots with hierarchical reinforcement learning. In: Pro- ceedings of the 43rd International ACM SIGIR Con- ference on Research and Development in Information Retrieval. 2020, 1793–1796 100. Lei W, Zhang Y , Song F, Liang H, Mao J, Lv J, Yang Z, Chua T S. Interacting with non-cooperative user: A new paradigm for proactive dialogue policy. In: Pro- ceedings of the 45th International ACM SIGIR Con- ference on Research and Development in Information Retrieval. 2022, 212–222 101. Zhong P, Liu Y , Wang H, Miao C. Keyword-guided neural conversational model. In: Proceedings of the AAAI Conference on Artificial Intelligence. 2021, 14568–14576 102. Zou Y , Liu Z, Hu X, Zhang Q. Thinking clearly, talk- ing fast: Concept-guided non-autoregressive genera- tion for open-domain dialogue systems. In: Proceed- ings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021, 2215–2226 103. Wang J, Lin D, Li W. Dialogue planning via brownian bridge stochastic process for goal-directed proactive dialogue. arXiv preprint arXiv:2305.05290, 2023 104. Wang S, Yin Z, Zhang W, Zheng D, Li X. Two stage learning for argument pairs extraction. In: Natural Language Processing and Chinese Computing: 10th CCF International Conference, NLPCC 2021, Qing- dao, China, October 13–17, 2021, Proceedings, Part II 10. 2021, 538–547 105. Yang Z, Wang B, Zhou J, Tan Y , Zhao D, Huang K, He R, Hou Y . Topkg: Target-oriented dialog via global planning on knowledge graph. In: Proceedings of the 29th International Conference on Computational Lin- guistics. 2022, 745–755 106. Tang J, Zhao T, Xiong C, Liang X, Xing E, Hu Z. Target-guided open-domain conversation. In: Pro- ceedings of the 57th Annual Meeting of the Associa- tion for Computational Linguistics. 2019, 5624–5634 107. Wang J, Lin D, Li W. A target-driven planning ap- proach for goal-directed dialog systems. IEEE Trans- actions on Neural Networks and Learning Systems, 2023 108. Wang J, Lin D, Li W. Dialogue planning via brownian bridge stochastic process for goal-directed proactive dialogue. 2023 109. Vecchi E M, Falk N, Jundi I, Lapesa G. Towards ar- gument mining for social good: A survey. In: Pro- ceedings of the 59th Annual Meeting of the Associa- tion for Computational Linguistics and the 11th Inter- national Joint Conference on Natural Language Pro- cessing (V olume 1: Long Papers). 2021, 1338–1352 110. Al Khatib K, Wachsmuth H, Hagen M, K ¨ohler J, Stein B. Cross-domain mining of argumentative text through distant supervision. In: Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human lan- guage technologies. 2016, 1395–1404 111. Hua X, Hu Z, Wang L. Argument generation with re- trieval, planning, and realization. In: Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics. 2019, 2661–2672 112. Srivastava P, Bhatnagar P, Goel A. Argument min-Mengqi Chen et al. 33 ing using bert and self-attention based embeddings. In: 2022 4th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N). 2022, 1536–1540 113. Niculae V , Park J, Cardie C. Argument mining with structured svms and rnns. In: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers). 2017, 985–995 114. Li J, Durmus E, Cardie C. Exploring the role of argu- ment structure in online debate persuasion. In: Pro- ceedings of the 2020 Conference on Empirical Meth- ods in Natural Language Processing (EMNLP). 2020, 8905–8912 115. Tran N, Litman D. Multi-task learning in argument mining for persuasive online discussions. In: Proceed- ings of the 8th Workshop on Argument Mining. 2021, 148–153 116. Cheng L, Bing L, He R, Yu Q, Zhang Y , Si L. Iam: A comprehensive and large-scale dataset for integrated argument mining tasks. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers). 2022, 2277– 2287 117. Sun J, Zhu Q, Bao J, Wu J, Yang C, Wang R, Xu R. A hierarchical sequence labeling model for argument pair extraction. In: Natural Language Processing and Chinese Computing: 10th CCF International Confer- ence, NLPCC 2021, Qingdao, China, October 13–17, 2021, Proceedings, Part II 10. 2021, 472–483 118. Chawla N V , Bowyer K W, Hall L O, Kegelmeyer W P. Smote: synthetic minority over-sampling tech- nique. Journal of artificial intelligence research, 2002, 16: 321–357 119. Zheng C, Sabour S, Wen J, Zhang Z, Huang M. Augesc: Dialogue augmentation with large language models for emotional support conversation. In: Find- ings of the Association for Computational Linguistics: ACL 2023. 2023, 1552–1568 120. Sun H, Lin Z, Zheng C, Liu S, Huang M. Psyqa: A chinese dataset for generating long counseling text for mental health support. In: Findings of the Asso- ciation for Computational Linguistics: ACL-IJCNLP 2021. 2021, 1489–1503 121. Walker M, Tree J E F, Anand P, Abbott R, King J. A corpus for research on deliberation and debate. In: Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12). 2012, 812–817 122. Roush A, Balaji A. Debatesum: A large-scale argu- ment mining and summarization dataset. In: Proceed- ings of the 7th Workshop on Argument Mining. 2020, 1–7 123. Zhou Y , He H, Black A W, Tsvetkov Y . A dynamic strategy coach for effective negotiation. In: 20th An- nual Meeting of the Special Interest Group on Dis- course and Dialogue. 2019, 367 124. Dutt R, Joshi R, Rose C. Keeping up appearances: Computational modeling of face acts in persuasion oriented discussions. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP). 2020, 7473–7485 125. Liu Z, Wang H, Niu Z Y , Wu H, Che W, Liu T. To- wards conversational recommendation over multi-type dialogs. In: Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics. 2020, 1036–1049 126. Hayati S A, Kang D, Zhu Q, Shi W, Yu Z. In- spired: Toward sociable recommendation dialog sys- tems. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020, 8142–8152 127. Abbott R, Ecker B, Anand P, Walker M. Internet ar- gument corpus 2.0: An sql schema for dialogic social media and the corpora to go with it. In: Proceedings of the Tenth International Conference on Language Re- sources and Evaluation (LREC’16). 2016, 4445–4452 128. Lavie A, Agarwal A. Meteor: An automatic metric for mt evaluation with high levels of correlation with hu- man judgments. In: Proceedings of the second work- shop on statistical machine translation. 2007, 228–231 129. Vedantam R, Lawrence Zitnick C, Parikh D. Cider: Consensus-based image description evaluation. In: Proceedings of the IEEE conference on computer vi- sion and pattern recognition. 2015, 4566–4575 130. Rus V , Lintean M. An optimal assessment of natu- ral language student input using word-to-word simi- larity metrics. In: Intelligent Tutoring Systems: 11th International Conference, ITS 2012, Chania, Crete, Greece, June 14-18, 2012. Proceedings 11. 2012, 675– 676 131. Wieting J, Bansal M, Gimpel K, Livescu K. Towards universal paraphrastic sentence embeddings. arXiv preprint arXiv:1511.08198, 2015 132. Forgues G, Pineau J, Larchev ˆeque J M, Tremblay R. Bootstrapping dialog systems with word embeddings. In: Nips, modern machine learning and natural lan- guage processing workshop. 2014, 168 133. Lowe R, Noseworthy M, Serban I V , Angelard-Gontier N, Bengio Y , Pineau J. Towards an automatic turing test: Learning to evaluate dialogue responses. In: Pro- ceedings of the 55th Annual Meeting of the Associa- tion for Computational Linguistics (V olume 1: Long34 Front. Comput. Sci., 2024, 0(0): 1–36 Papers). 2017 134. Devlin J, Chang M W, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the As- sociation for Computational Linguistics: Human Lan- guage Technologies, V olume 1 (Long and Short Pa- pers). 2019, 4171–4186 135. Chen Y , Deng S, Kwak D H, Elnoshokaty A, Wu J. A multi-appeal model of persuasion for online petition success: A linguistic cue-based approach. Journal of the Association for Information Systems, 2019, 20(2): 105–131 136. Jing Wen T, Kim E, Wu L, Dodoo N A. Activating persuasion knowledge in native advertising: the influ- ence of cognitive load and disclosure language. Inter- national Journal of Advertising, 2020, 39(1): 74–93 137. Thimm M. Strategic argumentation in multi-agent sys- tems. KI-K ¨unstliche Intelligenz, 2014, 28: 159–168 138. Poldrack R A, Farah M J. Progress and challenges in probing the human brain. Nature, 2015, 526(7573): 371–379 139. Arapakis I, Barreda-Angeles M, Pereda-Ba ˜nos A. In- terest as a proxy of engagement in news reading: Spectral and entropy analyses of eeg activity patterns. IEEE Transactions on A ffective Computing, 2017, 10(1): 100–114 140. Wolf T, Sanh V , Chaumond J, Delangue C. Trans- fertransfo: A transfer learning approach for neural network based conversational agents. arXiv preprint arXiv:1901.08149, 2019 141. Qian K, Yu Z. Domain adaptive dialog generation via meta learning. In: Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics. 2019, 2639–2649 142. Shi Z, Huang M. A deep sequential model for dis- course parsing on multi-party dialogues. In: Pro- ceedings of the AAAI Conference on Artificial Intel- ligence. 2019, 7007–7014 143. Ju D, Feng S, Lv P, Wang D, Zhang Y . Learning to improve persona consistency in multi-party dia- logue generation via text knowledge enhancement. In: Proceedings of the 29th International Conference on Computational Linguistics. 2022, 298–309 144. Yuan L, Chen F, Zhang Z, Yu Y . Communication- robust multi-agent learning by adaptable auxiliary multi-agent adversary generation. Frontiers of Com- puter Science, 2024, 18(6): 186331 145. Ito A, Nakano Y I, Nihei F, Sakato T, Ishii R, Fukayama A, Nakamura T. Predicting persuasive- ness of participants in multiparty conversations. In: 27th International Conference on Intelligent User In- terfaces. 2022, 85–88 146. Gu J C, Li T, Liu Q, Ling Z H, Su Z, Wei S, Zhu X. Speaker-aware bert for multi-turn response selec- tion in retrieval-based chatbots. In: Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020, 2041–2044 147. Gu J C, Ling Z H, Liu Q, Liu C, Hu G. Gift: Graph- induced fine-tuning for multi-party conversation un- derstanding. arXiv preprint arXiv:2305.09360, 2023 148. Belinkov Y , Gehrmann S, Pavlick E. Interpretability and analysis in neural nlp. In: Proceedings of the 58th annual meeting of the association for computational linguistics: tutorial abstracts. 2020, 1–5 149. Jacovi A, Goldberg Y . Towards faithfully interpretable nlp systems: How should we define and evaluate faith- fulness? In: Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics. 2020, 4198–4205 150. Gaur M, Faldu K, Sheth A. Semantics of the black- box: Can knowledge graphs help make deep learning systems more interpretable and explainable? IEEE Internet Computing, 2021, 25(1): 51–59 151. Yasunaga M, Ren H, Bosselut A, Liang P, Leskovec J. Qa-gnn: Reasoning with language models and knowl- edge graphs for question answering. In: Proceedings of the 2021 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies. 2021, 535–546 152. Quek F, McNeill D, Bryll R, Duncan S, Ma X F, Kir- bas C, McCullough K E, Ansari R. Multimodal human discourse: gesture and speech. ACM Transactions on Computer-Human Interaction (TOCHI), 2002, 9(3): 171–193 153. Turk M. Multimodal interaction: A review. Pattern recognition letters, 2014, 36: 189–195 154. Jaimes A, Sebe N. Multimodal human–computer in- teraction: A survey. Computer vision and image un- derstanding, 2007, 108(1-2): 116–134 155. Baltru ˇsaitis T, Ahuja C, Morency L P. Multimodal ma- chine learning: A survey and taxonomy. IEEE trans- actions on pattern analysis and machine intelligence, 2018, 41(2): 423–443 156. Qi J, Niu Y , Huang J, Zhang H. Two causal principles for improving visual dialog. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition. 2020, 10860–10869 157. Alamri H, Cartillier V , Das A, Wang J, Cherian A, Essa I, Batra D, Marks T K, Hori C, Anderson P, oth- ers . Audio visual scene-aware dialog. In: Proceedings of the IEEE/CVF Conference on Computer Vision andMengqi Chen et al. 35 Pattern Recognition. 2019, 7558–7567 158. Wu C, Yin S, Qi W, Wang X, Tang Z, Duan N. Vi- sual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023 159. Lu J, Batra D, Parikh D, Lee S. Vilbert: Pretrain- ing task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural infor- mation processing systems, 2019, 32 160. Pal S, Bhattacharya M, Lee S S, Chakraborty C. A domain-specific next-generation large language model (llm) or chatgpt is required for biomedical engineer- ing and research. Annals of Biomedical Engineering, 2023, 1–4 161. Liang J, Huang W, Xia F, Xu P, Hausman K, Ichter B, Florence P, Zeng A. Code as policies: Language model programs for embodied control. In: 2023 IEEE International Conference on Robotics and Automation (ICRA). 2023, 9493–9500 162. Wen H, Li Y , Liu G, Zhao S, Yu T, Li T J J, Jiang S, Liu Y , Zhang Y , Liu Y . Empowering llm to use smart- phone for intelligent task automation. arXiv preprint arXiv:2308.15272, 2023 163. Kim H, Hessel J, Jiang L, Lu X, Yu Y , Zhou P, Bras R L, Alikhani M, Kim G, Sap M, others . Soda: Million-scale dialogue distillation with so- cial commonsense contextualization. arXiv preprint arXiv:2212.10465, 2022 164. Zheng C, Sabour S, Wen J, Huang M. Augesc: Large- scale data augmentation for emotional support con- versation with pre-trained language models. arXiv preprint arXiv:2202.13047, 2022 Mengqi Chen was born in 1997. She received her master’s de- gree in digital textiles from Xi’an Polytechnical University (XPU) in 2022. She is currently working toward a Ph.D. degree at North- western Polytechnical University (NWPU). Her current research in- terests include natural language processing, dialog sys- tems, and large language models. Bin Guo was born in 1980. He is a Ph.D. professor and Ph.D. super- visor at Northwestern Polytechni- cal University (NWPU). He is a senior member of the China Com- puter Federation. His main re- search interests include ubiquitous computing, social and community intelligence, urban big data mining, mobile crowdsensing, and human- computer interaction. Hao Wang was born in 1996. He received his B.E. degree in com- puter science and technology from Northwestern Polytechnical Uni- versity (NWPU) in 2019. He is currently working toward a Ph.D. degree at NWPU. His current re- search interests include natural language processing, di- alog systems, and large language models. Haoyu Li was born in 2002. He received his B.E. degree in com- puter science and technology from Northwestern Polytechnical Uni- versity (NWPU) in 2023. He is currently working toward a mas- ter’s degree at NWPU. His cur- rent research interests include natural language process- ing, large language models, and robot dynamic obstacle avoidance.36 Front. Comput. Sci., 2024, 0(0): 1–36 Qian Zhao was born in 2001. She received her B.E. degree in In- ternet of Things engineering from Tianjin University of Technology (TUT) in 2023. She is currently working toward a master’s de- gree at Northwestern Polytechni- cal University (NWPU). Her cur- rent research interests include multimodal dialogue, large language models, and visual human-computer in- teraction. Jingqi Liu was born in 2002. She entered Northwestern Polytechni- cal University(NWPU) to study for a bachelor’s degree in infor- mation and computing science in 2020. Her current research inter- ests include natural language pro- cessing, dialogue systems, and large language models. Yasan Ding was born in 1995. He received his B.E. degree in com- puter science and technology from Northwestern Polytechnical Uni- versity (NWPU) in 2018. He is currently working toward a Ph.D. degree at NWPU. His current research interests include fake news detection and natural language processing. Yan Pan was born in 1991. He is a lecturer at the Science and Technology on Information Sys- tems Engineering Laboratory. He respectively received the B.S. de- gree in 2013 and the Ph.D. degree in 2020 from Northwestern Poly- technical University (NWPU). His research interests in- clude Big Data, Machine Learning, and Crowd Intelli- gence. Zhiwen Yu was born in 1977. He is a Ph.D. professor and Ph.D. su- pervisor. He is a senior member of the China Computer Federation. His main research interests in- clude mobile internet, ubiquitous computing, social and community intelligence, urban big data min- ing, mobile crowdsensing, and human-computer inter- action.",
      "references": [
        "Conditional text generation for harmonious human-machine interaction",
        "Challenges in building intelligent open-domain dialog systems",
        "The elaboration likelihood model of persuasion",
        "Persuasive technology: using computers to change what we think and do",
        "Persuasive technology for human well-being: setting the scene",
        "Mass interpersonal persuasion: An early view of a new phenomenon",
        "Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions",
        "Analyzing the semantic types of claims and premises in an online persuasive forum",
        "Persuasive system design: state of the art and future directions",
        "Cognitive theories of persuasion",
        "Persuasion for good: Towards a personalized persuasive dialogue system for social good",
        "Effects of persuasive dialogues: testing bot identities and inquiry strategies",
        "Dialograph: Incorporating interpretable strategy-graph networks into negotiation dialogues",
        "Recent advances in natural language processing via large pre-trained language models: A survey",
        "A survey of large language models",
        "Llama 2: Open foundation and fine-tuned chat models",
        "Constitutional ai: Harmlessness from ai feedback",
        "A comprehensive survey on pretrained foundation models: A history from bert to chatgpt",
        "Chatgpt: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope",
        "Generating empathetic responses through emotion tracking and constraint guidance",
        "Informative and diverse emotional conversation generation with variational recurrent pointer-generator",
        "Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models",
        "Lever: Learning to verify language-to-code generation with execution",
        "Wordcraft: story writing with large language models",
        "From human writing to artificial intelligence generated text: examining the prospects and potential threats of chatgpt in academic writing",
        "Mood and persuasion: A cognitive response analysis",
        "Emotion and persuasion: Cognitive and meta-cognitive processes impact attitudes",
        "Dynamic knowledge routing network for target-guided open-domain conversation",
        "A persuasive chatbot using a crowd-sourced argument graph and concerns",
        "The psychology of tailoring-ingredients in computer-tailored persuasion",
        "Intelligent cognitive assistants for attitude and behavior change support in mental health: state-of-the-art technical review",
        "An autonomous debating system",
        "Towards topic-guided conversational recommender system",
        "Recommendation as a communication game: Self-supervised bot-play for goal-oriented dialogue",
        "Seamlessly integrating factual information and social content with persuasive dialogue",
        "Persuasive natural language generation–a literature review",
        "Let’s negotiate! a survey of negotiation dialogue systems",
        "A survey on proactive dialogue systems: Problems, methods, and prospects",
        "Pre-suasion: A revolutionary way to influence and persuade",
        "Argument invention from first principles",
        "Does the chimpanzee have a theory of mind?",
        "Coke: A cognitive knowledge graph for machine theory of mind",
        "Neural theory-of-mind? on the limits of social intelligence in large lms",
        "Rmm: A recursive mental model for dialogue navigation",
        "The philosophy of rhetoric",
        "Towards emotional support dialog systems",
        "Decoupling strategy and generation in negotiation dialogues",
        "Improving multi-turn emotional support dialogue generation with lookahead strategy planning",
        "Effects of message repetition and position on cognitive response, recall, and persuasion",
        "Influence: The psychology of persuasion",
        "Hitkg: Towards goal-oriented conversations via multi-hierarchy learning",
        "Eagle: Enhance target-oriented dialogs by global planning and topic flow integration",
        "Communication and persuasion: Central and peripheral routes to attitude change",
        "Argument mining: Extracting arguments from online dialogue",
        "Ampersand: Argument mining for persuasive online discussions",
        "From argument search to argumentative dialogue: A topic-independent approach to argument acquisition for dialogue systems",
        "Arguetutor: An adaptive dialog-based learning system for argumentation skills",
        "Recent advances in deep learning based dialogue systems: A systematic survey",
        "Sparks of artificial general intelligence: Early experiments with gpt-4",
        "Long short-term memory",
        "Attention is all you need",
        "Pepds: A polite and empathetic persuasive dialogue system for charity donation",
        "Mortality in mental disorders and global disease burden implications: a systematic review and meta-analysis",
        "Survey on psychotherapy chatbots",
        "Identifying emotional causes of mental disorders from social media for effective intervention",
        "Case: Aligning coarse-to-fine cognition and affection for empathetic response generation",
        "Comet: Commonsense transformers for automatic knowledge graph construction",
        "Conceptnet 5.5: An open multilingual graph of general knowledge",
        "Fear of the russian bear? negotiating finnish national identity online",
        "Hierarchical argumentation structure for persuasive argumentative dialogue generation",
        "Increasing the naturalness of an argumentative dialogue system through argument chains",
        "Target-guided dialogue response generation using commonsense and data augmentation",
        "A unifying perspective on perception and cognition through linguistic representations of emotion",
        "Cognition, evolution, and behavior",
        "Designing persuasive dialogue systems: Using argumentation with care",
        "Why are persuasive strategies effective? exploring the strengths and weaknesses of socially-oriented persuasive strategies",
        "Making robots persuasive: the influence of combining persuasive strategies (gazing and gestures) by a storytelling robot on its persuasive power",
        "Empathetic persuasion: reinforcing empathy and persuasiveness in dialogue systems",
        "Bleu: a method for automatic evaluation of machine translation",
        "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
        "Automatic evaluation of summaries using n-gram co-occurrence statistics",
        "Prompt-based monte-carlo tree search for goal-oriented dialogue policy planning",
        "Augmenting non-collaborative dialog systems with explicit semantic and strategic dialog history",
        "Knowledge-enhanced memory model for emotional support conversation",
        "Misc: A mixed strategy-aware model integrating comet for emotional support conversation",
        "Handbook of communication and social interaction skills",
        "Helping skills: Facilitating, exploration, insight, and action",
        "Semi-supervised classification with graph convolutional networks",
        "Graph attention networks",
        "A comprehensive survey on graph neural networks",
        "Graph neural networks for natural language processing: A survey",
        "Towards informative and diverse dialogue systems over hierarchical crowd intelligence knowledge graph",
        "Modeling persuasion factor of user decision for recommendation",
        "Comae: A multi-factor hierarchical framework for empathetic response generation",
        "Improving dialog systems for negotiation with personality modeling",
        "Building emotional support chatbots in the era of llms",
        "Knowledge graph grounded goal planning for open-domain conversation generation",
        "Gochat: Goal-oriented chatbots with hierarchical reinforcement learning",
        "Interacting with non-cooperative user: A new paradigm for proactive dialogue policy",
        "Keyword-guided neural conversational model",
        "Thinking clearly, talking fast: Concept-guided non-autoregressive generation for open-domain dialogue systems",
        "Two stage learning for argument pairs extraction",
        "Topkg: Target-oriented dialog via global planning on knowledge graph",
        "Target-guided open-domain conversation",
        "A target-driven planning approach for goal-directed dialog systems",
        "Dialogue planning via brownian bridge stochastic process for goal-directed proactive dialogue",
        "Towards argument mining for social good: A survey",
        "Cross-domain mining of argumentative text through distant supervision",
        "Argument generation with retrieval, planning, and realization",
        "Argument mining using bert and self-attention based embeddings",
        "Argument mining with structured svms and rnns",
        "Exploring the role of argument structure in online debate persuasion",
        "Multi-task learning in argument mining for persuasive online discussions",
        "Iam: A comprehensive and large-scale dataset for integrated argument mining tasks",
        "A hierarchical sequence labeling model for argument pair extraction",
        "Smote: synthetic minority over-sampling technique",
        "Augesc: Dialogue augmentation with large language models for emotional support conversation",
        "Psyqa: A chinese dataset for generating long counseling text for mental health support",
        "A corpus for research on deliberation and debate",
        "Debatesum: A large-scale argument mining and summarization dataset",
        "Negotiation-coach",
        "Keeping up appearances: Computational modeling of face acts in persuasion oriented discussions",
        "Towards conversational recommendation over multi-type dialogs",
        "Inspired: Toward sociable recommendation dialog systems",
        "Internet argument corpus 2.0: An sql schema for dialogic social media and the corpora to go with it",
        "Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments",
        "Cider: Consensus-based image description evaluation",
        "An optimal assessment of natural language student input using word-to-word similarity metrics",
        "Towards universal paraphrastic sentence embeddings",
        "Bootstrapping dialog systems with word embeddings",
        "Towards an automatic turing test: Learning to evaluate dialogue responses",
        "Bert: Pre-training of deep bidirectional transformers for language understanding",
        "A multi-appeal model of persuasion for online petition success: A linguistic cue-based approach",
        "Activating persuasion knowledge in native advertising: the influence of cognitive load and disclosure language",
        "Strategic argumentation in multi-agent systems",
        "Progress and challenges in probing the human brain",
        "Interest as a proxy of engagement in news reading: Spectral and entropy analyses of eeg activity patterns",
        "Transfertransfo: A transfer learning approach for neural network based conversational agents",
        "Domain adaptive dialog generation via meta learning",
        "A deep sequential model for discourse parsing on multi-party dialogues",
        "Learning to improve persona consistency in multi-party dialogue generation via text knowledge enhancement",
        "Communication-robust multi-agent learning by adaptable auxiliary multi-agent adversary generation",
        "Predicting persuasiveness of participants in multiparty conversations",
        "Speaker-aware bert for multi-turn response selection in retrieval-based chatbots",
        "Gift: Graph-induced fine-tuning for multi-party conversation understanding",
        "Interpretability and analysis in neural nlp",
        "Towards faithfully interpretable nlp systems: How should we define and evaluate faithfulness?",
        "Semantics of the black-box: Can knowledge graphs help make deep learning systems more interpretable and explainable?",
        "Qa-gnn: Reasoning with language models and knowledge graphs for question answering",
        "Multimodal human discourse: gesture and speech",
        "Multimodal interaction: A review",
        "Multimodal human–computer interaction: A survey",
        "Multimodal machine learning: A survey and taxonomy",
        "Two causal principles for improving visual dialog",
        "Audio visual scene-aware dialog",
        "Visual chatgpt: Talking, drawing and editing with visual foundation models",
        "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
        "A domain-specific next-generation large language model (llm) or chatgpt is required for biomedical engineering and research",
        "Code as policies: Language model programs for embodied control",
        "Empowering llm to use smartphone for intelligent task automation",
        "Soda: Million-scale dialogue distillation with social commonsense contextualization",
        "Augesc: Large-scale data augmentation for emotional support conversation with pre-trained language models"
      ],
      "meta_data": {
        "arxiv_id": "2402.04631v1",
        "authors": [
          "Mengqi Chen",
          "Bin Guo",
          "Hao Wang",
          "Haoyu Li",
          "Qian Zhao",
          "Jingqi Liu",
          "Yasan Ding",
          "Yan Pan",
          "Zhiwen Yu"
        ],
        "published_date": "2024-02-07T07:28:34Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This review paper systematically surveys Cognitive Strategy-enhanced Persuasive Dialogue Agents (CogAgent). It formalizes the definition of three key cognitive strategies (persuasion, topic path planning, and argument structure prediction) derived from cognitive psychology theories. It proposes a new generic system architecture for CogAgent, investigates representative research works categorized by these strategies, and summarizes existing datasets and evaluation metrics. The paper also identifies open issues and promising future research directions in the field.",
        "methodology": "The paper proposes a formalized concept model and a generic system architecture for CogAgent. This architecture involves semantic understanding via LLMs, cognitive strategy mining (for persuasion strategies, topic paths over knowledge graphs, and argument structures), cognitive strategy prediction for dialogue modeling, and LLM-based response generation. It then categorizes and investigates existing CogAgent solutions based on whether they primarily employ persuasion strategies, topic path planning strategies (e.g., using reinforcement learning or graph-based methods), or argument structure prediction strategies (e.g., argument mining and structured prediction).",
        "experimental_setup": "As a review paper, it does not present new experimental results but summarizes existing datasets and evaluation metrics used in the field. Datasets are categorized by application scenarios including psychological counseling (ESConv, AUGESC, PsyQA), debate (IAC, Winning Arguments, DebateSum), price negotiation (CraigslistBargain, Negotiation-Coach), persuasion for donation (Persuasion for Social Good, EPP4G, ETP4G, FaceAct), and product recommendation (TG-ReDial, DuRecDial, INSPIRED). Evaluation metrics discussed include automatic methods like overlap-based (BLEU, ROUGE, METEOR, CIDEr), embedding-based (Greedy Matching, Embedding averaging, Vector Extreme), and learning-based (ADEM), as well as human evaluation criteria such as fluency, coherence, contextualization, emotional expression, diversity, and persuasiveness.",
        "limitations": "Key challenges and limitations identified include the exhaustive mining of cognitive strategies (often task-specific), the complex modeling and dynamic selection of appropriate cognitive strategies, difficulties in integrating abstract cognitive strategies into neural network models (including LLMs), and the absence of comprehensive evaluation metrics that capture the effectiveness of persuasive strategies and argument structures. Additionally, current CogAgents often lack cross-domain adaptivity/generality, primarily focus on two-party conversations, and suffer from a lack of interpretability in the persuasion process. The reliance on textual modalities also limits their multimodal comprehension, and existing datasets are often small-scale or lack detailed cognitive strategy annotations.",
        "future_research_directions": "Future research directions include comprehensive modeling of cognitive psychology theories (e.g., addressing cognitive dissonance, modeling user cognition, and leveraging neuroscience), improving CogAgent's model adaptivity and generality across various domains, developing multi-party CogAgents for more complex real-world interactions, enhancing the interpretability of the persuasive process, and integrating multimodal perception capabilities (visual, auditory) into CogAgent. Furthermore, co-optimizing data and model approaches for CogAgent, especially by combining LLMs with model-driven persuasion processes, and constructing large-scale, high-quality standardized datasets and benchmarks with rich cognitive strategy annotations are crucial areas for advancement.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "AlphaMath Almost Zero: Process Supervision without Process",
      "full_text": "Anomaly-based Framework for Detecting Power Overloading Cyberattacks in Smart Grid AMI Abdelaziz Amara Korbaa, Nouredine Tamanid, Yacine Ghamri-Doudaned, Nour El Islem karabadjib,c aNetworks and Systems Laboratory (LRS), Badji Mokhtar-Annaba University, Annaba, Algeria bHigher School of industrial technologies, Annaba, P.O. Box 218, 23000, Algeria. cElectronic Document Management Laboratory (LabGED), Badji Mokhtar-Annaba University, Annaba, Algeria dL3i, Univ. of la Rochelle, La Rochelle, France Abstract The Advanced Metering Infrastructure (AMI) is one of the key components of the smart grid. It provides interactive services for managing billing and electricity consumption, but it also introduces new vectors for cyberattacks. Although, the devastating and severe impact of power overloading cyber- attacks on smart grid AMI, few researches in the literature have addressed them. In the present paper, we propose a two-level anomaly detection frame- work based on regression decision trees. The introduced detection approach leverages the regularity and predictability of energy consumption to build reference consumption patterns for the whole neighborhood and each house- hold within it. Using a reference consumption pattern enables detecting power overloading cyberattacks regardless of the attacker’s strategy as they cause a drastic change in the consumption pattern. The continuous two-level monitoring of energy consumption load allows efficient and early detection of cyberattacks. We carried out an extensive experiment on a real-world pub- licly available energy consumption dataset of 500 customers in Ireland. We extracted, from the raw data, the relevant attributes for training the energy consumption patterns. The evaluation shows that our approach achieves a high detection rate, a low false alarm rate, and superior performances com- pared to existing solutions. Keywords: Smart grid, Advanced Metering Infrastructure (AMI), Overloading cyberattacks, Anomaly detection. Preprint submitted to Elsevier July 4, 2024 arXiv:2407.03264v1  [cs.CR]  3 Jul 20241. Introduction Information and communication technologies played a crucial role in the growth and performance of the smart grids. The advanced metering infras- tructure provides a two-way communication network between smart meters and utility systems, offering interactive services for managing billing and elec- tricity consumption. However, interconnecting the smart grid distributed elements, also introduces new vectors for cyberattacks. The first success- ful cyberattack on power grid is recorded in December 2015, it struck the Ukraine power grid causing power outages putting more than 100 cities in the dark. The hackers exploited vulnerable points in the infrastructure using a piece of malware known as Black Energy. Several other cyberattacks have followed showing how a hacker with a piece of malware can take control of a power plant’s circuit breaker and damage generators. Power overloading is one of the most severe cyberattacks, it aims at in- creasing the energy load to disrupt the load balance on the local power grid, cause a blackout, and damage the grid infrastructure. An attacker with low cost equipment could exploit security vulnerabilities within some points in the smart grid communication infrastructure, particularly within smart meter. The exploit may grant the attacker the command and control of thousands of smart meters that he can subsequently use to dramatically in- crease the demand of electricity, and to disrupt the load balance on the local power grid. The attacker can also compromise the communication infras- tructure or hack the substation, and then send fake pricing information to the local community. By exploiting the vulnerability of load control systems, an attacker can modify the consumption profile of the customers and the whole neighborhood (more details are provided in section 4). Although, the severe impact of power overloading cyberattacks, few works [1, 2, 3, 4] in the literature have addressed them. Traditional anomaly detection systems based on network features did not consider attack scenarios and inherent characteristics of the smart grid AMI. Current smart grid AMI anomaly detection systems consider fault detection [5], and address mainly two types of cyberattacks: electricity theft and pric- ing cyberattacks [6, 7, 8]. The goal of energy theft is to pay less than the actual price for the consumed energy, in this case the attacker can physically tamper the smart meter, or compromise the communication infrastructure 2[9]. Two common pricing cyberattacks on smart home systems, which ma- nipulate guideline pricing have been studied in the literature [2, 3, 4]. In the first one, named cyberattack for bill reduction, the attacker attempts to fake the guideline pricing curve such that it can reduce the cost of his own bill at the cost of bill increase of other customers. The goal of the second cyberattack is to create a peak energy load in the local community. Most of the existing anomaly detection systems for smart grid AMI in the literature have been proposed for energy theft detection [10, 11, 12, 13, 14, 15, 16] and pricing cyberattacks [2, 3, 4]. However, few researches have addressed overloading cyberattacks targeting grid blackout. Although some works [1, 2, 3, 4] addressed grid overloading cyberattacks, they only considered short term load increase resulting from pricing manipulation. In this case, the attacker’s goal is to make profit not to shut down the power grid. Anomaly detection systems which monitor the guideline pricing curve are only effective if the attacker overloads the grid by manipulating the guideline pricing curve. Otherwise, the grid overloading cyberattacks would not be detected if the attacker changes its strategy. Some existing anomaly detection systems proposed in the literature deal with energy theft and grid overloading in the same way. Although energy theft and grid overloading both correspond to an abnormality in the con- sumption pattern, the two attacks have their proper subtleties and differ in several points such as: attacker’s operating mode, detection delay, and im- pact on the AMI. Unlike energy theft, grid overloading cyberattack causes an immediate damage, therefore the detection delay is critical in this case. The impact of grid overloading cyberattacks is beyond the smart home, there- fore monitoring the energy load at a neighborhood level is needed to avoid cascading failure. Most of the existing anomaly detection systems in the literature used clas- sification algorithms. One issue with classification-based anomaly detection is the unavailability of malicious samples. Using synthetic malicious samples can solve this issue, however the classifier would not detect unseen attacks that significantly deviate from the synthetic malicious samples used to train the system. In this paper, we tackle the issue of grid overloading cyberattacks against the smart grid AMI. We propose a consumption pattern-based anomaly de- tection framework (CPADF) to detect and prevent grid overloading cyber- attacks. CPADF applies features engineering and regression-based learning algorithms on historical consumption data to generate normal consumption 3patterns for the whole neighborhood and for each customer within it. The obtained trained models are then harnessed in a decision making process, we developed, to detect anomalies in consumption patterns. To do so, CPADF monitors continuously electricity consumption at both home and neighbor- hood level and aggregates anomaly alerts received from customers. An ab- normal consumption raise is then detected at a given level if the observed consumption does not match up with its corresponding normal consumption pattern. We carried out experiments on a real-world publicly available energy con- sumption dataset of 500 customers in Ireland. We proceeded with data clean- ing and feature extraction on the raw data. Initially, the dataset provides 3 attributes only (smart meter identifier, timestamp, and energy consump- tion) from which we extracted some relevant attributes for training the energy consumption patterns, such as day time, day type, month and season, and we generated labelled datasets for both home and neighborhood levels for the model training and testing 1. The evaluation shows that our approach achieves a high detection rate, a low false alarm rate, and superior perfor- mances compared to existing solutions with an optimal training time and memory requirement. Furthermore, CPADF outperforms existing approach in terms of explor- ing and detecting sophisticated scenarios of power overloading cyberattacks against smart grid AMI. Indeed, the consumption pattern-based anomaly detection makes CPADF able to detect grid overloading cyberattacks re- gardless of the attacker’s strategy. Whereas, most of the existing solutions are attacker’s strategy oriented, which may fail in detection of cyberattacks if the attacker changes its strategy. The remainder of this paper is organized as follows. In Section 2, we sum- marize the state of the art in the field of anomaly detection systems developed to protect smart grid systems. In Section 3, we present the AMI network architecture. In Section 4, different types of power overloading cyberattacks are studied. Section 5 describes the CPADF framework from data collection to anomaly detection process. We evaluate the performance of CPADF in Section 6. Section 7 concludes the paper and draws some lines for future work. 1The generated labelled datasets are made available for free upon request. 42. Related work Most of the existing works in the literature are related to fraud detec- tion, such as electricity theft and pricing cyberattacks. In [2] and [3] the authors considered two smart home pricing cyberattacks: cyberattack for bill reduction; and cyberattack for forming a peak energy load. In the first cyberattack, the hacker attempts to fake the guideline pricing curve such that it can reduce the cost of his own bill at the cost of bill increase of other cus- tomers. The goal of the second cyberattack is to create a peak energy usage by faking the guideline pricing curve. A countermeasure technique which uses support vector regression and impact difference for detecting pricing manipulation has been proposed in [2]. The proposed system leverages the interdependence between the electricity pricing and the energy load in the power system. It detects the peak energy load by monitoring changes in the guideline pricing curve. To improve the detection system accuracy, the authors proposed in [3] a partially observable Markov decision process for modeling the long term impact of pricing cyberattacks. In [4], the authors introduced a new type of pricing cyberattack, which creates a sharp increase or decrease of the energy load, resulting in a dramatic drop of generation frequency. To tackle the scalability limitation of the system proposed in [3] and address the new pricing cyberattack, Liu et al.[4] proposed a new hier- archical framework, which models the attacking state of each smart meter in a distributed fashion. The proposed framework employs a global policy optimization algorithm to take a centralized decision on checking and repair- ing the compromised smart meters. In [2, 3, 4], the attacker’s objective is to make profit not to cause a blackout by overloading the grid. Although [2, 3, 4] address grid overloading cyberattack, they only consider short term load increase resulting from pricing manipulation. On the other hand, in this paper we consider different types of long term grid overloading cyberattacks. The proposed anomaly detection systems in [2, 3, 4] are only effective if the attacker creates a peak energy load by manipulating the guideline pricing curve. In this paper, we focus on detecting grid overloading cyberattacks based on consumption pattern changes, regardless of the attacker’s strategy. Jokar et al. [1] addressed grid overloading as well as energy theft. They considered the scenario, where the attacker increases the energy load by manipulating prices or compromising the direct load control system. The authors [1] proposed two anomaly detection algorithms based on the pre- dictability of consumption patterns of customers. In [10] Jokar et al. ex- 5tended and adapted their proposition to detect only energy theft attack. They used transformer meters and anomaly detectors, as well as appropriate classification and clustering techniques, to improve the performance and the robustness of the algorithm against nonmalicious changes in consumption pattern. Classification-based methods need malicious samples to train the classifier, which might not be available, since malicious behavior might never or seldom occur for a given customer. Using synthetic malicious samples can solve the problem. However the classifier would not detect attacks that deviate significantly from the synthetic malicious samples used to train the system. In this paper, we use regression decision trees to predict the con- sumption profiles during a particular time slot, and then we compare the expected profile with the actual one. Our approach is capable of detect- ing different attack types, because it does not build the classifier using a particular type of synthetic malicious samples. Ford et al. [17] and Cody et al. [18] also addressed grid overloading as well as energy theft. They used artificial neural networks and decision tree respectively to model the normal profile of customer’s energy consumption. Real historical data from the Irish smart energy trial [19] were used to gener- ate the regression models and predict future energy consumption. Then, the anomaly detection systems compare the predicted value with the actual con- sumption to detect malicious behaviors. Although the proposed approaches [17] [18] overcome the limitations of classification-based methods, only one type of grid overloading cyberattack has been considered. The proposed sys- tems do not monitor the consumption pattern at the neighborhood level. Monitoring pattern change at the neighborhood level improves the detection accuracy and reduces the detection delay, since the load increase is more noticeable at the neighborhood level than for a single customer or group of customers, particularly at the beginning of the attack. Important factors such as memory requirement and processing time have not been considered. Using more attributes to model the consumption pattern, CPADF provides a better prediction with lower error rates. CPADF shows good performance in terms of memory requirement and processing time. Our tests show that CPADF outperforms the anomaly detection systems proposed in [17] [18]. Faisal et al. [20] proposed a new intrusion detection system (IDS) archi- tecture for the whole AMI system at the levels of smart meter, data concen- trator, and headend. A feasibility analysis of the application of several data stream mining algorithms has been conducted to select the best algorithm for each AMI component. In [21], Zhang et al. proposed a distributed intrusion 6detection system for smart grids (SGDIDS) with a hierarchical three layer structure. The proposed IDS analyzes communication traffic using classifica- tion algorithms such as support vector machine (SVM) and artificial immune system (AIS). The proposed systems in [21] and [20] have been validated on the widely used public KDD Cup 1999 dataset [22]. However, this dataset was designed for intrusion detection in computer networks, the considered attacks are based on communication scenarios. The dataset did not consider characteristics inherent to the smart grid infrastructure and attack scenarios against AMI transactions. Furthermore, the KDD dataset [22] has a huge number of redundant records and biased distribution of attacks. In [23] an optimal strategy of on-site investigation and monitoring verifi- cation for potential anomalies and malware is proposed. Using the decision process framework of Markovian, and based on the observation from the deployed anomaly detectors, the proposed framework determines the best inspection strategies. Alcarez et al. [24] examined key security aspects of the Open Charge Point Protocol (OCPP) for communication between electric vehicle, charging points and central management system. The paper shows how a hacker can exploit OCPP vulnerabilities to carry out attacks to bur- den resource reservation related to electric vehicle, steal energy, or overload the grid. For instance, an attacker might inject forged OCPP transaction to destabilize network or to affect its functioning. In [25] the authors analyzed a set of existing anomaly detection approaches which use machine learning, knowledge and statistical detection-based techniques, and information and spectral theory. The authors investigated the functionalities of the detection approaches for context-awareness in smart grid environments. The paper provides a guideline regarding the choice of the most suitable schemes and detection modes. The suitability is examined based on the restrictions of the context and functional characteristics of the technologies and communication systems. In section 6.4, we show the suitability of CPADF to the smart grid context according to the set of requirements specified in [25]. 3. AMI network architecture The smart home (SH) constitutes an integral part of the smart grid AMI, it leverages sensors and networking technologies to be in continuous inter- action with its internal and external environments. The Energy Services Interface (ESI) represents the interface connecting the SH to the smart grid. Although there is a logical separation between the smart meter and ESI, their 7functionalities are generally integrated into one physical device (generally the smart meter) for cost effectiveness. The ESI has diverse functionalities, such as remote control of devices, transmission of consumption data to the util- ity, supervising of Distributed Energy Resources such as wind turbines, the management of demand response programs, Plug in Electric / Plugin Electric Hybrid Vehicles (PEV/PHEV) charging etc. The Energy Management Sys- tem (EMS) represents the entity responsible for managing diverse appliances and systems within the SH. It enables the SH to adjust its energy consump- tion to suit the grid’s capacities. The EMS enables the management of high consuming appliances such as air conditioning system, and offers the remote configuration of the smart home devices [26]. Figure 1 shows the different entities of the AMI network architecture. The connections to the ESI are represented by the green dot-dashed lines, whereas the red-dotted lines rep- resent the connections to the EMS. The communication between the smart home and the AMI infrastructure is represented by the blue dashed line. The EMS and ESI are in constant two-way communication to manage the internal environment in coherence with the external environment requirements and capabilities [26]. The Home Area Network (HAN) interconnects appliances with ESI/smart meters and EMS. The Neighborhood Area Network (NAN) represents the network interconnecting the smart meters with the data con- centrator. The Wide Area Networks (WAN) interconnects multiple NANs to the Utility headend. 4. Power overloading cyberattacks In this section we present three types of power overloading cyberattacks which exploit the vulnerability of load control systems (such as smart home scheduling systems), and the vulnerability of OCPP protocol. The goal of load control systems is balancing supply and demand to ensure a reliable grid operation. Indirect load control (ILC) mechanisms use dynamic pricing to incite customers to adapt their consumption profiles to suit the grid capabili- ties. There are two dynamic pricing models, which are usually used together. The first model called real time pricing, where the price is set based on the energy consumption in the local community. The second one is the guideline pricing, where the utility predicts the future load, sets a predictive pricing curve, and uses it for guiding the customers on energy scheduling. The Direct Load Control mechanisms (DLC) allow the utility to directly control the cus- tomers’ loads by sending control signals such as turn on/off, through AMI. 8web server A/C Appliances Pool Pump Water heater Lights PEV Data concentrator NANWAN ESI EMS Distributed ebergy ressources (DER) Smart Electricity Meter Other Meters Internet Home Router Neighborhood In Home DISPLAY Utility Figure 1: AMI network architecture The OCPP is an application protocol for communication between electric ve- hicle and charging point and a central management system. One advantage of the introduction of electric vehicles into smart grids, is their bidirectional charging which allows local and global smoothing of imbalances and load peaks. Alcaraz et al. [24] studied attacks that misuse the OCPP protocol to destabilize power networks and interfere with resource reservation initiated with the electric vehicle. Although the paper provides divers threat scenarios related to the logical functionality of the OCPP at different stages, in this paper, we consider power overloading scenario at transactions and control stage. 4.1. Cyberattacks against ILC By manipulating the pricing curve, an attacker can modify the consump- tion profile of the customers and the whole neighborhood consumption pro- file. The attacker can either compromise the communication infrastructure or hack the substation, and then send fake pricing information to the local community. The attacker can also use a malware to compromise the smart meter and then modify the received pricing information (see figure 2). He can then scale that up as much as he may take control of thousands of smart homes, depending on the propagation of the malware [23]. In this paper 9Modify received  prices Send turn on/off signal Internet AMI Attacker Smart Home Figure 2: Power overloading cyberattacks we consider the two following cyberattacks against ILC mechanisms, called pricing cyberattacks [2] as follows. 4.1.1. Cyberattack for bill reduction The attacker manipulates the guideline pricing curve, in such a way that the electricity price is high during a particular time slot. This will dissuade the other customers to schedule energy consumption during this time slot. Thus, this reduces the local community energy load during this time slot, resulting in the decrease of the real time electricity price there. Afterward, the attacker could schedule the energy consumption during this time slot, and makes profit through reducing his own bill at the cost of bill increase of other customers [2]. 4.1.2. Cyberattack for forming a peak energy load The attacker first identifies peak consumption energy hours, and then he manipulates the guideline pricing curve such that it is very low during peak energy consumption hours. Therefore, the customers will schedule their large controllable high consumption appliances during peak usage energy hours. This will form a peak in energy consumption leading to significant distur- bance in the power system. Also increasing energy load fluctuation could significantly impacts the power system dynamics and changes the generation frequency dramatically. The attacker could increase the energy load fluctu- ation by manipulating the guideline prices such that it is very high during a time slot then it is very low during the next one, the shorter the time slot is, the higher load fluctuation would be [2] [4]. 104.2. Cyberattacks against DLC The attacker compromises the EMS to send fake “turn on/off” signal or- dering a large number of appliances within the premises to get switched on [1]. For instance, the attacker can create a surge by turning air conditioners on during peak usage energy periods such as extreme cold/heat or during peak usage hours of the day. Also in this case, the attacker can increase the energy load fluctuation by repeatedly sending turn on/off signals to a large number of appliances, particularly the high consumption ones, such as air conditioning. This will create disturbances and imbalances in the grid that could stumble breakers beyond the targeted neighborhood and cause a large area blackout. Table 1 summaries the characteristics of power overload- ing cyberattacks against load control mechanisms, and shows the anomalous consumption pattern changes. 4.3. Cyberattack against OCPP It has been shown in [24] that an attacker may damage the energy safety if the communication channels are intercepted, and the security credentials of an OCPP user/object is known. A hacker might carry out several attacks such as: denial of power resources and services, energy theft, and power overload. As mentioned previously, in this paper, we are interested in power overloading scenario. In smart grids, the majority of charging points are configured to provide bidirectional interfaces for power charging/discharging, so that batteries discharge during peak periods and charge during off-peak times. The central management system defines the charging profiles which specify the amount of power that can be supplied per time interval to one or multiple points of charge with their charging schedules. To increase power demand at peak periods, the attacker alters the charging profiles, in such a way that the intensity of Wh has to be greater at peak hours or equal to the power consumption in off-peak periods. The fake charging profiles are then used, so that multiple compromised points of charge inject energy into electric vehicle during peak periods. 5. CPADF Framework In this section, the CPADF is described. Firstly, data collection and attributes extraction are described. Next, regression algorithms used for consumption pattern modeling are presented. Lastly, the anomaly detection processes at smart home and neighborhood level are described in details. 11Table 1: Characteristics of power overloading cyberattacks Cyberattacks Load controlmechanismsTime slotsUsage patterns Bill reduction ILC Random Decrease of the whole neighborhood consumptionIncrease of N compromised smart home consumptions Forming peak energy loadILC/DLC Peak hours Increase of the whole neighborhood consumption Increasing load fluctuationILC/DLC Random and shortSuccession of energy consumption increase and decrease Hereinafter, we use the abbreviations SH to refer to smart home, and NBH to refer to neighborhood. 5.1. Data collection and attributes extraction Data collection and training process of consumption prediction models for SH and NBH anomaly detectors are illustrated in figure 3. Firstly, me- tering data are collected from each SH, and from transformer meter. Then, a dataset is generated for each SH, also NBH dataset including the whole neighborhood half hourly consumption is generated. Each data vector within SH and NBH datasets includes the electricity consumption along with a set of time and seasonal related attributes extracted from the raw data (time stamp and consumption). We consider the following attributes: time, day period (day/night), day type (weekday/weekend), month, and season. These attributes are used to allow predicting electricity consumption. For instance, if we consider the attribute day period, most often, electricity consumption tends to decrease during night due to the decline of human activities. The day type attribute allows catching legitimate consumption pattern changes related to the customer activity. For instance, the consumption on the week- end may drop considerably if the customer usually leaves his/her place for some vacations. In contrast, if the customer stays at home, he/she may consume more electricity than usual by spending more time using entertain- ment devices such as video games, TV, PC, etc. It is important to underline that electricity usage is directly or indirectly affected by external conditions, particularly by the seasonal conditions as weather and temperature. In win- ter, the weather is cold and dark, people tend to stay at home, and thus consume more electricity on lighting and heating. While in summer, the weather is sunny and hot, people tend to be out to enjoy sunny weather, so that their electricity consumption decreases. Also, the month attribute needs to be considered, because even within the same season two months 12Model 1 SH ModelsSH Datsets NBH Models  NBH Datset  Decision Maker Model Model Data Concentrator Transformer Meter  Model Metering Data SH Prediction models Smart Homes  SH 1  SH 2  SH N  2 N Model NBH Model data Anomaly data Metering data Anomaly Detector Preprocessing Data Suspect Sample  1 2 3 4 Prediction NBH consumption 5 1 Metering data collection 1 SH/NBH data preprocessing 3 SH/NBH prediction Model Generation 2 3 4 Transferring SH prediction models Sending suspect samples  Anomaly decision 5 6 Suspect Sample  6 Figure 3: Data collection, traning and anomaly detection process could have different consumption pattern, such as September/ December or January/ March. For instance, the consumption pattern of the 1st of Octo- ber differs from the one of the 21th of December, due to several factors such as: the number of daytime hours and temperature. The NBH prediction model is trained using periodic NBH global consumption calculated and sent by the transformer meter. The SH/NBH historical electricity consumption data are used to model and predict future electricity consumption.The ma- chine learning algorithms used to model consumption pattern are described in the subsequent section 5.2. The SH prediction models are trained within the data concentrator to overcome resource limitation within the ESI. Ab- normal consumption samples flagged as suspect by SH anomaly detector are transferred to the data concentrator. 5.2. Modeling consumption patterns To model SH/ NBH electricity consumption pattern so we can predict consumption at any time of the day, five algorithms of supervised machine learning have been used. These algorithms are selected for their known per- 13formance and low prediction error rate. The following gives brief description of the machine learning algorithms used in this paper. 5.2.1. REPTree REPTree (Reduced Error Pruning Tree) is a fast decision tree learner that uses information gain ratio (Formula (2)) as splitting criterion, where D is the whole dataset, m is the number of classes, pi is the frequency of class i in the dataset, K is the number of subsets generated by the split [27]. info(D) = mX i=1 −pilog2(pi) (1) GainRatio(A) = info(D) − mP j=1 |Dj | |D| × info(Dj) kP j=1 |Dj | |D| × log2 \u0010 |Dj | |D| \u0011 (2) 5.2.2. M5P M5P combines decision tree and linear regression, it uses Standard devi- ation (SD) to determine the best attribute for splitting the dataset at each node [27]. The attribute to be chosen is the one that maximizes the error reduction (Formula (3)). ∆error = SD(S) − mX i=1 \u0012|Di| |D| SD(Di) \u0013 (3) 5.2.3. Random Forest Random forest [28] is a combination of unpruned regression trees, it uses random feature selection in the tree induction process. The forest averages the prediction outputs returned by the individual trees. 5.2.4. Artificial Neural Network An artificial neural network is computational system consisting of inter- connected simple elements called neurons, which produce output depending on one or more inputs and an activation function e.g., sigmoid function, hy- perbolic, etc. Where φ in Formula (4) represents the activation function that determines the output value o according to the values of entries e and their weights w [27]. 14o = φ( i=PX i=0 wiei) (4) 5.2.5. SVM SVM is a supervised learning model used for classification and regression problems. For regression, SVM uses ε the insensitive loss function that pe- nalizes error only if it is greater than ε [29]. Therefore, the |ξ|ε is represented as: |ξ|ε = \u001a 0 if |ξ| ≤ε |ξ| −ε otherwise. Using (non-negative) slack variablesξi and ξi ∗, the final optimization problem to be solved can be formulated as follows: Minimize 1 2∥W∥2 + C X i=1 (ξi + ξi ∗) Subjected to: yi − f(xi, w) ≤ ε − ξi ∗ f(xi, w) − yi ≤ ε − ξi ∗ ξi, ξi ∗ ≥ 0, i= 1, ..., n (5) Where xi is a n-dimensional vector, and yi is the target, w is the weight vector, C represents the penalty for the error term. SVM regression finds the linear regression in the high-dimension feature space using ε while reducing the model complexity by minimizing ∥W∥2 The performance evaluation of these algorithms is discussed in the next sec- tion (Tables 3,4). 5.3. Anomaly detection The CPADF two-level monitoring architecture operates within the two main components of the AMI: the ESI, and the data concentrator, as illus- trated in Figure 4. Periodic metering data from SH are inserted into the SH preprocessing module, which is responsible for attributes extraction and data preprocessing. Since the NBH consumption depends on the consump- tion of all SHs in the neighborhood, the same set of time related and seasonal 15Anomaly Benign Retraining SH Sample Retraining New NAN sample Data Concentrator Alert NBH Metering Data Decision Maker Transformer meter Response New classifiers (SH & NBH) Notification NBH Preprocessing NBH Anomaly Detector TrueFalse NAN Sample Attack Alert Benign Benign Attacks TrueFalse Training Figure 4: CPADF workflow attributes are extracted from NBH metering data. The expected consump- tion calculated by the prediction model, is then compared with the received consumption value. After preprocessing the SH metering data into the appropriate format consistent with the SH training set, the SH regression model predicts the SH electricity consumption for the given attributes vector. If withinN successive time intervals the number of times an anomaly (consumption increase) is detected with a certain threshold (Nbrincr), then an anomaly is reported and the suspect sample is sent to the data concentrator. Otherwise, the processed sample is added to the benign dataset which is periodically transferred to data concentrator for periodic retraining. The threshold ( Nbrincr) specifies the number of tolerable successive abnormal consumption increase. This threshold is used to mitigate false alerts caused by occasional legitimate consumption increases. An unusual legitimate consumption increase may be caused by operating one or multiple high energy consumption appliances (washing machine, dishwasher, vacuum cleaning, oven, etc) out of their usual 16operating time. On average, the length of use of such appliances is between 40 and 120 minutes. For instance, a cycle of washing machine lasts on average between 20 and 60 minutes, most dishwashers cycles are about 2 hours. Thus, the threshold is set to 2 (which corresponds to two successive time intervals of 1 hour). The threshold is set and updated by the system administrator. RMSE = vuut1 n nX j=1 (yj − ˆyj)2 (6) The prediction root mean squared error (RMSE) is used as prediction error ( PE) for both SH and NBH anomaly detection algorithms. RMSE measures the square root of the average of squared differences between the prediction (ˆy) and the actual observation ( y) (see equation (6)). Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. By penalizing large errors, the RMSE value increases with the variance of the frequency distribution of error magnitudes. Taking RMSE as prediction error to calculate the threshold, allows to mitigate the detection of legitimate consumption increases as anomalous. The prediction models are retrained when benign datasets are large enough in terms of in- stances. Periodic retraining allows CPADF to adapt to consumption pattern changes related to nonmalicious factors such as changes of residents or appli- ances, etc. Pseudocodes of the SH anomaly detection algorithm is provided in Algorithm 1. At the SH level, the periodic consumption monitoring is set to 1 hour, because humans typically operate on hour interval, therefore it is difficult to notice pattern change over a smaller time interval. However, consumption pattern changes of a large number of SHs even over a shorter period of time results in drastic consumption pattern change of the whole NBH. There- fore, at the NBH level, the periodic consumption monitoring is set to the smart meter data collection frequency (30 minutes) to provide the minimal detection delay. The NBH total electricity is measured by the transformer meter. After calculating and preprocessing the NBH attributes vector, this latter is given to the NBH regression model to predict consumption. If the received consumption is larger than the sum of the predicted consumption and the prediction error (RMSE), then a Neighborhood Abnormal Consump- tion Raise (NACR) is detected, the suspect sample is stored, and an alert is sent to the decision maker. Otherwise, the processed sample is added to the 17Algorithm 1:SH Anomaly detection algorithm 1 BEGIN 2 Input: SHmeter (SH metering data) ; 3 Output: anomaly (boolean); 4 Variables: t (time interval), P redSH (SH prediction model), SHCons (SH observed consumption), counter (number of times an increase is detected),SHPE (prediction error), Nbrincr (threshold of successive consumption increase), BDSH (SH benign dataset) ; 5 for t ∈ {1, ...,24} do 6 Calculate attributes vector NSSH from SHmeter 7 end for 8 SHPC = P redSH (NSSH ); 9 if ( SHCons > SHPC + SHPE ) then 10 if (counter > Nbrincr) during the last N time intervals) then 11 anomaly=true ; 12 Send alert to decision maker ; 13 Transfer the suspect sample to the data concentrator ; 14 else 15 counter++; 16 Add NSSH to BDSH ; 17 end if 18 else 19 Add NSSH to BDSH ; 20 end if 21 Periodic transfer of BDSH to the data concentrator ; 22 END benign dataset ( BDNBH ) for the periodic retraining of NBH consumption profile. Pseudocodes of the NBH anomaly detection algorithm is provided in Algorithm 2. The decision maker module confirms the anomaly and notifies the op- erator in two cases: 1) more than half of SH anomaly detection systems in the NBH report an anomaly; 2) when a neighborhood abnormal consumption raise is reported. The first case corresponds to bill reduction cyberattack, the second one corresponds to forming peak energy load cyberattack. Then, the utility headend checks whether the detected anomaly is caused by a cyberat- 18Algorithm 2:NBH Anomaly Detection Algorithm 1 BEGIN 2 Input: NBH meter (NBH metering data) 3 Output: NACR (boolean) 4 Variables: t (time interval), P redNBH (NBH prediction model),NBH PE (prediction error), NBH Cons (NBH observed consumption), BDNBH (NBH benign Dataset) 5 for t ∈ {1, ...,48} do 6 Calculate attributes vector NSNBH from NBH meter ; 7 NBH PC = P redNBH (NSNBH ); 8 if (NBH Cons > NBHPC + NBH PE ) then 9 NACR=true ; 10 Send alert to the decision maker ; 11 Store the suspect sample ; 12 else 13 NACR=false ; 14 Add NSNBH to BDNBH ; 15 end if 16 end for 17 Periodic training of P redNBH ; 18 END tack or it is related to a temporary pattern change such as special occasions. After decision, the appropriate response is triggered, the attack or normal samples are stored into either attack or benign datasets. Initially, attack datasets are empty unless external sources are used. Malicious samples clas- sified by the decision maker will be added to the attack datasets. Once the two datasets are large enough, they will be used to build new classifiers for SH and NBH. These classifiers will constitute a second detection level and a decision support system for the utility headend. This approach allows for overcoming issues related to using synthetic malicious samples to train the classifiers. If NACR was detected, but no anomalies have been reported, it appears that an attack might be occurring but the SH anomaly detection system cannot recognize it. In this situation, the SH dataset is analyzed for sign of gradual overloading cyberattack, in which the attacker gradually in- creases the consumption data to mislead the learning machine to consider a 19malicious pattern as a normal one. The long-term tendency in daily consump- tion (historical data) of the smart home is analyzed. A gradual overloading can be characterized by an ascending slope in long-term consumption curve. Pseudocodes of the decision making algorithm is provided in Algorithm 3. Algorithm 3:Decision Making Algorithm 1 BEGIN 2 Input: NACR (boolean), Nbalert 3 Output: attack (boolean) 4 Variables: NbSH (number of SH in the neighborhood) 5 for t ∈ {1, ...,48} do 6 if (NACR == true)||(Nbalert > 1 2 ∗ NbSH ) then 7 attack=true ; 8 Send an alert to the operator ; 9 else 10 Add NSNBH /NSSH to BDNBH /BDSH ; 11 end if 12 end for 13 if (attack is confirmed) then 14 Add NSNBH /NSSH to the attack datasets ; 15 Initiate a response ; 16 else 17 Add NSNBH /NSSH to BDNBH /BDSH ; 18 end if 19 END 6. Experimental results We used in our experimentation the smart meter energy consumption data from the Irish Smart Energy Trial [19], the dataset was released by SEAI in January 2012. The dataset has been created within Smart Me- tering Electricity Customer Behaviour Trials (CBTs) which has taken place from 2009 to 2010. The purpose of the trials was to assess the impact on consumer’s electricity consumption in order to inform the cost-benefit anal- ysis for a national rollout. The dataset contains the energy consumption data of over 5000 residential households and businesses [19]. The dataset is 20Table 2: Raw data file format Meter ID Encoded date/time Energy Consumption (KWh) 1392 19535 0.256 1392 19538 0.265 1951 19604 0.042 1951 19605 0.021 constituted of six data files with millions of entries per file. Each data file contains three columns, the first column indicates the smart meter ID which identifies a particular resident or business. The second column represents timestamps corresponding to the time and date of the meter reading. Digits 1-3 represents the day code (day 1 = 1st January 2009), time code is repre- sented by digits 4-5 (1-48 for each 30 minutes with 1= 00:00:00 – 00:29:59). The third column indicates the energy consumption value in kilowatt-hours (kWh). Table 2 shows a small sample of the raw data. 6.1. Datasets generation and preprocessing The raw dataset includes the energy consumption data of all customers. To model each customer’s consumption pattern separately, the raw consump- tion data are split by meter ID into a collection of consumption datasets. For each customer dataset, a set of attributes are generated. Each vector in the new dataset includes the following attributes: SH consumption per hour; hour (1, ..., 24); day type (weekday or weekend); month and season. Among four consecutive weeks one week is randomly chosen for the valida- tion set and the other 3 weeks for the training set. Thus we use 75 % of the dataset for training and 25 % for validation. Likewise, from the raw data the NBH dataset is generated. Each vector in NBH dataset consists of the half hourly consumption of the whole NBH (the summation of meter reading of all customers within the neighborhood) and the same attributes used for SH datasets: hour, day type, day period, month and season. Among four consecutive weeks one week is randomly chosen for the validation set and the other 3 weeks for the training set. Data preprocessing includes operations such as cleaning and normaliza- tion. The cleaning task consists in identifying missing values and eliminating outliers and extreme values. Outliers and extreme values such as peak energy consumption may correspond to unusual activities such as holidays or special occasions. The mean and standard deviation σ for each time interval within 21Figure 5: Datasets generation process each month is calculated. All consumption values that do not lie within three σ of the mean are removed from the dataset. Figure 5 summarizes the datasets generation process. As indicated previously, different time intervals have been used for SH and NBH datasets because at SH level it is difficult to notice pattern change over a small time interval, humans typically operate on hour interval. However, at NBH level the pattern change can be noticed. 6.2. Energy consumption prediction We have used Weka [30] in our experiment, it is a collection of open source machine learning algorithms for data mining tasks. The performance of each of the five algorithms discussed in the previous section is measured in terms of the following metrics: 1. Mean Absolute Error (MAE): measures the average absolute differ- ences between the predicted value and the actual value in the validation dataset (see Equation (7)). 2. Root Mean Squared Error (RMSE): measures the square root of the av- erage of squared differences between the predicted value and the actual value in the validation dataset (explained previously) (6)). 3. Running time (in seconds): the time taken to build the model 4. Model size (KB): the size of the prediction model in kilobytes 22Table 3: Performances of regression algorithms on SHs data Algorithm MAE RMSE Running Time (s) Model Size (KB) REPTree 0.395 0.534 0.123 9.979 M5P 0.329 0.453 0.724 13.273 RandomForest 0.403 0.549 3.366 3,810.904 SVM 0.372 0.508 6.132 110.735 MLP 0.426 0.562 17.263 16.286 Table 4: Performances of regression algorithms on NBH data Algorithm MAE RMSE Running Time (s) Model Size (KB) REPTree 351,803 478,770 5,000 113,000 M5P 353,312 480,546 21,000 86,000 Random Forest 350,087 476,013 59,000 11386,000 SVM 585,336 773,824 12832,000 2709,000 MLP 451,850 582,765 246,000 15,000 MAE = 1 n nX j=1 |yj − ˆyj \f\f (7) We can see from the results of the energy consumption prediction of 500 customers in Table 3, that M5P algorithm gives the smallest average error rate within a reasonable running time and with low memory requirement. Therefore, M5P constitutes the best algorithm to use for SH energy pre- diction. The MLP algorithm shows the highest error rates and the longest running time. The random forest presents a huge memory usage in com- parison with the other algorithms. Concerning the NBH energy prediction, table 4 shows that REPTree algorithm provides the best trade-off between error rates, running time and memory requirement. Therefore, we choose REPTree algorithm for NHB energy prediction, and M5P algorithm for SH energy prediction. 6.3. Overloading cyberattacks detection To the best of our knowledge, no real smart grid AMI transaction dataset including overloading cyberattacks data is publicly available. Thus, we sim- 23ulate the power overloading cyberattacks against ILC/DLC discussed in Sec- tion 5 for 500 customers. We implement theses attacks based on datasets of normal samples, for each instance of the dataset we generate four types of malicious samples as follows (refer to table 5 for variable description): 1. Attack of type 1: this attack simulates forming peak energy load, where the attacker attempts to overload the grid during times of high demand when the grid becomes under pressure: M1(e) = e + αt αt = \u001a random(0.8, 4), P eakstart ≤ t ≤ P eakend 1 otherwise P eak hours: {7 − 9}{19 − 22} Where e is the normal consumption value and M is the modified con- sumption value. 2. Attack of type 2: this attack simulates bill reduction cyberattack, where the attacker manipulates the guideline price with a low price from timestart to timeend to urge customers in the community to sched- ule energy during this period, and a high price at other time slots during which he can schedule his energy load. Thus, the energy load increases from timestart to timeend. M2(e) = βt + e βt = \u001a (0.8, 4), timestart ≤ t ≤ timeend 1 otherwise timestart = random(0, 23 − min OffT ime) duration = random(min OffT ime,24) timeend = timeend + duration min OffT ime= 4; 3. Attack of type 3: to ensure a higher impact, the attacker may attempt to create a sharp energy increase. This attack simulates a variant of forming peak energy cyberattack. In this case, the amount of energy increase is greater than in the case of attack of type 1. 4. Attack of type 4: this attack simulates increasing load fluctuation cy- berattack. The attacker alternates repeatedly between normal behavior and grid overloading to disturb the grid. 240 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Time (hour) 0 1 2 3 4Consumption (KW) (a) Normal consumption pattern 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Time (Hour) 0 1 2 3 4 5 6 7Consumption (KW) Attack Type 1 Attack Type 2 Attack Type 3 Attack Type 4 (b) Attack consumption patterns Figure 6: A sample one-day period of energy consumption Figure 6a shows an example of the energy consumption of a particular cus- tomer during a single day. Figure 6b illustrates the corresponding attack patterns. We simulated the same four attack types on the neighborhood dataset, we implemented the attacks based on NBH dataset of normal samples. For each instance of the NBH dataset, we generated four types of malicious samples in the same way as described previously. We adjust the amount of energy consumption increase based on the neighborhood average consumption. The performance of the anomaly detection algorithms is measured in terms of the following metrics: Accuracy (AC); True Positive rate (TPR); False Positive Rate (FPR); True Negative Rate (TNR); and False Negative Rate (FNR). As we can see in Table 6, the average RMSE on SH attack samples (RMSE- A) deviates considerably from the average RMSE on SH normal samples (RMSE) regardless of the attack type. The deviation is more considerable in the case of attack of types 3 and 4 because the amount of energy increase is more important. The SH anomaly detection algorithm shows high detection performances. It delivers high accuracy and detection rate with low false 25Table 5: Variables description Variables Descriptions M1 Malicious consumption pattern generated through attack of type 1 M2 Malicious consumption pattern generated through attack of type 2 e Normal consumption P eakstart starting time of peak hours, e.g. 7 am P eakend ending time of peak hours, e.g., 10 pm αt the amount of electricity increase in attack of type 1 βt the amount of electricity increase in attack of type 2 timestart attack of type 2 starting time timeend attack of type 2 ending time minOffTime attack of type 2 minimal duration, e.g. 4 hours positive and false negative rates. We observe the best detection rate on attack of type 4, and the lowest false positive rate on attack of type 1. To highlight the trade-off between TPR and FPR, we relied on the Re- ceiver Operator Characteristic (ROC) curve which plots the TPR (y-axis) against the FPR (x-axis). Figure 7 shows the ROC curves of three cus- tomers with best, intermediate, and worst performances of attack of types 1, 2, 3, 4, and all the attack types combined, respectively. As we can notice, the curves are closer to the top-left corner indicating a good performance on detecting the four attack types combined or separated. The ROC curve 7d confirms that CPADF delivers the best detection performances on attack of type 4. Figure 7e shows the capacity of CPADF to maintain high detection rate with low false positive rate against all attack types combined. A sum- mary of the NBH anomaly detection results is listed in Table 7, the NBH detection algorithm shows high detection performance. We observe the best detection rate on attack of type 3, and the lowest false positive rate on attack of type 1. 6.4. Discussion and comparison The CPADF shows high accuracy on detecting the four attack types com- bined or separated, at both SH and NBH levels. However, in the context of smart grids, the two classes (attack and normal) are not equally important. It is known that TPR would be the metric to use when there is a high impact associated with false negative (attack classified as normal). It is safer for the system to tolerate false positive (normal consumption change detected as attack) rather than false negative. The impact of false negative would be 260 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 False Positive Rate 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Detection Rate Best Average Worst (a) Attack of type 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 False Positive Rate 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Detection Rate Best Average Worst (b) Attack of type 2 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 False Positive Rate 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Detection Rate Best Average Worst (c) Attack of type 3 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 False Positive Rate 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Detection Rate Best Average Worst (d) Attack of type 4 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 False Positive Rate 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Detection Rate Best Average Worst (e) All attacks Figure 7: ROC curves of SH anomaly detection system extremely high if the target system is connected to other systems (cascading failures). Against all attacks combined, the TPR is higher than 96 %, and the FNR is less than 4 %. The superior TPR on detecting attack types 3 27Table 6: SHs Anomaly Detection. RMSE RMSE-A AC TPR FPR TNR FNR Type 1 0.332 0.965 0.901 0.918 0.104 0.896 0.082 Type 2 0.332 0.942 0.903 0.920 0.111 0.889 0.080 Type 3 0.332 2.625 0.924 0.989 0.120 0.880 0.011 Type 4 0.332 2.617 0.947 0.991 0.120 0.880 0.009 All 0.332 2.673 0.907 0.965 0.161 0.839 0.035 Table 7: NBH Anomaly Detection RMSE RMSE-A AC TPR FPR TNR FNR Type 1 478.770 1168.923 0.890 0.882 0.108 0.892 0.118 Type 2 478.770 1130.237 0.893 0.909 0.121 0.879 0.091 Type 3 478.770 2678.249 0.899 0.998 0.168 0.832 0.002 Type 4 478.770 2687.301 0.931 0.993 0.168 0.832 0.007 All 478.770 2923.112 0.901 0.958 0.166 0.834 0.042 and 4 shows the effectiveness of CPADF when there are drastic changes in consumption patterns, as illustrated in Figure 8. The highest FNR is no- ticed on attack of type 1, due the fact that during peak hours, differentiating between legitimate and malicious consumption increase is more challenging. Furthermore, this is in part because the random generation of the amount of energy increase can in some cases return consumption values which are close to normal consumption values. Due the aforementioned facts, a slight drop in TPR at NBH level can be observed in the cases of attack types 1 and 2 (see Figure 8). The results showed the effectiveness and high performances of CPADF on detecting different types of overloading cyberattacks at SH and NBH levels. According to [25] an anomaly-based detection system must fulfil a set of requirements to be suitable to the smart grid context: operational perfor- mance ([R1]); reliability and integrity in the control ([R2]); resilience ([R3]); security ([R4]) and privacy ([R5]). CPADF complies with security and re- silience requirements ([R3, R4, R5]) thanks to the periodic retraining ensur- ing incremental learning to update the knowledge of the system with new le- gitimate consumption patterns. Using RMSE in threshold calculation allows 28controlling subtle changes, while two-level monitoring (home and neighbor- hood) of the consumption load enables controlling drastic changes in elec- tricity consumption and load demand. The decision trees low computational complexity and fast learning, along with their comprehensible outputs to hu- mans [25], makes CPADF meets the operational requirement ([R1]). Further- more, CPADF two-level monitoring, accuracy and low false positive/negative rate allows understanding the electricity consumption changes so as to act accordingly ([R2]). Type 1 Type 2 Type 3 Type 4 All0.8 0.82 0.84 0.86 0.88 0.9 0.92 0.94 0.96 0.98 1 1.02 0.91 0.92 0.98 0.99 0.97 0.88 0.9 0.99 0.99 0.95 SH NBH 1 Figure 8: Comparison of TPR among attack types Table 8: Comparison among anomaly detection systems Jokar et al. [10]Ford et al. [17] Cody et al. [18] CPADF HD (%) 70 68.75 NA 79.4 DR (%) 86 93.75 NA 96 FPR (%) 16 25 NA 16.6 RMSE NA 0.33 0.47 0.29 Anomaly Type Energy theft Grid overloading Energy theft Grid overloading Energy theft Grid overloading Load fluctuation To the best of our knowledge, there are only three papers [17, 18, 10] which have used the same dataset [19] for AMI anomaly detection. Ford et al. 29Experiment1 Experiment2 Experiment30 0.1 0.2 0.3 0.4 0.5 0.48 0.47 0.47 0.34 0.34 0.32 0.27 0.31 0.27 Cody et al.[18] Ford et al.[17] CPADF 4 Figure 9: Comparison of prediction error between CPADF and the state of the art. [17] and Cody et al. [18] used neural network and decision tree, receptively, to detect two types of energy fraud, whereas Jokar et al [10] used SVM based classification to detect energy theft. Since the performance of anomaly detection depends on the accuracy of energy prediction, we first compare the energy prediction performance of CPADF with Ford et al. [17] and Cody et al. [18]. For the sake of fairness, we consider the same experiments used in [17, 18]. The aim of the first experiment is to evaluate to which extent the regression model can predict electricity consumption for the same month a year after the training set, as in [17, 18], we exploited August 2009 for training, and August 2010 for validation. Experiment 2 examines the ability to predict electricity consumption the week following several weeks, as in [17, 18] we considered weeks from September. To evaluate the ability of electricity prediction within the same weather season, experiment 3 uses electricity consumption from June 2010 for training, then validated the model on July of the same year. The three experiment results are presented in Figure 9, as we can notice CPADF provides the lowest root mean squared error for the three experiments. Table 8 displays a comparison between the anomaly detection overall performances of CPADF, [17], [18] and [10]. As we can see, CPADF provides 30the best detection rate and the lowest prediction error (RMSE), however CPADF presents 0.6% of extra FPR in comparison with [10]. The proposed system in [10] uses synthetic malicious samples to build the system, which may cause FNR to increase when the malicious pattern changes, because the classifier would not detect attack types that deviate significantly from the synthetic malicious samples used to train the system. 7. Conclusion In this paper, grid overloading cyberattacks in the context of smart grid AMI are considered. These cyberattacks aim at increasing the energy us- age and load fluctuation to disturb the power grid and cause a large area blackout. After analyzing them, CPADF a distributed anomaly detection system based on regression decision trees is proposed. CPADF relies on the predictability of smart home and neighborhood consumption patterns. We showed that CPADF can detect grid overloading cyberattacks regardless of the strategy employed by the attacker and with an optimal detection delay. The simulation results on a real dataset of 500 customers demonstrate that CPADF provides a high detection rate and a low false positive rate with short running time and memory requirement. As future work, we need to explore more cyberattacks and to improve the anomaly detection algorithm using more sophisticated machine learning methods. 31References [1] P. Jokar, N. Arianpoo, V. C. Leung, Intrusion detection in advanced metering infrastructure based on consumption pattern, in: 2013 IEEE International Conference on Communications (ICC), IEEE, 2013, pp. 4472–4476. doi:10.1109/ICC.2013.6655271. URL http://ieeexplore.ieee.org/document/6655271/ [2] Y. Liu, S. Hu, T.-Y. Ho, Vulnerability assessment and defense tech- nology for smart home cybersecurity considering pricing cyberattacks, in: 2014 IEEE/ACM International Conference on Computer-Aided De- sign (ICCAD), IEEE, 2014, pp. 183–190. doi:10.1109/ICCAD.2014. 7001350. URL http://ieeexplore.ieee.org/document/7001350/ [3] Y. Liu, S. Hu, T.-Y. Ho, Leveraging Strategic Detection Techniques for Smart Home Pricing Cyberattacks, IEEE Transactions on Depend- able and Secure Computing 13 (2) (2016) 220–235. doi:10.1109/TDSC. 2015.2427841. URL http://ieeexplore.ieee.org/document/7115920/ [4] Y. Liu, S. Hu, A. Y. Zomaya, The Hierarchical Smart Home Cyberattack Detection Considering Power Overloading and Frequency Disturbance, IEEE Transactions on Industrial Informatics 12 (5) (2016) 1973–1983. doi:10.1109/TII.2016.2591911. URL http://ieeexplore.ieee.org/document/7515006/ [5] C. Alcaraz, J. Lopez, Wasam: A dynamic wide-area situational awareness model for critical domains in smart grids, Future Gen- eration Computer Systems 30 (2014) 146 – 154, special Issue on Extreme Scale Parallel Architectures and Systems, Cryp- tography in Cloud Computing and Recent Advances in Par- allel and Distributed Systems, ICPADS 2012 Selected Papers. doi:https://doi.org/10.1016/j.future.2013.06.030. URL http://www.sciencedirect.com/science/article/pii/ S0167739X13001441 [6] J. Giraldo, A. C´ ardenas, N. Quijano, Integrity attacks on real-time pric- ing in smart grids: Impact and countermeasures, IEEE Transactions on Smart Grid 8 (5) (2017) 2249–2257. doi:10.1109/TSG.2016.2521339. 32[7] S. A. Foroutan, F. R. Salmasi, Detection of false data injection attacks against state estimation in smart grids based on a mixture gaussian dis- tribution learning method, IET Cyber-Physical Systems: Theory Ap- plications 2 (4) (2017) 161–171. doi:10.1049/iet-cps.2017.0013. [8] Y. Zhou, Y. Liu, S. Hu, Smart home cyberattack detection framework for sponsor incentive attacks, IEEE Transactions on Smart Grid (2017) 1–1doi:10.1109/TSG.2017.2781695. [9] Y. Zhou, X. Chen, A. Y. Zomaya, L. Wang, S. Hu, A dynamic program- ming algorithm for leveraging probabilistic detection of energy theft in smart home, IEEE Transactions on Emerging Topics in Computing 3 (4) (2015) 502–513. doi:10.1109/TETC.2015.2484841. [10] P. Jokar, N. Arianpoo, V. C. M. Leung, Electricity Theft Detection in AMI Using Customers’ Consumption Patterns, IEEE Transactions on Smart Grid 7 (1) (2016) 216–226. doi:10.1109/TSG.2015.2425222. URL http://ieeexplore.ieee.org/document/7108042/ [11] Y. Liu, S. Hu, Cyberthreat analysis and detection for energy theft in social networking of smart homes, IEEE Transactions on Computa- tional Social Systems 2 (4) (2015) 148–158. doi:10.1109/TCSS.2016. 2519506. [12] S.-C. Yip, K. Wong, W.-P. Hew, M.-T. Gan, R. C.-W. Phan, S.-W. Tan, Detection of energy theft and defective smart me- ters in smart grids using linear regression, International Jour- nal of Electrical Power & Energy Systems 91 (2017) 230–240. doi:https://doi.org/10.1016/j.ijepes.2017.04.005. URL http://www.sciencedirect.com/science/article/pii/ S0142061516316386 [13] S. Amin, G. A. Schwartz, A. A. Cardenas, S. S. Sastry, Game-theoretic models of electricity theft detection in smart utility networks: Providing new capabilities with advanced metering infrastructure, IEEE Control Systems 35 (1) (2015) 66–81. doi:10.1109/MCS.2014.2364711. [14] M. Zanetti, E. Jamhour, M. Pellenz, M. Penna, V. Zambenedetti, I. Chueiri, A tunable fraud detection system for advanced metering in- frastructure using short-lived patterns, IEEE Transactions on Smart Grid (2017) 1–1doi:10.1109/TSG.2017.2753738. 33[15] S.-C. Yip, W.-N. Tan, C. Tan, M.-T. Gan, K. Wong, An anomaly de- tection framework for identifying energy theft and defective meters in smart grids, International Journal of Electrical Power & Energy Sys- tems 101 (2018) 189 – 203. doi:https://doi.org/10.1016/j.ijepes. 2018.03.025. [16] S.-C. Yip, K. Wong, W.-P. Hew, M.-T. Gan, R. C.-W. Phan, S.-W. Tan, Detection of energy theft and defective smart me- ters in smart grids using linear regression, International Jour- nal of Electrical Power & Energy Systems 91 (2017) 230 – 240. doi:https://doi.org/10.1016/j.ijepes.2017.04.005. URL http://www.sciencedirect.com/science/article/pii/ S0142061516316386 [17] V. Ford, A. Siraj, W. Eberle, Smart grid energy fraud detection using artificial neural networks, in: 2014 IEEE Symposium on Computational Intelligence Applications in Smart Grid (CIASG), 2014, pp. 1–6. doi: 10.1109/CIASG.2014.7011557. [18] C. Cody, V. Ford, A. Siraj, Decision tree learning for fraud detection in consumer energy consumption, in: 2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA), 2015, pp. 1175–1179. doi:10.1109/ICMLA.2015.80. [19] Irish social science data archive. [online]. available:. URL http://www.ucd.ie/issda/data/ commissionforenergyregulationcer/ [20] M. A. Faisal, Z. Aung, J. R. Williams, A. Sanchez, Data-Stream-Based Intrusion Detection System for Advanced Metering Infrastructure in Smart Grid: A Feasibility Study, IEEE Systems Journal 9 (1) (2015) 31–44. doi:10.1109/JSYST.2013.2294120. URL http://ieeexplore.ieee.org/document/6720175/ [21] Y. Zhang, L. Wang, W. Sun, R. C. Green II, M. Alam, Distributed Intrusion Detection System in a Multi-Layer Network Architecture of Smart Grids, IEEE Transactions on Smart Grid 2 (4) (2011) 796–808. doi:10.1109/TSG.2011.2159818. URL http://ieeexplore.ieee.org/document/5963752/ 34[22] Kdd cup 1999 data, 1999. [online]. available:. URL http://kdd.ics.uci.edu/databases/kddcup99/kddcup99. html [23] Y. Guo, C.-W. Ten, S. Hu, W. W. Weaver, Preventive Maintenance for Advanced Metering Infrastructure Against Malware Propagation, IEEE Transactions on Smart Grid 7 (3) (2016) 1314–1328.doi:10.1109/TSG. 2015.2453342. URL http://ieeexplore.ieee.org/document/7182779/ [24] C. Alcaraz, J. Lopez, S. Wolthusen, Ocpp protocol: Security threats and challenges, IEEE Transactions on Smart Grid 8 (5) (2017) 2452–2459 . [25] C. Alcaraz, L. Cazorla, G. Fernandez, Context-awareness using anomaly-based detectors for smart grid domains, in: J. Lopez, I. Ray, B. Crispo (Eds.), Risks and Security of Internet and Systems, Springer International Publishing, Cham, 2015, pp. 17–34. [26] N. Komninos, E. Philippou, A. Pitsillides, Survey in Smart Grid and Smart Home Security: Issues, Challenges and Countermeasures, IEEE Communications Surveys & Tutorials 16 (4) (2014) 1933–1954. doi:10.1109/COMST.2014.2320093. URL http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm? arnumber=6805165 [27] I. H. Witten, E. Frank, M. a. Hall, Data Mining: Practical Ma- chine Learning Tools and Techniques, Third Edition, Vol. 54, 2011. arXiv:arXiv:1011.1669v3, doi:10.1002/1521-3773(20010316)40: 6<9823::AID-ANIE9823>3.3.CO;2-C . URL http://www.cs.waikato.ac.nz/{~}ml/ weka/book.html{%}5Cnhttp://www.amazon.com/ Data-Mining-Practical-Techniques-Management/dp/0123748569 [28] L. Breiman, Random forests, Machine Learning 45 (1) (2001) 5–32. doi: 10.1023/A:1010933404324. URL https://doi.org/10.1023/A:1010933404324 [29] M. Suganyadevi, C. Babulal, Support Vector Regression Model for the prediction of Loadability Margin of a Power System, Applied Soft Computing 24 (2014) 304–315. doi:10.1016/j.asoc.2014.07.015. 35URL http://linkinghub.elsevier.com/retrieve/pii/ S1568494614003469 [30] The university of waikato, “weka 3: Data mining software in java” cs.waikato.ac.nz. [online]. [accessed: February 3, 2018]. URL https://www.cs.waikato.ac.nz/ml/weka/citing.html 36",
      "references": [
        "Intrusion detection in advanced metering infrastructure based on consumption pattern",
        "Vulnerability assessment and defense technology for smart home cybersecurity considering pricing cyberattacks",
        "Leveraging Strategic Detection Techniques for Smart Home Pricing Cyberattacks",
        "The Hierarchical Smart Home Cyberattack Detection Considering Power Overloading and Frequency Disturbance",
        "Wasam: A dynamic wide-area situational awareness model for critical domains in smart grids",
        "Integrity attacks on real-time pricing in smart grids: Impact and countermeasures",
        "Detection of false data injection attacks against state estimation in smart grids based on a mixture gaussian distribution learning method",
        "Smart home cyberattack detection framework for sponsor incentive attacks",
        "A dynamic programming algorithm for leveraging probabilistic detection of energy theft in smart home",
        "Electricity Theft Detection in AMI Using Customers’ Consumption Patterns",
        "Cyberthreat analysis and detection for energy theft in social networking of smart homes",
        "Detection of energy theft and defective smart meters in smart grids using linear regression",
        "Game-theoretic models of electricity theft detection in smart utility networks: Providing new capabilities with advanced metering infrastructure",
        "A tunable fraud detection system for advanced metering infrastructure using short-lived patterns",
        "An anomaly detection framework for identifying energy theft and defective meters in smart grids",
        "Smart grid energy fraud detection using artificial neural networks",
        "Decision tree learning for fraud detection in consumer energy consumption",
        "Irish social science data archive. [online]. available:",
        "Data-Stream-Based Intrusion Detection System for Advanced Metering Infrastructure in Smart Grid: A Feasibility Study",
        "Distributed Intrusion Detection System in a Multi-Layer Network Architecture of Smart Grids",
        "Kdd cup 1999 data, 1999. [online]. available:",
        "Preventive Maintenance for Advanced Metering Infrastructure Against Malware Propagation",
        "Ocpp protocol: Security threats and challenges",
        "Context-awareness using anomaly-based detectors for smart grid domains",
        "Survey in Smart Grid and Smart Home Security: Issues, Challenges and Countermeasures",
        "Data Mining: Practical Machine Learning Tools and Techniques, Third Edition",
        "Random forests",
        "Support Vector Regression Model for the prediction of Loadability Margin of a Power System",
        "The university of waikato, “weka 3: Data mining software in java” cs.waikato.ac.nz. [online]. [accessed: February 3, 2018]."
      ],
      "meta_data": {
        "arxiv_id": "2407.03264v1",
        "authors": [
          "Abdelaziz Amara Korba",
          "Nouredine Tamani",
          "Yacine Ghamri-Doudane",
          "Nour El Islem karabadji"
        ],
        "published_date": "2024-07-03T16:52:23Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes a two-level anomaly detection framework (CPADF) to address power overloading cyberattacks in Smart Grid Advanced Metering Infrastructure (AMI). The primary goal is to detect and prevent grid overloading cyberattacks, which aim to disrupt load balance, cause blackouts, and damage infrastructure, regardless of the attacker's strategy. Key contributions include leveraging the regularity and predictability of energy consumption to build reference patterns for individual households and entire neighborhoods, enabling early and efficient detection of drastic changes in consumption patterns.",
        "methodology": "The CPADF framework utilizes regression decision trees for anomaly detection. It involves a two-level monitoring architecture: at the Smart Home (SH) level and the Neighborhood (NBH) level. For SH energy prediction, the M5P algorithm (combining decision trees and linear regression) is chosen due to its low error rate. For NBH energy prediction, the REPTree algorithm (a fast decision tree learner) is selected for its balance of error rates, running time, and memory requirements. The methodology includes features engineering (extracting attributes like time, day period, day type, month, and season from raw data) and continuous monitoring. Anomalies are detected by comparing observed consumption with predicted patterns using Root Mean Squared Error (RMSE) as a prediction error threshold, with periodic retraining of models to adapt to legitimate pattern changes.",
        "experimental_setup": "The framework was evaluated using a real-world, publicly available energy consumption dataset from the Irish Smart Energy Trial, comprising data from 500 customers in Ireland (2009-2010). Data preprocessing involved cleaning, outlier removal, and normalization. Datasets were generated for individual SHs and the aggregated NBH consumption. The data was split into 75% for training and 25% for validation. Four types of power overloading cyberattacks (forming peak energy load, bill reduction, sharp energy increase, and increasing load fluctuation) were simulated due to the unavailability of real attack data. Performance metrics included Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Running Time, Model Size, Accuracy (AC), True Positive Rate (TPR), False Positive Rate (FPR), True Negative Rate (TNR), and False Negative Rate (FNR).",
        "limitations": "A key limitation is the reliance on simulated power overloading cyberattacks due to the unavailability of real smart grid AMI transaction datasets containing such attack data. This means the system's performance is validated against synthetic attack scenarios. Additionally, the paper notes that differentiating between legitimate and malicious consumption increases can be challenging, particularly for certain attack types (e.g., attack type 1 during peak hours), which can lead to a slight drop in TPR at the NBH level. The random generation of energy increase amounts in simulations could sometimes result in values close to normal consumption, making detection harder.",
        "future_research_directions": "Future work includes exploring more diverse cyberattack scenarios beyond the four types simulated in this study. Additionally, there is a direction to improve the anomaly detection algorithm by incorporating more sophisticated machine learning methods to enhance its capabilities and robustness.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "AutoPSV: Automated Process-Supervised Verifier",
      "full_text": "Tensor Networks for Lattice Gauge Theories beyond one dimension: a Roadmap Giuseppe Magnifico ,1, 2, 3 Giovanni Cataldi ,3, 4, 5 Marco Rigobello ,3, 4, 5 Peter Majcen ,3, 4, 5 Daniel Jaschke ,3, 4, 5, 6 Pietro Silvi ,3, 4, 5 and Simone Montangero 3, 4, 5 1Dipartimento di Fisica, Universit` a di Bari, I-70126 Bari, Italy. 2Istituto Nazionale di Fisica Nucleare (INFN), Sezione di Bari, I-70125 Bari, Italy. 3Dipartimento di Fisica e Astronomia “G. Galilei”, Universit` a di Padova, I-35131 Padova, Italy. 4Padua Quantum Technologies Research Center, Universit` a degli Studi di Padova 5Istituto Nazionale di Fisica Nucleare (INFN), Sezione di Padova, I-35131 Padova, Italy. 6Institute for Complex Quantum Systems, Ulm University, D-89069 Ulm, Germany (Dated: July 4, 2024) Tensor network methods are a class of numerical tools and algorithms to study many-body quan- tum systems in and out of equilibrium, based on tailored variational wave functions. They have found significant applications in simulating lattice gauge theories approaching relevant problems in high-energy physics. Compared to Monte Carlo methods, they do not suffer from the sign prob- lem, allowing them to explore challenging regimes such as finite chemical potentials and real-time dynamics. Further development is required to tackle fundamental challenges, such as accessing con- tinuum limits or computations of large-scale quantum chromodynamics. In this work, we review the state-of-the-art of Tensor Network methods and discuss a possible roadmap for algorithmic develop- ment and strategies to enhance their capabilities and extend their applicability to open high-energy problems. We provide tailored estimates of the theoretical and computational resource scaling for attacking large-scale lattice gauge theories. Gauge theories play a role of paramount importance in our understanding and description of the fundamental constituents of matter, their spectrum, and their inter- actions. At low energies, gauge theories characterize a large variety of collective phases of matter and phenom- ena, such as ferromagnetic superconductivity, spin liq- uids, topological order, and the fractional quantum Hall effect [1–3]. At high energies, as elegantly summarised in the Standard Model of particle physics, they are at the heart of the microscopical description of the build- ing blocks of our universe, i.e. quarks, leptons and their interactions mediated by gauge bosons [4, 5]. A powerful approach to studying and simulating gauge theories in nonperturbative regimes lies in Lattice Gauge Theories (LGTs), in which the matter and the gauge de- grees of freedom are discretized and regularized on a fi- nite lattice. LGTs were originally introduced by Wilson to encode Quantum Chromodynamics (QCD) on a lat- tice, as a model for quark confinement beyond the per- turbative regime [6, 7]. In LGTs, matter and antimatter fermionic fields are defined on lattice sites, whereas the gauge fields live on the links connecting nearest-neighbor sites. This approach has opened the doors to the ap- plication of powerful numerical methods, such as Monte Carlo (MC), to the simulations of LGTs on classical com- puters [8]. In the last decades, MC methods have pro- vided a wide variety of significant results on LGTs in the context of high-energy physics, such as phases dia- grams at equilibrium, characterization of the quark-gluon plasma, precise determination of the masses of quarks, mesons, and baryons, hadronic and nucleon form factors, hadronic spectra, and predictions for dark-matter models [9]. Furthermore, MC simulations of LGTs currently rep- resent a powerful numerical tool to predict and interpret data from multiple large-scale high-energy experiments, such as the ones performed at the Large Hadron Collider (LHC). Despite their impressive success, MC sampling meth- ods are limited in some regimes of parameters of LGTs, such as in the presence of finite baryon chemical po- tentials, topological θ-terms, or for simulating out-of- equilibrium dynamics in real-time. In these cases, the no- torious sign problems make the MC numerical approach ineffective and inaccurate [10]. Ranging from these problems, sign-problem-free meth- ods based on Tensor Networks (TNs) have found signifi- cant applications in the simulations of LGTs in the last years [11]. TNs were originally introduced as a class of variational wave functions in the field of quantum many- body physics. They provide a compressed representation of physical states based on their entanglement content, capable of efficiently reproducing equilibrium properties, such as phase diagrams, and real-time dynamics of inter- acting quantum systems [12–14]. Nowadays, they repre- sent one of the state-of-the-art numerical tools for sim- ulating quantum many-body systems, even with strong correlations. In the context of high-energy physics, TN meth- ods have proven noteworthy achievements in simulating LTGs in (1+1) dimensions, for both Abelian and non- Abelian gauge groups [15–43]. They have recently found applications to Abelian LGTs up to (3+1) dimensions [44–49], and also in simulating (2+1)-dimensional non- Abelian SU(2) models [50]. Despite their effectiveness in these first important applications, further and inten- sive developments are still required to tackle, with TN methods, high-energy physics problems at the center of current research efforts, such as large-scale non-Abelian arXiv:2407.03058v1  [hep-lat]  3 Jul 20242 LGTs and their continuum limits. In this work, we present a general overview of TN methods for LGTs, and we discuss a possible roadmap in terms of algorithmic development and strategies to im- prove TN capabilities, toward the ambitious long-term goal of applying TNs to (3+1)-dimensional QCD. Importantly, TN methods share a common language with quantum computers and simulators. Thus, these developments could also be relevant for encoding, validat- ing, and benchmarking the current and future quantum computations and simulations of LGTs on experimental quantum hardware [51–61]. The paper is organized into sections as follows. In Sec. I, we give a general overview of TN methods, par- ticularly the computational complexity of the most used algorithms for ground state computations and real-time dynamics. In Sec. II, we introduce the main concepts related to LGTs and the main strategies for simulating them with TN methods. In Sec. III, we present a possible roadmap of algorithmic development and optimization strategies crucial for making the TN approach compet- itive as a complementary method to MC techniques for simulating challenging LGT models. In Sec. IV, we draw our conclusions. I. TENSOR NETWORKS OVERVIEW In this section, we present a general overview of TN methods with a particular focus on Tree Tensor Networks (TTNs), the latter being particularly useful for LTGs in higher dimensions [44, 46, 50]. However, this work’s main concepts and ideas can be easily generalized and applied to other TN structures. Consider a quantum many-body (QMB) system de- fined on a lattice of N sites. The generic site j is described by a d-dimensional local Hilbert space Hj, spanned by a local basis of vectors |i⟩1≤i≤d. The quan- tum states of the whole system live on the total Hilbert space H = H1 ⊗H2 ⊗···⊗H N , that is the tensor product of the local Hilbert space of the lattice sites, with dimen- sion dN . Then, any pure state of the system |ψ⟩ can be exactly expanded in terms of a complete basis set of H: |ψ⟩ = dX i1,i2,...,iN=1 ci1,i2,...,iN |i1, i2, ..., iN ⟩ , (1) where |i1, i2, ..., iN ⟩ represents the tensor product of the local basis vectors, i.e. |i1⟩⊗| i2⟩⊗| ...⟩⊗| iN ⟩. The coeffi- cients of the linear combination ci1,i2,...,iN are in general complex scalars; their number scales as dN , i.e. they scale exponentially with the system size N. This is a fundamental limitation when solving a QMB problem on a classical computer since an exponential scaling with the number of degrees of freedom implies that the exact representation described in Eq. (1) is com- pletely unfeasible from a computational and numerical point of view. Nonetheless, a great variety of natural QMB systems happens to be described by ground and thermal equi- librium states that own little-to-moderate entanglement content. The physical states of these systems, instead of exploring the exponentially large dimension of the Hilbert space, live in a small corner of it, which can be efficiently targeted and parameterized. This property is formally described by the entanglement area law, fulfilled by low-energy states of local Hamiltonians [62]: the en- tanglement between a partition of the system and the rest is proportional to the area of the boundary between them, instead of its volume, as happens for the majority of states in the Hilbert space. Thus, obeying area law implies that the state contains much fewer quantum cor- relations than expected for a generic (or random) QMB state. Small corrections to the area law exist for instance close to a transition point for one-dimensional quantum systems (logarithmic corrections). However, the entan- glement remains overall moderate [63]. From a theoret- ical point of view, the entanglement area law has been rigorously proven for (i) one-dimensional gapped local Hamiltonians, where the locality means that a lattice site interacts only with neighboring sites, without two-body all-to-all interactions [64–66]; (ii) for quantum states at thermal equilibrium, independently from the dimension- ality of the system [67]. Even though rigorous proof for QMB systems in higher dimensions is lacking, several nu- merical and phenomenological shreds of evidence suggest that area law still holds in the presence of local interac- tions [62, 68–71]. The area law has important implications on the TN simulation of quantum lattice models: indeed, it is possi- ble to obtain an approximate but efficient representation capable of describing the main properties of these states if the entanglement content is low-to-moderate [62]; the condition applies for example to ground-states and first excited states of local Hamiltonians. TNs give a natural language for this representation, by replacing the com- plete tensor of rank- N ci1,i2,...,iN of Eq. (1) with a chain of smaller tensors interconnected using auxiliary bond in- dices. The network keeps a number N of physical indices of dimension d (one for each lattice site), whereas the di- mension of the bond indices (called bond dimension) is a control parameter χ that can be tuned in the numerical simulations and is related to the Schmidt decomposition [18, 72]. The key advantage of passing from the exact rep- resentation of Eq. (1) to a TN representation is that the number of parameters in the TN is of the order O(poly(d)poly(N)poly(χ)), e.g., O(Nχ max(χ, d)2) for the TTN. The scaling with the system size is now poly- nomial and not exponential. In this way, we obtain an efficient representation of the quantum state in terms of computational complexity. It is worth noting that the bond dimension χ determines the degree of entanglement and quantum correlations encoded in the TN, e.g. for χ=1 the TN describes a product state (no entanglement), whereas one recovers the exact but inefficient representa-3 MPS  PEPS <latexit sha1_base64=\"evLsJgYPR1JhShTB3ZZqnA5vJ0g=\">AAAB83icbVDLSgNBEOz1GeMr6tHLYBA8hV1R9BjQg8cEzAOSJcxOOnHI7IOZHiGEfIFXPXkTr36QB//F3XUPmlinoqqbrq4gUdKQ6346K6tr6xubpa3y9s7u3n7l4LBtYqsFtkSsYt0NuEElI2yRJIXdRCMPA4WdYHKT+Z1H1EbG0T1NE/RDPo7kSApOqdQcDipVt+bmYMvEK0gVCjQGla/+MBY2xIiE4sb0PDchf8Y1SaFwXu5bgwkXEz7GXkojHqLxZ3nQOTu1hlPMEtRMKpaL+HtjxkNjpmGQToacHsyil4n/eT1Lo2t/JqPEEkYiO0RSYX7ICC3TBpANpUYiniVHJiMmuOZEqCXjQqSiTSspp314i98vk/Z5zbusuc2Lav22aKYEx3ACZ+DBFdThDhrQAgEIT/AML451Xp035/1ndMUpdo7gD5yPb2lJkXM=</latexit> d <latexit sha1_base64=\"I0OcdTLU7qoQvZ5LLRQkXA1hp+s=\">AAAB9nicbVC7TsNAEDyHVwivACXNiQiJKrIRCMpIUFAGiTykxIrWl01yyvmhuzUisvILtFDRIVp+h4J/wTYuIGGq0cyudna8SElDtv1plVZW19Y3ypuVre2d3b3q/kHbhLEW2BKhCnXXA4NKBtgiSQq7kUbwPYUdb3qd+Z0H1EaGwT3NInR9GAdyJAVQJvXFRA6qNbtu5+DLxClIjRVoDqpf/WEoYh8DEgqM6Tl2RG4CmqRQOK/0Y4MRiCmMsZfSAHw0bpJnnfOT2ACFPELNpeK5iL83EvCNmfleOukDTcyil4n/eb2YRlduIoMoJgxEdoikwvyQEVqmJSAfSo1EkCVHLgMuQAMRaslBiFSM01YqaR/O4vfLpH1Wdy7q9t15rXFTNFNmR+yYnTKHXbIGu2VN1mKCTdgTe2Yv1qP1ar1Z7z+jJavYOWR/YH18A64Qkr0=</latexit> \u0000 <latexit sha1_base64=\"evLsJgYPR1JhShTB3ZZqnA5vJ0g=\">AAAB83icbVDLSgNBEOz1GeMr6tHLYBA8hV1R9BjQg8cEzAOSJcxOOnHI7IOZHiGEfIFXPXkTr36QB//F3XUPmlinoqqbrq4gUdKQ6346K6tr6xubpa3y9s7u3n7l4LBtYqsFtkSsYt0NuEElI2yRJIXdRCMPA4WdYHKT+Z1H1EbG0T1NE/RDPo7kSApOqdQcDipVt+bmYMvEK0gVCjQGla/+MBY2xIiE4sb0PDchf8Y1SaFwXu5bgwkXEz7GXkojHqLxZ3nQOTu1hlPMEtRMKpaL+HtjxkNjpmGQToacHsyil4n/eT1Lo2t/JqPEEkYiO0RSYX7ICC3TBpANpUYiniVHJiMmuOZEqCXjQqSiTSspp314i98vk/Z5zbusuc2Lav22aKYEx3ACZ+DBFdThDhrQAgEIT/AML451Xp035/1ndMUpdo7gD5yPb2lJkXM=</latexit> d <latexit sha1_base64=\"I0OcdTLU7qoQvZ5LLRQkXA1hp+s=\">AAAB9nicbVC7TsNAEDyHVwivACXNiQiJKrIRCMpIUFAGiTykxIrWl01yyvmhuzUisvILtFDRIVp+h4J/wTYuIGGq0cyudna8SElDtv1plVZW19Y3ypuVre2d3b3q/kHbhLEW2BKhCnXXA4NKBtgiSQq7kUbwPYUdb3qd+Z0H1EaGwT3NInR9GAdyJAVQJvXFRA6qNbtu5+DLxClIjRVoDqpf/WEoYh8DEgqM6Tl2RG4CmqRQOK/0Y4MRiCmMsZfSAHw0bpJnnfOT2ACFPELNpeK5iL83EvCNmfleOukDTcyil4n/eb2YRlduIoMoJgxEdoikwvyQEVqmJSAfSo1EkCVHLgMuQAMRaslBiFSM01YqaR/O4vfLpH1Wdy7q9t15rXFTNFNmR+yYnTKHXbIGu2VN1mKCTdgTe2Yv1qP1ar1Z7z+jJavYOWR/YH18A64Qkr0=</latexit> \u0000 <latexit sha1_base64=\"evLsJgYPR1JhShTB3ZZqnA5vJ0g=\">AAAB83icbVDLSgNBEOz1GeMr6tHLYBA8hV1R9BjQg8cEzAOSJcxOOnHI7IOZHiGEfIFXPXkTr36QB//F3XUPmlinoqqbrq4gUdKQ6346K6tr6xubpa3y9s7u3n7l4LBtYqsFtkSsYt0NuEElI2yRJIXdRCMPA4WdYHKT+Z1H1EbG0T1NE/RDPo7kSApOqdQcDipVt+bmYMvEK0gVCjQGla/+MBY2xIiE4sb0PDchf8Y1SaFwXu5bgwkXEz7GXkojHqLxZ3nQOTu1hlPMEtRMKpaL+HtjxkNjpmGQToacHsyil4n/eT1Lo2t/JqPEEkYiO0RSYX7ICC3TBpANpUYiniVHJiMmuOZEqCXjQqSiTSspp314i98vk/Z5zbusuc2Lav22aKYEx3ACZ+DBFdThDhrQAgEIT/AML451Xp035/1ndMUpdo7gD5yPb2lJkXM=</latexit> d <latexit sha1_base64=\"I0OcdTLU7qoQvZ5LLRQkXA1hp+s=\">AAAB9nicbVC7TsNAEDyHVwivACXNiQiJKrIRCMpIUFAGiTykxIrWl01yyvmhuzUisvILtFDRIVp+h4J/wTYuIGGq0cyudna8SElDtv1plVZW19Y3ypuVre2d3b3q/kHbhLEW2BKhCnXXA4NKBtgiSQq7kUbwPYUdb3qd+Z0H1EaGwT3NInR9GAdyJAVQJvXFRA6qNbtu5+DLxClIjRVoDqpf/WEoYh8DEgqM6Tl2RG4CmqRQOK/0Y4MRiCmMsZfSAHw0bpJnnfOT2ACFPELNpeK5iL83EvCNmfleOukDTcyil4n/eb2YRlduIoMoJgxEdoikwvyQEVqmJSAfSo1EkCVHLgMuQAMRaslBiFSM01YqaR/O4vfLpH1Wdy7q9t15rXFTNFNmR+yYnTKHXbIGu2VN1mKCTdgTe2Yv1qP1ar1Z7z+jJavYOWR/YH18A64Qkr0=</latexit> \u0000 TTN 2D TTN <latexit sha1_base64=\"evLsJgYPR1JhShTB3ZZqnA5vJ0g=\">AAAB83icbVDLSgNBEOz1GeMr6tHLYBA8hV1R9BjQg8cEzAOSJcxOOnHI7IOZHiGEfIFXPXkTr36QB//F3XUPmlinoqqbrq4gUdKQ6346K6tr6xubpa3y9s7u3n7l4LBtYqsFtkSsYt0NuEElI2yRJIXdRCMPA4WdYHKT+Z1H1EbG0T1NE/RDPo7kSApOqdQcDipVt+bmYMvEK0gVCjQGla/+MBY2xIiE4sb0PDchf8Y1SaFwXu5bgwkXEz7GXkojHqLxZ3nQOTu1hlPMEtRMKpaL+HtjxkNjpmGQToacHsyil4n/eT1Lo2t/JqPEEkYiO0RSYX7ICC3TBpANpUYiniVHJiMmuOZEqCXjQqSiTSspp314i98vk/Z5zbusuc2Lav22aKYEx3ACZ+DBFdThDhrQAgEIT/AML451Xp035/1ndMUpdo7gD5yPb2lJkXM=</latexit> d <latexit sha1_base64=\"I0OcdTLU7qoQvZ5LLRQkXA1hp+s=\">AAAB9nicbVC7TsNAEDyHVwivACXNiQiJKrIRCMpIUFAGiTykxIrWl01yyvmhuzUisvILtFDRIVp+h4J/wTYuIGGq0cyudna8SElDtv1plVZW19Y3ypuVre2d3b3q/kHbhLEW2BKhCnXXA4NKBtgiSQq7kUbwPYUdb3qd+Z0H1EaGwT3NInR9GAdyJAVQJvXFRA6qNbtu5+DLxClIjRVoDqpf/WEoYh8DEgqM6Tl2RG4CmqRQOK/0Y4MRiCmMsZfSAHw0bpJnnfOT2ACFPELNpeK5iL83EvCNmfleOukDTcyil4n/eb2YRlduIoMoJgxEdoikwvyQEVqmJSAfSo1EkCVHLgMuQAMRaslBiFSM01YqaR/O4vfLpH1Wdy7q9t15rXFTNFNmR+yYnTKHXbIGu2VN1mKCTdgTe2Yv1qP1ar1Z7z+jJavYOWR/YH18A64Qkr0=</latexit> \u0000 (a) (b) (c) (d) FIG. 1. Examples of tensor network structures: (a) Matrix Product States (MPS); (b) Projected Entangled Pair States (PEPS); (c) Tree Tensor Networks (TTN) for an underlying one-dimensional system; (d) TTN for an underlying two-dimensional square lattice. The physical links with local dimension d and the virtual links with bond dimension χ are highlighted in all the figures. tion in the limit χ≲dN . Tuning χ properly allows inter- polating between these two extreme regimes, efficiently reproducing the entanglement of the quantum state. The most widely used TN architectures are represented in Fig. 1. Matrix product states (MPS) are an established ansatz for one-dimensional systems, in which the struc- ture of the lattice is reproduced with a network of ten- sors, one for each lattice site [73]. As shown in Fig. 1(a), each tensor in the bulk of the network has three indices: one physical leg of dimension d representing the local degrees of freedom, and two virtual legs of dimension χ connected to the neighboring sites. In open bound- ary conditions, the tensors at the boundaries have one trivial leg together with one physical and one virtual leg. MPS intrinsically satisfies area law and allows for efficient computation of scalar products between two states and physical observables. Currently, the MPS-based Den- sity Matrix Renormalization Group (DMRG) stands as one of the most consolidated and accurate techniques for numerical simulations of one-dimensional QMB systems as well as quasi-one-dimensional systems such as ladder structures [74]. The computational complexity of this ground state searching algorithm for MPS is of the or- der O(Ndχ3 + Nd2χ2) or O(Nd3χ3), depending if the algorithm optimizes a single MPS tensor at a time, or two-tensors simultaneously [74–76]. Two-site optimiza- tion is usually important to avoid getting stuck in local minima or meta-stable configurations during the energy variational minimization [77]. The subspace expansion is an intermediate approach with the benefits of the two- tensor update and a tunable computational cost in be- tween the two approaches [18]. The generalization of the MPS ansatz to two- or higher-dimensional lattices is represented by Projected Entangled Pair State (PEPS) [72]. In this case, each tensor in the bulk has a physical leg of dimension d, and a number of χ-dimensional virtual legs depending on the coordination number of the considered lattice. For ex- ample, this coordination number is four in the case of a two-dimensional square lattice, as shown in Fig. 1(b). PEPS directly encode in their structure the area law of entanglement, however, their exact contraction is an ex- ponentially hard problem, meaning that PEPS can not be efficiently contracted for numerical computing, e.g. scalar products of states or physical observables [78]. To circumvent this problem, approximate contraction meth- ods have been developed during the last years, and are still at the center of current research efforts [72]. But even with exploiting these approximate techniques, the com- putational complexity for ground state optimization re- mains quite high, e.g. of the order of O(Nd2χ8) [79, 80], limiting the maximum reachable bond dimensions (typi- cal values are of the order χ ≈ 10). Another important family of TN ans¨ atze is represented by Tree Tensor Networks (TTN), in which the wave func- tion is decomposed into a hierarchical network of tensors that do not contain internal loops [18, 81]. This way, the network can be efficiently contracted and manipu- lated in polynomial time. A particular class of TTN is represented by binary tree tensor networks, reported in Fig. 1(c)-(d) for one- and two-dimensional lattices. In these structures, tensors in the lowest layer have two physical legs of dimension d (representing two lattice sites) and a virtual leg of dimension χ, whereas, in the upper layers, they have three virtual legs of dimensions up to χ. The network intrinsically encodes a renor- malization procedure, in which, at each layer, two sites are mapped into a single effective one. In finite-range models, ground searching algorithms for binary TTN ar- chitectures display a numerical complexity of the order O(Nd2χ2+Nχ4), see [18, 76]. This is a much more favor- able scaling concerning equivalent algorithms for other TN structures, such as PEPS, that allows reaching quite large values of bond dimensions ( χ ≈ 500) [82]. The drawback of loopless structures, such as binary TTN, is that the area law may not be explicitly reproduced in di- mensions higher than one [83], which becomes a limiting factor when large systems are addressed. The conver- gence and the precision of the numerical results obtained via a variational optimization of TTN can be analyzed and kept under control, e.g. by exploiting the large range of available bond dimensions. Furthermore, it is possible to explicitly encode the area law of high dimensional sys- tems in the TTN ansatz by introducing an additional layer of independent disentanglers, acting on different couples of lattice sites and connected to the correspond-4 ing physical legs. This process augments the expressive power of TTN, and the resulting ansatz is known as aug- mented Tree Tensor Network (aTTN) [84]. The compu- tational complexity of variationally optimizing an aTTN structure, which means optimizing both the tensors and the disentanglers, is of the order O(Nχ4d4 + Nχd7). We point out that the scaling of the computational costs with the local dimension d is particularly severe in the case of aTTN due to the presence of the disentanglers layer. Besides variational optimization for ground state searching, the previous TN families can also be exploited to simulate the real-time dynamics of local Hamiltonians via at least four methods [85]. One of the most widely used approaches, the Time Evolved Block Decimation (TEBD) algorithm, is based on a Suzuki-Trotter decom- position of the time evolution exponential [86]. The to- tal evolution time is discretized in small time steps. The corresponding evolution operator is computed as prod- ucts of local terms, such as two-body operators, and re- peatedly applied to the TN wave function to generate the time-evolved state. Each application can determine an increase in the bond dimension of the network, so an optimized truncation is needed to maintain an effi- cient and manageable description of the quantum state. This truncation reduces the bond dimension back to χ and is performed through a singular value decomposi- tion that minimizes the distance between the evolved and the truncated state. In general, the TEBD method al- lows the simulation of the real-time dynamics for nearest- neighbor or finite-range Hamiltonians; one time-step with an MPS for a one-dimensional system with local interac- tions comes with a computational cost that is below a two-tensor sweep for the ground-state search algorithm. Another method for simulating the evolution of quan- tum states via TN is the Time-Dependent Variational Principle (TDVP), which does not rely on the Suzuki- Trotter decomposition [87, 88]. In general, TDVP con- strains the time evolution to the specific TN manifold considered, such as MPS or TTN, of a given initial bond dimension [89]. This is obtained by projecting the action of the Hamiltonian into the tangent space of the TN man- ifold and then solving the time-dependent Schr¨ odinger equation within this manifold. This approach automati- cally preserves the energy and the norm of the quantum states during the time evolution. The TDVP algorithm and the variational ground state search rely both on a set of Krylov vectors and therefore have the same computa- tional scaling for one time step compared to one sweep. These algorithms represent important and efficient tools for simulating with TN the real-time dynamics of QMB systems. While equilibrium states satisfy the aforementioned area law, out-of-equilibrium time evolu- tion can generate a linear growth of entanglement. In this case, the time-evolved state requires an exponential growth of the bond dimension as a function of the total time [90]. For this reason, TN methods are currently lim- ited to studying the dynamics for low-to-moderate times, or close-to-equilibrium phenomena [91]. In this frame- U αβ x,μ Lx,μ Rx,μ Gauge Fields x x + μ ψx,α Lx,μyRx−μy,μy Rx−μx,μx Lx,μxQx GAUSS LAW Gx α ∈ { … } Matter Fields Hyper-cubic spatial Lattice Λ a (c) LINK  SYMMETRY ψψ x x + μ (a) ψ ψ x + μx (b) RISHONS ζ ζ ψ ψ x + μx (d ) x + μx ker Gx GAUGE-SINGLETS FIG. 2. Graphical representation of the degrees of freedom of a 2D LGT: fermionic matter fields ˆψx,α, defined on lattice sites, and gauge fields (the parallel transporter, ˆUαβ x,µ, and the chromo-electric fields, ˆLν x,µ and ˆRν x,µ), living on lattice links. A local gauge transformation at x acts on a matter site and all its attached links. work, further developments are extremely important to avoid or at least mitigate this barrier, by devising new algorithms or optimizing existing strategies [92, 93]. II. TENSOR NETWORKS FOR HAMILTONIAN LATTICE GAUGE THEORIES In the traditional MC approach to LGT, the action of a continuum gauge theory is regularized by working on a fi- nite and discrete Euclidean spacetime — i.e., both space and (imaginary) time are discretized [8]. Instead, TN (and quantum) simulations typically rely on the Hamil- tonian formalism, where time remains a real, continuous variable while D-dimensional space is replaced by a cu- bic lattice Λ. Simultaneously, Hamiltonian LGTs present features that distinguish them from other lattice models commonly simulated via TN [7]. We now discuss these properties in more detail and outline the main steps that have to be taken to exploit TN algorithms in LGT. We fo- cus on matter-coupled LGTs of the Yang-Mills type, such as those routinely employed in high-energy physics to describe nature’s fundamental interactions; the paradig- matic example is lattice QCD, the SU(3) LGT describing quarks, gluons and their strong interactions. A. LGT building blocks As depicted in Fig. 2, LGTs involve two types of de- grees of degrees of freedom, matter and gauge, hosted respectively on lattice sites x ∈ Λ and links (x, µ), where µ denotes one of the lattice basis vectors. Matter fields — such as the electron or quark fields. As for these examples, we assume matter consists of Dirac fermions and employ staggered fermions [94] to tame the fermion doubling problem, i.e., the proliferation of propagating fermionic degrees of freedom on the lattice.5 Then, matter is represented by a multiplet ˆψx,α of anti- commuting fields, { ˆψx,α, ˆψ† y,β} = δαβδxy. Assuming a single matter flavor in the fundamental representation of the gauge group G, gauge transformations are generated by Qν x = P α,β ˆψ† x,αλν αβ ˆψx,β, where {λν}dim G ν=1 are the her- mitian generators of G — e.g., the electric charge for U(1) or the three spin matrices for SU(2). The generalization to multiple matter flavors is straightforward. Gauge fields — such as the photon or gluon fields. They are represented by bosonic operators, namely the parallel transporter ˆUαβ x,µ and the generators of its left and right gauge transformations, ˆLν x,µ and ˆRν x,µ, [ˆLν x,µ, ˆUαβ y,µ′ ] = −δxyδµµ′ X γ λν αγ ˆUγβ x,µ , (2a) [ ˆRν x,µ, ˆUαβ y,µ′ ] = + δxyδµµ′ X γ ˆUαγ x,µλν γβ ; (2b) In the Abelian U(1) case there is only one generator: the electric field, ˆEx,µ = ˆLx,µ = ˆRx,µ, for which parallel transporters act as raising operators, [ˆEx,µ, ˆUx,µ] = ˆUx,µ. In terms of the above ingredients, a possible discretiza- tion of a matter coupled Yang-Mills theory is defined by the Kogut-Susskind Hamiltonian [7] HLGT = − cℏ 2a X x,µ X α,β \u0010 sx,µ ˆψ† x,α ˆUαβ x,µ ˆψx+µ,β + h.c. \u0011 + mc2 X x X α sx ˆψ† x,α ˆψx,α + cℏg2 2aD−2 X x,µ ˆE2 x,µ − cℏ 2g2a4−D X □ Tr(ˆU□ + ˆU† □) , (3) where c is the speed of light, ℏ is the Plank constant, a is the lattice spacing, g is the gauge coupling, and m is the mass parameter. The first HLGT term describes matter hopping between neighboring lattice sites, medi- ated by the gauge field; the second term is the mass- energy; sx,µ and sx are phases that arise due to the use of staggered fermions. The last two terms represent the (chromo)electric and (chromo)magnetic energy of the gauge field, respectively. The electric energy density is given by the Casimir operator: ˆE2 x,µ = X ν (ˆLν x,µ)2 = X ν ( ˆRν x,µ)2 . (4) The magnetic energy density is a plaquette term that, on a cubic lattice, corresponds to a four-body interaction: ˆU□ = X α,β,γ,δ ˆUαβ x,µ ˆUβγ x+µ,µ′ ˆU†γδ x+µ′,µ ˆU†δα x,µ′ (5) where µ and µ′ span the plaquette’s plane. Plaquette terms only exist in D >1, contributing to the increased complexity of quantum and TN simulations of LGTs in higher dimensions [56]. B. Gauge field truncation The link Hilbert space is the space of square-integrable functions on the gauge group, L2(G), which is infinite- dimensional for a continuous G [95]. In one space dimension, gauge degrees of freedom are unphysical (absence of transverse polarizations) and can thus be integrated out, albeit at the price of introducing non-local interactions [96]. Beyond one dimension, the removal is much more delicate, because it requires first decoupling the gauge field’s longitudinal component [97]. When impossible or inconvenient to remove, gauge degrees of freedom might have to be truncated to perform TN or quan- tum simulation. Among known truncation recipes are Quantum Link Models (QLM) [98–101], which have been already adopted for quantum simulation of LGTs [52, 56, 58, 60, 102–106], finite subgroups [26, 30, 107], digitization of gauge fields [108], and fusion-algebra de- formation [109]. Another adopted solution is truncating the spectrum of the electric energy density operator \r\r\r ˆE2 x,µ \r\r\r ≤ Θ on each link [50, 110]. The cutoff is conveniently imposed in the irreducible representation (irrep) basis {|jmn⟩} [95] of L2(G), where ˆE2 x,µ is diagonal: ˆE2 x,µ |jmn⟩ = C2(j) |jmn⟩ . (6) Here m and n are indices in the j-irrep of G and C2(j) is the quadratic Casimir of j [95]. In the strong coupling limit, where the electric energy term dominates HLGT, this truncation is equivalent to an energy cutoff [110]. C. Gauss law and the dressed site The most distinctive feature of gauge theories is ar- guably the presence of local constraints, analogous to the Gauss law of classical electrodynamics, relating the con- figuration of the gauge field to the spatial distribution of charges [111]. At the quantum level, Gauss law is the statement that only gauge invariant states are physical, namely, ˆGν x |Ψphys⟩ = 0 ∀x, ν, where ˆGν x are the genera- tors of local gauge transformations at x: ˆGν x = ˆQν x + ˆqν x + X µ [ˆLν x,µ + ˆRν x−µ,µ] , (7) with ˆqν x representing eventual static background charges (typically vanishing). On the lattice, Gauss law provides a set of vertex constraints, each involving a lattice site and its 2D neighboring links. Due to Gauss law, the physical Hilbert space of LGTs is much smaller than the tensor product of all local sites and link Hilbert spaces. Properly exploiting gauge sym- metries can thus significantly speed up numerical sim- ulations [16]. Strategies that solve Gauss law by elim- inating (partially or entirely) either the gauge fields or6 U αβ x,μ Lx,μ Rx,μ Gauge Fields x x + μ ψx,α Lx,μyRx−μy,μy Rx−μx,μx Lx,μxQx GAUSS LAW Gx α ∈ { … } Matter Fields Hyper-cubic spatial Lattice Λ a (c) LINK  SYMMETRY ψψ x x + μ (a) ψ ψ x + μx (b) RISHONS ζ ζ ψ ψ x + μx (d ) x + μx ker Gx GAUGE-SINGLETS L, U, R FIG. 3. Pictorial representation of the dressed site formal- ism adopted for TN simulations of LGTs: (a) starting from the original formulation of matter and gauge fields, (b) each truncated gauge link is split into two representations, one per half-link; each is equipped with a proper fermionic rishon mode ζ. (c) All the half-links are absorbed into the attached matter site, forming a gauge-invariant dressed site (d) whose Hilbert space spans all the possible gauge singlets. the matter fields have been developed. Nonetheless, such approaches come with specific limitations: the range of interaction has to be extended, moreover, integrating-out gauge fields become problematic in D >1 [97], while the recipe for removing matter is a model (matter content) dependent [112]. Another possibility is to enforce Gauss law using a dressed site construction, sketched in Fig. 3 and out- lined below. Dressed sites have local dimensions typ- ically larger than those resulting from the aforemen- tioned approaches, but they are obtained from a model- independent prescription which has the advantage of pre- serving the locality of the interactions [16, 110]. a. Rishons. As a first step, every link is decomposed into a pair of left (L) and right (R) rishon degrees of free- dom, each associated with a Hilbert space spanned by the basis states |jm⟩, identifying |jmn⟩ ,→ |jm⟩L ⊗|jn⟩R, and writing parallel transporters as rishon bilinears [50]: ˆUαβ x,µ → X k ζL(k)α x,+µ ζR(k)β † x+µ,−µ . (8) b. Abelian link symmetries. Physical gauge link configurations, i.e. those with the left and right rishons in the same irrep, are selected introducing a local link sym- metry at the TN simulation level [16, 110]. Notice this is always Abelian, regardless of the Abelian or non-Abelian nature of the gauge group. c. Gauge invariant computational basis. Crucially, the gauge generators ˆGν x now involve only the matter site at x and its 2 D neighboring rishons. Fusing these degrees of freedom in a composite site, Gauss law be- comes an internal constraint that singles out the dressed site Hilbert space as their gauge invariant subspace: Hdress = kerG ⊂ Hmatt ⊗ (Hrish)⊗2D . (9) The expansion of the gauge singlet basis states of Hdress in terms of the matter and rishon bases is computed via Clebsch-Gordan decomposition [110]. d. Defermionization. It is possible to effectively eliminate fermionic matter, and treat dressed sites as large spins, for any gauge theory where the gauge field has a well-defined parity. Specifically, a local parity operator Px,µ = P† x,µ such that P2 x,µ = 1 must sat- isfy {ˆUαβ x,µ, Px,µ} = 0, as it happens for Z2N , U(N), and SU(2N) [50, 112, 113]. In this case, it is possible to take fermionic rishons, and as a result, all physical (gauge invariant) dressed site operators are genuinely lo- cal, i.e. they mutually commute at a nonzero distance (as spins or bosons) [114]. In particular, this applies to the Hamiltonian HLGT, making gauge-defermionization particularly convenient for higher-dimensional systems, where Jordan-Wigner strings result in long-range inter- actions [115]. D. Scaling of the local basis dimension Table I lists the dressed site dimension d = dim Hdress associated with the first few nontrivial gauge truncations of three representative LGT models in D = 2 , 3 space dimensions. All the considered LGTs include dynami- cal matter, represented by one fermionic field multiplet in the fundamental representation of the gauge group. The ℓ-th row of Table I is obtained keeping only the first ℓ nonzero electric energy levels (i.e., using the ℓ- th smallest quadratic Casimir eigenvalue as cutoff Θ, see Sec. II B). As Table I shows, the local dimension increases rapidly with ℓ. For 3-dimensional non-Abelian LGTs, d ∼ O(104) is reached already within the first two trun- cations, making the study of the untruncated limit pro- hibitive. Differently from the models typically encountered in condensed matter physics, the local dimension of LGTs can thus be a limiting factor for TN simulation — es- pecially when d becomes comparable to commonly used TN bond dimensions (100 ≲ χ ≲ 500 for TTN). In these cases, strategies aimed at further compressing the local computational basis are needed (see Sec. III A). As just discussed, such truncation strategies are particularly cru- cial for high-dimensional LGTs. Several numerical analyses suggest that, in some cases, a small-to-moderate truncation of the gauge group is enough for accurately approximating the continuum lim- its, at least for low-energy states [24, 26, 32, 116–118]. However, the optimal gauge truncation depends on the Hamiltonian parameters m and g. As an example, we consider the (2+1)D U(1) LGT including dynamical matter (QED), whose Hamiltonian7 ℓ d (2 + 1)-dimensions (3 + 1)-dimensions U(1) SU(2) SU(3) U(1) SU(2) SU(3) 1 35 30 164 267 178 3096 2 165 168 752 3437 3670 52476 3 455 600 3738 18487 35280 813438 4 969 1650 19878 64953 214958 17490134 5 1771 3822 43698 177155 967466 69232482 6 2925 7840 82128 408421 3509062 228461186 7 4495 14688 212496 835311 10828494 1245755754 8 6545 25650 333538 1561841 29473038 2782999996 TABLE I. Dressed site Hilbert space dimension d for in- creasing number ℓ of allowed electric energy density levels in some 2- and 3-dimensional paradigmatic LGTs with dy- namical matter and gauge groups U(1), SU(2), and SU(3). can be obtained from Eq. (3): HQED = cℏ 2a X x h -i ˆψ† x ˆUx,µx ˆψx+µx − (−1)jx+jy ˆψ† x ˆUx,µy ˆψx+µy + H.c. i + mc2 X x (−1)jx+jy ˆψ† x ˆψx + g2cℏ 2a X x,µ ˆE2 x,µ − cℏ 2g2a X □ Tr(ˆU□ + ˆU† □), (10) Since the Abelian U(1) group has only one generator, the gauge fields Ex and ˆUx,µ can be represented as Ex,µ = σz x,µ(ℓ) ˆUx,µ = ζx,+µ(ℓ) · ζx+µ,−µ(ℓ) (11) where σz(ℓ) is the spin z-operator in the ℓ SU(2) irrep, while ζ(ℓ) is a ℓ×ℓ ladder operator with Fermi statistics. We focus on a single plaquette in open boundary con- ditions, as it provides the minimal setting allowing for both electric and magnetic effects. Then, to characterize the convergence in the gauge truncation ℓ, we consider a candidate observable O and compute its ground state expectation value ⟨ ˆO⟩ℓ for increasing ℓ. We iterate un- til the relative deviation between consecutive truncations drops below some threshold ϵtrunc: |⟨ ˆO⟩ℓ∗ − ⟨ˆO⟩ℓ∗−1| < ϵtrunc|⟨ ˆO⟩ℓ∗| for some ℓ∗. Sec. II D(a) shows the mini- mal truncation ℓ∗ at which the magnetic energy operator ˆO = Re ˆU□ is converged to ϵtrunc = 10−5. We explore a grid of model parameters, whose extent has been chosen according to standard MC literature [8, 119–130]. Re ˆU□ is used as a benchmark due to its relevance in the weak coupling regime, where the continuum limit of D < 3 lattice QED is located [8]. As Sec. II D(b) shows, ℓ∗ de- pends heavily on the coupling, growing asymptotically like ℓ∗ ∼ g−1 as g is decreased, while m plays almost no role. An analogous inverse dependence of the min- imal gauge truncation on the coupling is expected for non-Abelian LGT in arbitrary dimensions. Moreover, 10−1 100 101 m (a) 10−1 100 101 g2 (c) 10−1 100 101 g2 100 101 ℓ∗ (b) ∼ g−1 m = 0.1 5 10 ℓ∗ 1 2 3 S FIG. 4. Exact diagonalization of a QED plaquette for a grid of masses and couplings, m ∈ [10−2, 101] and g2 ∈ [10−1, 101]. (a,b) Minimal gauge truncation ℓ∗ required to reach a preci- sion ϵtrunc = 10−5 in the magnetic energy ⟨Re ˆU□⟩. (c) Cor- responding entanglement entropy S associated with a sym- metric bipartition of the plaquette. the continuum limit of non-Abelian LGTs in D ≤ 3 is also expected at g → 0 [8], further substantiating the need to compress the local dimension in TN simulations of LGT whenever extrapolation to the continuum is in order. Apart from the growth of the local dimension, extrap- olation to the continuum is further complicated by the fact that the continuum limit of a lattice quantum field theory corresponds to a critical point of the underlying lattice model [131]. Close to criticality, quantum corre- lations are boosted and violations of the entanglement area law are expected [79]. The higher entanglement entropy in the proximity of the continuum ( g, m≪ 1 regime) is already captured by the single-plaquette anal- ysis of Sec. II D(c). Continuum limits of LGTs are thus an area of potential advantage for quantum computa- tion over classical methods, as the former is not limited by entanglement. Nonetheless, quantum computation is also affected by the need to relax gauge truncations when g → 0, either by increasing the number of qubits used to encode a dressed site, which is at least ⌈log2(d)⌉, or by using hybrid devices, that have both qubits and bosonic modes [132].8 III. ROADMAP FOR ADVANCED LGT SIMULATIONS VIA TENSOR NETWORKS As detailed in the previous sections, LGT models present some peculiar features that make TN simulations particularly challenging, especially for large system sizes and for studying the continuum limits in terms of gauge field truncation, lattice spacing, and volume. State-of-the-art techniques, such as TTN algorithms, have been applied for simulating ground state proper- ties of QED in (2+1)- and (3+1)-dimensions for small-to- intermediate sizes [44, 46]. Very recently, they have been also applied to the SU(2) Yang-Mills model in (2+1)- dimensions [50]. In all these simulations, small non- trivial representations of the gauge groups have been exploited, e.g. three electric field levels for QED and the first two irreducible representations of SU(2) for the Yang-Mills model. Nowadays, lattice computations with MC-based tech- niques are performed on large lattices, e.g. of the or- der 643 × 128 sites (space and time discretization), and with no truncation of the gauge fields. These large sizes are required to control finite-volume effects and to per- form extrapolations toward the continuum limits [133]. In the last decades, the impressive progress in algorith- mic development, high-performance optimizations, and the availability of increasingly powerful supercomputer facilities have played a major role in the advancement of MC-based LGT computations. Indeed, this progress has opened the doors to large-scale simulations, that cur- rently represent the standard approach for studying non- perturbative phenomena in quantum field theory. However, MC techniques are generally based on com- putations of path integrals in which the integrand func- tions are overall positive. Many physically relevant sce- narios, such as finite baryon density regimes or real-time dynamics of quarks, give rise to a change in the sign of the integrands and highly oscillating behaviors. Thus, numerical evaluations suffer from the near cancellation of the opposite-sign contributions to the integrals. This is the essence of the infamous sign problem, a long-standing issue of LGT simulations with MC methods [10]. Hence the quest of conceiving, developing, and op- timizing alternative approaches that enable simulating these regimes, being the latter at the heart of many open problems related to our understanding of high-energy physics. As described in the previous sections, TNs represent a promising complementary method, which found the first applications in simulating non-trivial instances of high- dimensional LGTs on small-to-intermediate lattice sizes. TNs are intrinsically sign-problem-free, enabling the sim- ulation of both static properties at equilibrium, such as low-energy states, and real-time dynamics, even in the presence of finite chemical potentials or non-trivial topo- logical terms. It is worth noting that, in addition to local observables and correlation functions, TNs allow the nu- merical computations of entanglement properties, such MPS  PEPS <latexit sha1_base64=\"evLsJgYPR1JhShTB3ZZqnA5vJ0g=\">AAAB83icbVDLSgNBEOz1GeMr6tHLYBA8hV1R9BjQg8cEzAOSJcxOOnHI7IOZHiGEfIFXPXkTr36QB//F3XUPmlinoqqbrq4gUdKQ6346K6tr6xubpa3y9s7u3n7l4LBtYqsFtkSsYt0NuEElI2yRJIXdRCMPA4WdYHKT+Z1H1EbG0T1NE/RDPo7kSApOqdQcDipVt+bmYMvEK0gVCjQGla/+MBY2xIiE4sb0PDchf8Y1SaFwXu5bgwkXEz7GXkojHqLxZ3nQOTu1hlPMEtRMKpaL+HtjxkNjpmGQToacHsyil4n/eT1Lo2t/JqPEEkYiO0RSYX7ICC3TBpANpUYiniVHJiMmuOZEqCXjQqSiTSspp314i98vk/Z5zbusuc2Lav22aKYEx3ACZ+DBFdThDhrQAgEIT/AML451Xp035/1ndMUpdo7gD5yPb2lJkXM=</latexit> d <latexit sha1_base64=\"I0OcdTLU7qoQvZ5LLRQkXA1hp+s=\">AAAB9nicbVC7TsNAEDyHVwivACXNiQiJKrIRCMpIUFAGiTykxIrWl01yyvmhuzUisvILtFDRIVp+h4J/wTYuIGGq0cyudna8SElDtv1plVZW19Y3ypuVre2d3b3q/kHbhLEW2BKhCnXXA4NKBtgiSQq7kUbwPYUdb3qd+Z0H1EaGwT3NInR9GAdyJAVQJvXFRA6qNbtu5+DLxClIjRVoDqpf/WEoYh8DEgqM6Tl2RG4CmqRQOK/0Y4MRiCmMsZfSAHw0bpJnnfOT2ACFPELNpeK5iL83EvCNmfleOukDTcyil4n/eb2YRlduIoMoJgxEdoikwvyQEVqmJSAfSo1EkCVHLgMuQAMRaslBiFSM01YqaR/O4vfLpH1Wdy7q9t15rXFTNFNmR+yYnTKHXbIGu2VN1mKCTdgTe2Yv1qP1ar1Z7z+jJavYOWR/YH18A64Qkr0=</latexit> \u0000 <latexit sha1_base64=\"evLsJgYPR1JhShTB3ZZqnA5vJ0g=\">AAAB83icbVDLSgNBEOz1GeMr6tHLYBA8hV1R9BjQg8cEzAOSJcxOOnHI7IOZHiGEfIFXPXkTr36QB//F3XUPmlinoqqbrq4gUdKQ6346K6tr6xubpa3y9s7u3n7l4LBtYqsFtkSsYt0NuEElI2yRJIXdRCMPA4WdYHKT+Z1H1EbG0T1NE/RDPo7kSApOqdQcDipVt+bmYMvEK0gVCjQGla/+MBY2xIiE4sb0PDchf8Y1SaFwXu5bgwkXEz7GXkojHqLxZ3nQOTu1hlPMEtRMKpaL+HtjxkNjpmGQToacHsyil4n/eT1Lo2t/JqPEEkYiO0RSYX7ICC3TBpANpUYiniVHJiMmuOZEqCXjQqSiTSspp314i98vk/Z5zbusuc2Lav22aKYEx3ACZ+DBFdThDhrQAgEIT/AML451Xp035/1ndMUpdo7gD5yPb2lJkXM=</latexit> d <latexit sha1_base64=\"I0OcdTLU7qoQvZ5LLRQkXA1hp+s=\">AAAB9nicbVC7TsNAEDyHVwivACXNiQiJKrIRCMpIUFAGiTykxIrWl01yyvmhuzUisvILtFDRIVp+h4J/wTYuIGGq0cyudna8SElDtv1plVZW19Y3ypuVre2d3b3q/kHbhLEW2BKhCnXXA4NKBtgiSQq7kUbwPYUdb3qd+Z0H1EaGwT3NInR9GAdyJAVQJvXFRA6qNbtu5+DLxClIjRVoDqpf/WEoYh8DEgqM6Tl2RG4CmqRQOK/0Y4MRiCmMsZfSAHw0bpJnnfOT2ACFPELNpeK5iL83EvCNmfleOukDTcyil4n/eb2YRlduIoMoJgxEdoikwvyQEVqmJSAfSo1EkCVHLgMuQAMRaslBiFSM01YqaR/O4vfLpH1Wdy7q9t15rXFTNFNmR+yYnTKHXbIGu2VN1mKCTdgTe2Yv1qP1ar1Z7z+jJavYOWR/YH18A64Qkr0=</latexit> \u0000 <latexit sha1_base64=\"evLsJgYPR1JhShTB3ZZqnA5vJ0g=\">AAAB83icbVDLSgNBEOz1GeMr6tHLYBA8hV1R9BjQg8cEzAOSJcxOOnHI7IOZHiGEfIFXPXkTr36QB//F3XUPmlinoqqbrq4gUdKQ6346K6tr6xubpa3y9s7u3n7l4LBtYqsFtkSsYt0NuEElI2yRJIXdRCMPA4WdYHKT+Z1H1EbG0T1NE/RDPo7kSApOqdQcDipVt+bmYMvEK0gVCjQGla/+MBY2xIiE4sb0PDchf8Y1SaFwXu5bgwkXEz7GXkojHqLxZ3nQOTu1hlPMEtRMKpaL+HtjxkNjpmGQToacHsyil4n/eT1Lo2t/JqPEEkYiO0RSYX7ICC3TBpANpUYiniVHJiMmuOZEqCXjQqSiTSspp314i98vk/Z5zbusuc2Lav22aKYEx3ACZ+DBFdThDhrQAgEIT/AML451Xp035/1ndMUpdo7gD5yPb2lJkXM=</latexit> d <latexit sha1_base64=\"I0OcdTLU7qoQvZ5LLRQkXA1hp+s=\">AAAB9nicbVC7TsNAEDyHVwivACXNiQiJKrIRCMpIUFAGiTykxIrWl01yyvmhuzUisvILtFDRIVp+h4J/wTYuIGGq0cyudna8SElDtv1plVZW19Y3ypuVre2d3b3q/kHbhLEW2BKhCnXXA4NKBtgiSQq7kUbwPYUdb3qd+Z0H1EaGwT3NInR9GAdyJAVQJvXFRA6qNbtu5+DLxClIjRVoDqpf/WEoYh8DEgqM6Tl2RG4CmqRQOK/0Y4MRiCmMsZfSAHw0bpJnnfOT2ACFPELNpeK5iL83EvCNmfleOukDTcyil4n/eb2YRlduIoMoJgxEdoikwvyQEVqmJSAfSo1EkCVHLgMuQAMRaslBiFSM01YqaR/O4vfLpH1Wdy7q9t15rXFTNFNmR+yYnTKHXbIGu2VN1mKCTdgTe2Yv1qP1ar1Z7z+jJavYOWR/YH18A64Qkr0=</latexit> \u0000 TTN 2D TTN <latexit sha1_base64=\"evLsJgYPR1JhShTB3ZZqnA5vJ0g=\">AAAB83icbVDLSgNBEOz1GeMr6tHLYBA8hV1R9BjQg8cEzAOSJcxOOnHI7IOZHiGEfIFXPXkTr36QB//F3XUPmlinoqqbrq4gUdKQ6346K6tr6xubpa3y9s7u3n7l4LBtYqsFtkSsYt0NuEElI2yRJIXdRCMPA4WdYHKT+Z1H1EbG0T1NE/RDPo7kSApOqdQcDipVt+bmYMvEK0gVCjQGla/+MBY2xIiE4sb0PDchf8Y1SaFwXu5bgwkXEz7GXkojHqLxZ3nQOTu1hlPMEtRMKpaL+HtjxkNjpmGQToacHsyil4n/eT1Lo2t/JqPEEkYiO0RSYX7ICC3TBpANpUYiniVHJiMmuOZEqCXjQqSiTSspp314i98vk/Z5zbusuc2Lav22aKYEx3ACZ+DBFdThDhrQAgEIT/AML451Xp035/1ndMUpdo7gD5yPb2lJkXM=</latexit> d <latexit sha1_base64=\"I0OcdTLU7qoQvZ5LLRQkXA1hp+s=\">AAAB9nicbVC7TsNAEDyHVwivACXNiQiJKrIRCMpIUFAGiTykxIrWl01yyvmhuzUisvILtFDRIVp+h4J/wTYuIGGq0cyudna8SElDtv1plVZW19Y3ypuVre2d3b3q/kHbhLEW2BKhCnXXA4NKBtgiSQq7kUbwPYUdb3qd+Z0H1EaGwT3NInR9GAdyJAVQJvXFRA6qNbtu5+DLxClIjRVoDqpf/WEoYh8DEgqM6Tl2RG4CmqRQOK/0Y4MRiCmMsZfSAHw0bpJnnfOT2ACFPELNpeK5iL83EvCNmfleOukDTcyil4n/eb2YRlduIoMoJgxEdoikwvyQEVqmJSAfSo1EkCVHLgMuQAMRaslBiFSM01YqaR/O4vfLpH1Wdy7q9t15rXFTNFNmR+yYnTKHXbIGu2VN1mKCTdgTe2Yv1qP1ar1Z7z+jJavYOWR/YH18A64Qkr0=</latexit> \u0000 (a) (b) (c) (d) <latexit sha1_base64=\"KLwIyIQatXPp/A6LUgZLhsVBF40=\">AAAB+nicbVC7TsNAEDzzDOEVoKQ5ESFRRTYCQRlBQxkk8pASK1pfNuGU80N3a6Qo5CdooaJDtPwMBf/C2biAhKlGM7ua3QkSJQ257qeztLyyurZe2ihvbm3v7Fb29lsmTrXApohVrDsBGFQywiZJUthJNEIYKGwH4+vMbz+gNjKO7miSoB/CKJJDKYCs1OnFJEM0vF+pujU3B18kXkGqrECjX/nqDWKRhhiRUGBM13MT8qegSQqFs3IvNZiAGMMIu5ZGYFP8aX7vjB+nBijmCWouFc9F/L0xhdCYSRjYyRDo3sx7mfif101peOlPZZSkhJHIgkgqzIOM0NIWgXwgNRJBdjlyGXEBGohQSw5CWDG1zZRtH97894ukdVrzzmvu7Vm1flU0U2KH7IidMI9dsDq7YQ3WZIIp9sSe2Yvz6Lw6b877z+iSU+wcsD9wPr4BnJ6UYA==</latexit> ⌦ <latexit sha1_base64=\"KLwIyIQatXPp/A6LUgZLhsVBF40=\">AAAB+nicbVC7TsNAEDzzDOEVoKQ5ESFRRTYCQRlBQxkk8pASK1pfNuGU80N3a6Qo5CdooaJDtPwMBf/C2biAhKlGM7ua3QkSJQ257qeztLyyurZe2ihvbm3v7Fb29lsmTrXApohVrDsBGFQywiZJUthJNEIYKGwH4+vMbz+gNjKO7miSoB/CKJJDKYCs1OnFJEM0vF+pujU3B18kXkGqrECjX/nqDWKRhhiRUGBM13MT8qegSQqFs3IvNZiAGMMIu5ZGYFP8aX7vjB+nBijmCWouFc9F/L0xhdCYSRjYyRDo3sx7mfif101peOlPZZSkhJHIgkgqzIOM0NIWgXwgNRJBdjlyGXEBGohQSw5CWDG1zZRtH97894ukdVrzzmvu7Vm1flU0U2KH7IidMI9dsDq7YQ3WZIIp9sSe2Yvz6Lw6b877z+iSU+wcsD9wPr4BnJ6UYA==</latexit> ⌦ <latexit sha1_base64=\"E15UxALi/uMB0qBTfYExXAWjE0w=\">AAAB9XicbVC7TsNAEDyHVwivACXNiQiJyrIRCMoIGsogyENKrGh92YRTzg/drUFRlE+ghYoO0fI9FPwLtnEBCVONZna1s+PHShpynE+rtLS8srpWXq9sbG5t71R391omSrTApohUpDs+GFQyxCZJUtiJNULgK2z746vMbz+gNjIK72gSoxfAKJRDKYBS6da27X615thODr5I3ILUWIFGv/rVG0QiCTAkocCYruvE5E1BkxQKZ5VeYjAGMYYRdlMaQoDGm+ZRZ/woMUARj1FzqXgu4u+NKQTGTAI/nQyA7s28l4n/ed2EhhfeVIZxQhiK7BBJhfkhI7RMO0A+kBqJIEuOXIZcgAYi1JKDEKmYpKVU0j7c+e8XSevEds9s5+a0Vr8smimzA3bIjpnLzlmdXbMGazLBRuyJPbMX69F6td6s95/RklXs7LM/sD6+Ae5Tkas=</latexit> ... <latexit sha1_base64=\"KLwIyIQatXPp/A6LUgZLhsVBF40=\">AAAB+nicbVC7TsNAEDzzDOEVoKQ5ESFRRTYCQRlBQxkk8pASK1pfNuGU80N3a6Qo5CdooaJDtPwMBf/C2biAhKlGM7ua3QkSJQ257qeztLyyurZe2ihvbm3v7Fb29lsmTrXApohVrDsBGFQywiZJUthJNEIYKGwH4+vMbz+gNjKO7miSoB/CKJJDKYCs1OnFJEM0vF+pujU3B18kXkGqrECjX/nqDWKRhhiRUGBM13MT8qegSQqFs3IvNZiAGMMIu5ZGYFP8aX7vjB+nBijmCWouFc9F/L0xhdCYSRjYyRDo3sx7mfif101peOlPZZSkhJHIgkgqzIOM0NIWgXwgNRJBdjlyGXEBGohQSw5CWDG1zZRtH97894ukdVrzzmvu7Vm1flU0U2KH7IidMI9dsDq7YQ3WZIIp9sSe2Yvz6Lw6b877z+iSU+wcsD9wPr4BnJ6UYA==</latexit> ⌦ PS-DMRG <latexit sha1_base64=\"evLsJgYPR1JhShTB3ZZqnA5vJ0g=\">AAAB83icbVDLSgNBEOz1GeMr6tHLYBA8hV1R9BjQg8cEzAOSJcxOOnHI7IOZHiGEfIFXPXkTr36QB//F3XUPmlinoqqbrq4gUdKQ6346K6tr6xubpa3y9s7u3n7l4LBtYqsFtkSsYt0NuEElI2yRJIXdRCMPA4WdYHKT+Z1H1EbG0T1NE/RDPo7kSApOqdQcDipVt+bmYMvEK0gVCjQGla/+MBY2xIiE4sb0PDchf8Y1SaFwXu5bgwkXEz7GXkojHqLxZ3nQOTu1hlPMEtRMKpaL+HtjxkNjpmGQToacHsyil4n/eT1Lo2t/JqPEEkYiO0RSYX7ICC3TBpANpUYiniVHJiMmuOZEqCXjQqSiTSspp314i98vk/Z5zbusuc2Lav22aKYEx3ACZ+DBFdThDhrQAgEIT/AML451Xp035/1ndMUpdo7gD5yPb2lJkXM=</latexit> d <latexit sha1_base64=\"L/lYlzbxMeg/WCo9g0LIj/f0Elg=\">AAAB+HicbVC7TsNAEDyHVwivACXNiQiJKrIRCMoIGsogkYeUWNH5sglHzufT3RopWPkHWqjoEC1/Q8G/YBsXkDDVaGZXOzuBlsKi6346paXlldW18nplY3Nre6e6u9e2UWw4tHgkI9MNmAUpFLRQoISuNsDCQEInmFxlfucBjBWRusWpBj9kYyVGgjNMpbYaJNrOBtWaW3dz0EXiFaRGCjQH1a/+MOJxCAq5ZNb2PFejnzCDgkuYVfqxBc34hI2hl1LFQrB+kqed0aPYMoyoBkOFpLkIvzcSFlo7DYN0MmR4Z+e9TPzP68U4uvAToXSMoHh2CIWE/JDlRqQ1AB0KA4gsSw5UKMqZYYhgBGWcp2Kc9lJJ+/Dmv18k7ZO6d1Z3b05rjcuimTI5IIfkmHjknDTINWmSFuHknjyRZ/LiPDqvzpvz/jNacoqdffIHzsc3sAyT5w==</latexit> n ps (a) <latexit sha1_base64=\"E15UxALi/uMB0qBTfYExXAWjE0w=\">AAAB9XicbVC7TsNAEDyHVwivACXNiQiJyrIRCMoIGsogyENKrGh92YRTzg/drUFRlE+ghYoO0fI9FPwLtnEBCVONZna1s+PHShpynE+rtLS8srpWXq9sbG5t71R391omSrTApohUpDs+GFQyxCZJUtiJNULgK2z746vMbz+gNjIK72gSoxfAKJRDKYBS6da27X615thODr5I3ILUWIFGv/rVG0QiCTAkocCYruvE5E1BkxQKZ5VeYjAGMYYRdlMaQoDGm+ZRZ/woMUARj1FzqXgu4u+NKQTGTAI/nQyA7s28l4n/ed2EhhfeVIZxQhiK7BBJhfkhI7RMO0A+kBqJIEuOXIZcgAYi1JKDEKmYpKVU0j7c+e8XSevEds9s5+a0Vr8smimzA3bIjpnLzlmdXbMGazLBRuyJPbMX69F6td6s95/RklXs7LM/sD6+Ae5Tkas=</latexit> ... LBO-MPS <latexit sha1_base64=\"evLsJgYPR1JhShTB3ZZqnA5vJ0g=\">AAAB83icbVDLSgNBEOz1GeMr6tHLYBA8hV1R9BjQg8cEzAOSJcxOOnHI7IOZHiGEfIFXPXkTr36QB//F3XUPmlinoqqbrq4gUdKQ6346K6tr6xubpa3y9s7u3n7l4LBtYqsFtkSsYt0NuEElI2yRJIXdRCMPA4WdYHKT+Z1H1EbG0T1NE/RDPo7kSApOqdQcDipVt+bmYMvEK0gVCjQGla/+MBY2xIiE4sb0PDchf8Y1SaFwXu5bgwkXEz7GXkojHqLxZ3nQOTu1hlPMEtRMKpaL+HtjxkNjpmGQToacHsyil4n/eT1Lo2t/JqPEEkYiO0RSYX7ICC3TBpANpUYiniVHJiMmuOZEqCXjQqSiTSspp314i98vk/Z5zbusuc2Lav22aKYEx3ACZ+DBFdThDhrQAgEIT/AML451Xp035/1ndMUpdo7gD5yPb2lJkXM=</latexit> d <latexit sha1_base64=\"I0OcdTLU7qoQvZ5LLRQkXA1hp+s=\">AAAB9nicbVC7TsNAEDyHVwivACXNiQiJKrIRCMpIUFAGiTykxIrWl01yyvmhuzUisvILtFDRIVp+h4J/wTYuIGGq0cyudna8SElDtv1plVZW19Y3ypuVre2d3b3q/kHbhLEW2BKhCnXXA4NKBtgiSQq7kUbwPYUdb3qd+Z0H1EaGwT3NInR9GAdyJAVQJvXFRA6qNbtu5+DLxClIjRVoDqpf/WEoYh8DEgqM6Tl2RG4CmqRQOK/0Y4MRiCmMsZfSAHw0bpJnnfOT2ACFPELNpeK5iL83EvCNmfleOukDTcyil4n/eb2YRlduIoMoJgxEdoikwvyQEVqmJSAfSo1EkCVHLgMuQAMRaslBiFSM01YqaR/O4vfLpH1Wdy7q9t15rXFTNFNmR+yYnTKHXbIGu2VN1mKCTdgTe2Yv1qP1ar1Z7z+jJavYOWR/YH18A64Qkr0=</latexit> \u0000 <latexit sha1_base64=\"1ISFhiA8sSOVFMm25ZyGuW9IghU=\">AAAB9HicbVC7TsNAEFyHVwivACXNiQhBFdkIBGUEDWVA5CElVnS+bMIp54fu1pEiK39ACxUdouV/KPgXbOMCAlONZna1s+NFShqy7Q+rtLS8srpWXq9sbG5t71R399omjLXAlghVqLseN6hkgC2SpLAbaeS+p7DjTa4zvzNFbWQY3NMsQtfn40COpOCUSnfD40G1ZtftHOwvcQpSgwLNQfWzPwxF7GNAQnFjeo4dkZtwTVIonFf6scGIiwkfYy+lAffRuEmedM6OYsMpZBFqJhXLRfy5kXDfmJnvpZM+pwez6GXif14vptGlm8ggigkDkR0iqTA/ZISWaQXIhlIjEc+SI5MBE1xzItSScSFSMU47qaR9OIvf/yXt07pzXrdvz2qNq6KZMhzAIZyAAxfQgBtoQgsEjOARnuDZmlov1qv19j1asoqdffgF6/0LysuRog==</latexit> d 0 (b) FIG. 5. Graphical representation of (a) the pseudo site DMRG (PS-DMRG) approach, in which a single site having large local dimension d is replaced with nps pseudo sites with a smaller local dimension, e.g two; (b) the local basis opti- mization embedded in the MPS ansatz (LBO-MPS), so that an additional layer of tensors connected to the physical legs of the MPS performs the reduction from a large local basis with dimension d into an effective basis with smaller dimension d′. as entanglement entropies and central charges, that could potentially shield new light on LGT phenomena [32]. Nevertheless, TN simulations of high-dimensional LGTs still represent a challenging problem, especially for large lattice sizes or higher representations of gauge groups needed for analyzing continuum limits. In this framework, further and intensive developments are re- quired to tackle TNs’ current problems related to LGTs, such as QCD’s non-perturbative effects on lattices of sizes comparable with MC simulations. In this regard, we note that sign-problem-free TN ans¨ atze can also be used in combination with variational MC methods to tackle high- dimensional lattice gauge theories with arbitrary gauge groups [134]. In the following, we present a possible roadmap in terms of algorithmic development and optimization strategies that we foresee to be crucial for making the TN approach competitive as a complementary method to MC techniques. Therein, the first two sections, i.e. Secs. III A and III B, can be approached with existing TN algorithms and a good intuition on LGT problems; then, Secs. III C and III D outline optimization for ex- isting algorithms to leverage HPC systems; finally, we discuss new classes of ans¨ atze to tackle finite tempera- ture problems in Sec. III E. In some parts, we focus on TTNs, but the vast majority of the presented concepts and techniques can be straightforwardly applied to other TN ans¨ atze. A. Local basis truncation One of the main issues in simulating LGTs with TN methods lies in the very large local basis dimensiond that9 one needs to handle to represent matter and gauge-field degrees of freedom properly. To some extent, this situ- ation is very similar to some condensed matter models, e.g. the Holstein model, involving lattice fermions cou- pled with phonons [135], or bosonic systems coupled to optical cavities [136, 137]. Also in these cases, the local Hilbert space is in principle infinite-dimensional, and a truncation to a fixed cutoff is needed for performing TN simulations. For one-dimensional systems of this type, some efficient algorithms based on MPS have been devel- oped in the last years [138], e.g. the pseudosite DMRG (PS-DMRG) method [139], and the DMRG with local basis optimization (DMRG-LBO) [140–142]. The key idea of the PS-DMRG is to replace a single site having large local dimension d with nps ≈ log2(d) pseudo sites of local dimension 2, as shown Fig. 5(a). Since a large class of optimization algorithms for TNs scale at least quadratically with the local dimension but linearly with the total number of sites, as detailed in Sec. I, one obtains a more efficient and manageable representation according to this procedure. The price to pay lies in the range of the interactions: short-range interactions in the Hamiltonian are trans- formed into long-range operators due to the pseudo-site encoding of the local degrees of freedom. As a conse- quence, PS-DMRG should require a larger bond dimen- sion and a larger number of variational steps to con- verge, and the benefits offered by the pseudo-sites could progressively fade for increasing values of nps at fixed bond dimension. PS-DMRG has been applied to one- dimensional QMB systems with d up to O(100) [138]. The core of the DMRG-LBO algorithm, instead, lies in a local basis optimization protocol which enables a con- trolled and efficient truncation of the local Hilbert space [143]. For each site x′ of the lattice, the optimized local basis is computed by starting from its reduced density matrix, i.e. ρx′ = Trx̸=x′ |ψ⟩⟨ψ|, (12) where |ψ⟩ is a general state of the whole system and the trace is over all the degrees of freedom which do not in- volve the site x′. By performing the diagonalization pro- cedure, the eigenvalues λα and the eigenvectors vα of ρx′ can be easily determined. The set of values λα represent the probabilities associated with the states vα. If λα is small, e.g. below a certain numerical threshold, the re- lated eigenvector vα can be discarded from the local basis of the site x′, with a controllable loss of accuracy for the state |ψ⟩. Thus, for reducing the local basis dimension of x′ from d to a smaller value d′, an optimal choice is keeping the d′ eigenvectors of ρx′ with the largest prob- abilities. Then, the original state |ψ⟩ can be projected on the new basis, without losing the relevant physical information. Let us notice that the site x′ can also be a generic unit cell of the system, composed of a certain number of lattice sites. A crucial point of the LBO procedure is the knowl- edge of the original state |ψ⟩, generally the ground state of the system, that is not known prior. This issue can be overcome in several ways: by performing exact di- agonalization of the system’s Hamiltonian with local di- mension d for small lattice sizes, to determine |ψ⟩, and then truncating the basis from d to d′ for increasing val- ues of the cutoff d′. From this procedure, we can obtain an optimized local basis ensuring a controlled approx- imation of the original ground state. This optimized basis can then be exploited in optimization algorithms for simulating larger lattice sizes, such as DMRG. An- other strategy directly incorporates the LBO procedure in the tensor network ansatz, as shown Fig. 5(b): by in- serting an additional tensor on each physical leg of the MPS, the large local basis with dimension d is trans- formed into an effective basis with smaller dimension d′. In this way, the MPS tensors only see the effective ba- sis in the optimization procedures, with a significant re- duction of the computational costs described in Sec. I. This method has been used for both static DMRG and time evolution algorithms, such as TEBD, for the study of one-dimensional quantum impurity models and corre- lated electron-phonon systems [140, 141, 144]. In the context of LGTs, the dimension of the local ba- sis d can easily go beyond values of the order of 10 6, es- pecially for higher-dimensional non-Abelian models and large representations of the gauge groups, as shown in Sec. II. In this scenario, numerical simulations with TNs are practically infeasible without an optimized scheme for truncating the local degrees of freedom. Techniques like PS-DMRG might offer some benefits for small sys- tem sizes, such as unveiling the most relevant degrees of freedom in the low-energy states, but it is difficult to scale them up for large sizes due to the long-range in- teractions induced between the pseudo-sites. Also, the local constraints imposed by Gauss law would become highly non-local when splitting a single site into multiple pseudo-sites representing the matter and link fields. In principle, LBO-based procedures could instead rep- resent a well-grounded route for addressing the problem. Their use in condensed matter, e.g. bosonic systems, is a rather consolidated approach, whereas their application to TN simulations of LGTs currently is an uncharted but promising territory. The main steps that we foresee as needed and important in this direction are the following: (i) Employing exact diagonalization, testing the con- vergence of LBO procedures for one-dimensional systems, such as the φ4-theory or the Schwinger model, for which analytical solutions, at least in some regimes of the phase diagram, and numerical results are widely available, also in the limit of no gauge field truncations. In this way, we can obtain valuable information about the scaling of the basis cutoffs concerning the final accuracy of the state representation for small system sizes. (ii) Performing the same analysis on (2+1)-dimensional LGTs, such as QED or SU (N) models, for one unit cell, like a single lattice-plaquette, to systematically10 study the effects of the magnetic interactions on the local degrees of freedom. Indeed, the QLM rep- resentation of LGTs, detailed in Sec. II, generally exploits the “electric field” basis, in which the elec- tric field terms of the Hamiltonian and Gauss law are diagonal. In this scheme, the magnetic interac- tions correspond to non-diagonal operators which can increase the number of local electric states to include for an accurate description of the system, especially for small values of the coupling constant g of Eq. (3), as highlighted in the numerical analy- sis in Sec. II D. (iii) Exploiting the optimized local bases obtained from exact diagonalization as input of TN simulations for larger sizes, testing the effects on the global ground state accuracy and in computing physically relevant quantities, such as the mass gap [130]. (iv) In the same spirit of LBO-MPS, implementing LBO protocols directly in TN ans¨ atze that are more suit- able for simulating high-dimensional LGT models, such as TTN. This step could be of great benefit in particular for aTTNs, which encode the area law for the entanglement but are severely limited by large local bases (see Sec. I). By following and combining all these steps, we expect to reduce the effective local basis of LGT models, poten- tially enabling TN simulations for large representations of Abelian and non-Abelian gauge groups. It is worth noting that constructing optimal bases for numerical and quantum computation of LGTs is an ac- tive area of research. Several approaches that have been recently proposed involve performing canonical transfor- mations of the gauge degrees of freedom before trunca- tion [107, 145, 146]. By exploiting a resource-efficient protocol of this type, Ref. [107] has shown that the number of local states required to reach a 1% accuracy level when computing the expectation value of the pla- quette operators in two-dimensional (2+1)-dimensional QED can be reduced by more than 94% compared to the unoptimized truncation. Integrating these approaches into TN algorithms could greatly benefit LGT simula- tions. B. Tailored initial states In TN algorithms for ground state searching, the op- timization procedure generally starts from a random TN initial state |ψinit⟩, i.e. the tensors in the network are filled with random coefficients at the beginning. This strategy usually guarantees that the probability of over- lapping with the true ground state |⟨ψinit|ψgs⟩|2 is not vanishing. To reach a small error in the final energy, this procedure typically requires from 10 to 50 optimization sweeps for LGTs simulations, depending on the specific models, the Hamiltonian parameters, and the lattice size. Since the time for completing a sweep can be very long, especially for large bond dimensions, strategies for re- ducing the number of needed sweeps could be beneficial for scaling up system sizes. From this perspective, con- structing appropriate states to be used as initial guesses can speed up the convergence, similar to the choice of the trial wave function for variational Monte Carlo sim- ulations. We consider the following options: (i) Physical insight: Initial states can be constructed by following physical intuition, at least in those regimes in which analytical or partial numerical results are available. For instance, initial guesses can be constructed by starting from the TN ground states numerically obtained for a lower representa- tion to simulate large spin representations of the gauge fields. (ii) Machine learning: Machine learning-assisted pro- tocols can improve the construction of tailored ini- tial states in the different regimes of the model pa- rameters. For instance, feed-forward neural net- works have been proposed as trial wave functions for quantum Monte Carlo simulations [147], and machine learning techniques have been used to feed TN simulations [148]. Similarly, neural networks might reveal great potential in constructing initial states for LGTs to be used in large-scale TN simu- lations, in which reducing the number of sweeps is a key point for feasibility. (iii) Tensor network results: Following the idea of the physical insight, it is also possible to feed neighbor- ing ground states as initial guesses into a ground state search. This option exists especially when scanning a phase diagram and varying parameters in a small increment such that the overlap between neighboring wave functions is sufficient; this over- lap decreases for two points on the opposite sides of a quantum critical point. The same idea can be im- plemented by preparing an initial guess quenching from an easily accessible ground state to the tar- get parameters; the quench itself does not have to be adiabatic or free of numerical errors, but must only have sufficient overlap with the ground state. The advantage here is that one quench can gen- erate multiple initial guesses along the quench for different parameters. C. Leverage HPC techniques for local optimization We dedicate the two following sections to the numer- ical optimization of the TN algorithms. In this section, we give an overview of the topic and we discuss possible strategies to improve the optimization. The more tech- nical steps are discussed in the following Sec. III D. To scale up TN simulations of LGTs, in particular regarding lattice sizes, another important factor is the11 number of optimization steps to be carried out. The num- ber of optimization steps scales linearly with the number of sweeps as well as with the system size for MPS, PEPS, and TTN. The choice of the number of sweeps is set such that the algorithm reaches convergence when computing ground or low energy states. Let us briefly describe the general procedure for ground state searches: we focus on the TTN optimization, however, the main points de- scribed here can be applied to other TN structures, such as MPS or PEPS. For a complete and technical descrip- tion of the algorithms and implementation details, see Ref. [18]. Consider a generic QMB Hamiltonian H and a generic normalized state |ψ⟩, defined on the same Hilbert space. To numerically determine the ground state of H, the fol- lowing global minimization problem has to be solved: min |ψ⟩ {E(|ψ⟩)} = min |ψ⟩ ⟨ψ|H |ψ⟩ . (13) If |ψ⟩ is written in terms of the TTN ansatz introduced in Sec. I, the variational parameters are the coefficients in the TTN. In this case, the global optimization prob- lem of Eq. (13) is broken down into a sequence of smaller optimizations, each of which involves only a minimal sub- set of tensors in the TTN. The algorithm solves the op- timization via an eigenproblem searching for the eigen- vector with the lowest eigenvalue. Without any loss of generality, one single tensor at a time is optimized in the simplest case, as shown in Fig. 6(a). In detail, the energy is computed by contracting the Hamiltonian between the TTN and its complex conjugate. Then, we start the opti- mization procedure from a target tensor T, by computing its environment, i.e. the network without the tensors T and T†, which represents the effective Hamiltonian Heff for the local problem. At this stage, an eigenproblem of Heff is solved, and the tensorT is updated with the newly found ground state. The whole procedure is sequentially iterated for all the tensors in the network, performing an optimization sweep. For each operation, efficient algorithms from linear al- gebra are typically used, e.g. the Arnoldi algorithm im- plemented in the ARPACK library [149, 150]. We recall that the numerical complexity of this procedure for a sin- gle TTN-tensor is O(d2χ2 + χ4) [76]. Therefore, a single optimization can be time-consuming whenχ is very large, e.g. χ ≈ 1000, as for simulating high-dimensional LGTs. We point out the established and promising future par- allelization schemes for the single tensor optimization: (i) opemMP: An efficient openMP implementation of the contraction between the effective operators with the tensor can speed up simulations. More- over, the Arnoldi algorithm of ARPACK is op- timized for large-scale linear algebra operations and supports intra-node multi-core parallelization based on openMP [151]; thus, ARPACK does not become a bottleneck in the openMP implementa- tion. Nonetheless, many simulations remain ex- pensive in computation time even with 64 or more cores available in HPC facilities; therefore, we con- sider more approaches beyond the well-established openMP path. (ii) Accelerators: Graphics Processing Units (GPU) and Tensor Processing Units (TPU) offer a path to accelerate linear algebra routines, where both have demonstrated their usefulness: GPUs have reported speedups of up to a factor of 10 due to the efficient tensor manipulations [152–155]; ten- sor processing units have shown great potential in large-scale simulations of several quantum systems, e.g. drastically reducing the computational time of DMRG calculations with very large bond di- mensions from months to hours [156–159]. Tensor processing units are application-specific integrated circuits originally introduced for machine learning; we consider the integration and tuning of TPUs therefore as a step after the successful integration of GPUs. While single GPUs can solve TTN- problems up to a bond dimension of χ < 1000, multi-GPU support is available for libraries; HPC systems typically provide hardware with four GPUs per node. (iii) Multi-node approaches to local tensor optimiza- tions: Both CPU and GPU algorithms can be further scaled by using multiple nodes. The un- derlying linear algebra routines of the local eigen- value problem are parallelizable via libraries such as ScaLAPACK or MAGMA. Both libraries provide routines for distributed memory machines [160]; MAGMA supports moreover CPU and GPUs. In this way, the workload of the eigenproblem proce- dure can be split into several computation nodes. Then, it is important to analyze the performances as a function of the bond dimension χ, to test the effectiveness of this approach against the latency of the inter-node communications. (iv) Tuning of parameters and algorithms: accelerators developed for machine learning applications have excellent support for lower and real precision. Tun- ing parameters over the different sweeps is benefi- cial, e.g. increasing the precision towards the end of the sweep. This approach profits from faster single- precision implementations during the first sweeps. Selecting algorithms like random singular value de- compositions can also bring benefits [161]. D. Sweeps and HPC parallelization So far, we have parallelized single tensor optimizations within a sweep, but the sweep itself was sequential, i.e. serial. Recent works formulated parallel versions of MPS algorithms for ground state search and time evolution, e.g. via the Message Passing Interface (MPI) [162–164].12 The main difference between the serial and parallel algo- rithms is the effective operators used in the optimization. In the serial version, the effective operators contain the information of the most recent version of all other ten- sors. This update is delayed in the parallel version, i.e. the tensor that entered the effective operator is not neces- sarily the one of the current sweep but can be the version of an earlier sweep. If the delay becomes an obstacle to convergence, there is the option to modify parameters during the sweeps. Typically, ten to fifty sweeps are necessary to converge to a solution. As the initial state is usually random, MPI can be used especially at the beginning. To ensure con- vergence, one can consider serial steps at the end; even a gradual reduction of the MPI processes as the sweeps proceed is possible and gradually reduces the delays. Considering the MPS scenario of a chain in Fig. 6(b), we split the chain into equal parts of ( N/nMPI−threads) sites. Each part of the chain communicates with its two neighbors apart from the two boundaries. The effective operators take into account the tensors of the same MPI process with zero delay as in the serial case. The tensors of the i-th neighboring MPI process have a delay of i. The worst-case scenario of the delay scales linearly with the number of MPI processes. The delay can be avoided by communicating the effective operators after each up- date through the chain, which is a quasi-serial step with no more than two MPI processes active at the same time. The problem becomes more complicated for the TTNs suggested for LGT, but we expect a benefit for the par- allelization of a TTN versus an MPS for higher dimen- sional systems. Fig. 6(c) shows an example of how each MPI process gets assigned a sub-tree within the com- plete TTN. Unlike the MPS, the number of neighbor- ing MPI processes for communication is at least three and increases with ( N/nMPI−threads) tensors per thread. For simplicity, we assume equally shaped subtrees for all MPI processes. The delay of the tensor update is now 2 · log2(N) in the worst-case scenario. One-dimensional systems with nearest-neighbor inter- actions thus exhibit a delay of 1 in the worst-case sce- nario in the MPS, while the delay is up to 2 ·log2(N) for the nearest neighbors in the center of the TTN. Rather, higher dimensional systems change this aspect, e.g., for an N × N two-dimensional system mapped to 1D via a zig-zag mapping. The MPS has a worst-case delay of the tensor update of N for the slow index. In contrast, the TTN has the same log behavior and a maximum de- lay at the center of the TTN as 2 · log2(N2). Thus, the worst-case delay is equal for 16 × 16 systems; increasing N further, TTNs exhibit smaller maximum delays dur- ing parallel sweeps. Moreover, the TTN is unaffected by the type of mapping used; in contrast, the worst-case delay for the MPS grows to 2 N for the snake mapping and to at least N2/2 for the Hilbert curve [165]. Equal arguments hold for 3D systems and delays of N2 (MPS, zig-zag) versus 2log2(N3) (TTN, any mapping). To get an intuition of what parallelized simulations can solve, we sketch out the specifications for a parallel simulation on the pre-exascale cluster Leonardo hosted by Cineca. We choose an MPI approach together with the GPUs. Leonardo has 3456 nodes with four GPUs totaling 13824 GPUs available for the complete cluster. Bond dimensions on the order of χ = 450 consume 54GB of memory without effective operators (assuming dou- ble complex precision, 40 Lanczos vectors) and allow to solve the eigenproblem on the GPU without temporar- ily storing data on the CPU. We use the single-tensor per MPI-thread with χ = 450 as a baseline where we ex- tract a rough empirical estimate with Leonardo; in detail, we use a 2D quantum Ising model with Z2 symmetry in the vicinity of the quantum critical point and the initial tensor optimizations [166, 167]. Due to the delay of the tensor in the effective operators, the minimum number of sweeps must be beyond 24. Then, we consider the scal- ing of the TTN previously introduced and generate Ta- ble II with an overview of different system sizes and bond dimensions. These results provide a coarse-grained esti- mate, since plaquette terms, different symmetries, entan- glement generation, and optimization time within later sweeps could further impact the computational time. Our estimate predicts that a system of 256 × 256 sites takes about two months for bond dimension χ = 450 on 1024 GPU nodes of Cineca’s leonardo. Future improve- ments are likely to bring this simulation time down, e.g., the next-generation GPUs in comparison to the A100 or further optimization in data movement. In contrast, a three-dimensional system with large entanglement and many sites requires three to four orders of magnitude improvement, where cluster size and other improvements have to come together to reach this challenge. Further- more, Table II provides an estimate of the boundary for a potential quantum advantage in simulating lattice gauge theories with quantum computers or simulators. E. Finite temperature regime To date, TN simulations of high-dimensional LGTs including dynamical matter are exploring zero temper- ature regimes, which are important to understand the low-energy properties of the models. To explore finite temperature phenomena, particularly relevant for open research problems such as the QCD phase diagram, tech- nical and numerical challenges have to be tackled, e.g. devising and testing efficient TN algorithms for target- ing quantum states at equilibrium. As suggested in the next paragraph, we foresee two possible paths toward fi- nite temperature TN states. Matrix product density operators (MPDOs) and lo- cally purified tensor networks (LPTNs) provide already today the option to tackle finite temperature regimes via an imaginary time evolution [168–170]. Herein, the algorithm starts at the infinite temperature state and starts “cooling” the system via a specified number of time steps and specified step size to reach a given tem-13 <latexit sha1_base64=\"tL6+qY++4p38XMPxjWctrLhjglM=\">AAACCXicbVC7TsNAEDzzDOEVQFQ0JyIkqshGIKhQBA1lkMhDiq3ofNkkp5zP1t0aKTL5Ar6CFio6RMtXUPAv2MYFJEw1mtnVzo4fSWHQtj+thcWl5ZXV0lp5fWNza7uys9syYaw5NHkoQ93xmQEpFDRRoIROpIEFvoS2P77O/PY9aCNCdYeTCLyADZUYCM4wlXqVfVfCAOkDdSMjqKvFcIT0slep2jU7B50nTkGqpECjV/ly+yGPA1DIJTOm69gRegnTKLiEadmNDUSMj9kQuilVLADjJXn8KT2KDcOQRqCpkDQX4fdGwgJjJoGfTgYMR2bWy8T/vG6MgwsvESqKERTPDqGQkB8yXIu0F6B9oQGRZcmBCkU50wwRtKCM81SM06LKaR/O7PfzpHVSc85q9u1ptX5VNFMiB+SQHBOHnJM6uSEN0iScJOSJPJMX69F6td6s95/RBavY2SN/YH18A/rimWk=</latexit> | i <latexit sha1_base64=\"w1zI8qMVuzZ6aEt8W66eMiO5m7Q=\">AAACCXicbVC7TsNAEDzzDOEVQFQ0JyIkqshGICgoImgog0QeUmxF58smOeV8tu7WSJHJF/AVtFDRIVq+goJ/wTYuIGGq0cyudnb8SAqDtv1pLSwuLa+sltbK6xubW9uVnd2WCWPNoclDGeqOzwxIoaCJAiV0Ig0s8CW0/fF15rfvQRsRqjucROAFbKjEQHCGqdSr7LsSBkgvqRsZQV0thiOkD71K1a7ZOeg8cQpSJQUavcqX2w95HIBCLpkxXceO0EuYRsElTMtubCBifMyG0E2pYgEYL8njT+lRbBiGNAJNhaS5CL83EhYYMwn8dDJgODKzXib+53VjHFx4iVBRjKB4dgiFhPyQ4VqkvQDtCw2ILEsOVCjKmWaIoAVlnKdinBZVTvtwZr+fJ62TmnNWs29Pq/WropkSOSCH5Jg45JzUyQ1pkCbhJCFP5Jm8WI/Wq/Vmvf+MLljFzh75A+vjG/REmWc=</latexit> h | <latexit sha1_base64=\"I0OcdTLU7qoQvZ5LLRQkXA1hp+s=\">AAAB9nicbVC7TsNAEDyHVwivACXNiQiJKrIRCMpIUFAGiTykxIrWl01yyvmhuzUisvILtFDRIVp+h4J/wTYuIGGq0cyudna8SElDtv1plVZW19Y3ypuVre2d3b3q/kHbhLEW2BKhCnXXA4NKBtgiSQq7kUbwPYUdb3qd+Z0H1EaGwT3NInR9GAdyJAVQJvXFRA6qNbtu5+DLxClIjRVoDqpf/WEoYh8DEgqM6Tl2RG4CmqRQOK/0Y4MRiCmMsZfSAHw0bpJnnfOT2ACFPELNpeK5iL83EvCNmfleOukDTcyil4n/eb2YRlduIoMoJgxEdoikwvyQEVqmJSAfSo1EkCVHLgMuQAMRaslBiFSM01YqaR/O4vfLpH1Wdy7q9t15rXFTNFNmR+yYnTKHXbIGu2VN1mKCTdgTe2Yv1qP1ar1Z7z+jJavYOWR/YH18A64Qkr0=</latexit> \u0000 <latexit sha1_base64=\"OAI/48igXcy82Np+Ta7q6T6Tg1s=\">AAAB83icbVC7TsNAEDyHVwivACXNiQiJKrIRCMoImpSJRB5SYkXnyyaccj5bd3tIkZUvoIWKDtHyQRT8C7ZxAQlTjWZ2tbMTxFIYdN1Pp7S2vrG5Vd6u7Ozu7R9UD4+6JrKaQ4dHMtL9gBmQQkEHBUroxxpYGEjoBbO7zO89gjYiUvc4j8EP2VSJieAMU6ndHFVrbt3NQVeJV5AaKdAaVb+G44jbEBRyyYwZeG6MfsI0Ci5hURlaAzHjMzaFQUoVC8H4SR50Qc+sYRjRGDQVkuYi/N5IWGjMPAzSyZDhg1n2MvE/b2BxcuMnQsUWQfHsEAoJ+SHDtUgbADoWGhBZlhyoUJQzzRBBC8o4T0WbVlJJ+/CWv18l3Yu6d1V325e1xm3RTJmckFNyTjxyTRqkSVqkQzgB8kSeyYtjnVfnzXn/GS05xc4x+QPn4xs9C5FV</latexit> H <latexit sha1_base64=\"w38u6i/To2w/CRluGFKhYbct1Ow=\">AAAB83icbVC7TsNAEDyHVwivACXNiQiJKrIRCMoIGspEyktKrOh82YRTzmfrbg8psvIFtFDRIVo+iIJ/wTYuIGGq0cyudnaCWAqDrvvplNbWNza3ytuVnd29/YPq4VHXRFZz6PBIRrofMANSKOigQAn9WAMLAwm9YHaX+b1H0EZEqo3zGPyQTZWYCM4wlVrtUbXm1t0cdJV4BamRAs1R9Ws4jrgNQSGXzJiB58boJ0yj4BIWlaE1EDM+Y1MYpFSxEIyf5EEX9MwahhGNQVMhaS7C742EhcbMwyCdDBk+mGUvE//zBhYnN34iVGwRFM8OoZCQHzJci7QBoGOhAZFlyYEKRTnTDBG0oIzzVLRpJZW0D2/5+1XSvah7V3W3dVlr3BbNlMkJOSXnxCPXpEHuSZN0CCdAnsgzeXGs8+q8Oe8/oyWn2Dkmf+B8fANPv5Fh</latexit> T <latexit sha1_base64=\"PhvF9KWEEbtX8mTFmdUdJszEF+U=\">AAAB/XicbVC7TsNAEDzzDOEVoKQ5ESFRRTYCQRlBQxmkvKQkROvLJpxyPlt3a6TIivgKWqjoEC3fQsG/YJsUkDDVaGZXOzt+pKQl1/10lpZXVtfWCxvFza3tnd3S3n7ThrER2BChCk3bB4tKamyQJIXtyCAEvsKWP77O/NYDGitDXadJhL0ARloOpQBKpU79LukOYDRCM+2Xym7FzcEXiTcjZTZDrV/66g5CEQeoSSiwtuO5EfUSMCSFwmmxG1uMQIxhhJ2UagjQ9pI88pQfxxYo5BEaLhXPRfy9kUBg7STw08kA6N7Oe5n4n9eJaXjZS6SOYkItskMkFeaHrDAy7QL5QBokgiw5cqm5AANEaCQHIVIxTssppn14898vkuZpxTuvuLdn5erVrJkCO2RH7IR57IJV2Q2rsQYTLGRP7Jm9OI/Oq/PmvP+MLjmznQP2B87HNz7HleE=</latexit> T † <latexit sha1_base64=\"w38u6i/To2w/CRluGFKhYbct1Ow=\">AAAB83icbVC7TsNAEDyHVwivACXNiQiJKrIRCMoIGspEyktKrOh82YRTzmfrbg8psvIFtFDRIVo+iIJ/wTYuIGGq0cyudnaCWAqDrvvplNbWNza3ytuVnd29/YPq4VHXRFZz6PBIRrofMANSKOigQAn9WAMLAwm9YHaX+b1H0EZEqo3zGPyQTZWYCM4wlVrtUbXm1t0cdJV4BamRAs1R9Ws4jrgNQSGXzJiB58boJ0yj4BIWlaE1EDM+Y1MYpFSxEIyf5EEX9MwahhGNQVMhaS7C742EhcbMwyCdDBk+mGUvE//zBhYnN34iVGwRFM8OoZCQHzJci7QBoGOhAZFlyYEKRTnTDBG0oIzzVLRpJZW0D2/5+1XSvah7V3W3dVlr3BbNlMkJOSXnxCPXpEHuSZN0CCdAnsgzeXGs8+q8Oe8/oyWn2Dkmf+B8fANPv5Fh</latexit> T <latexit sha1_base64=\"qMtnurlJnN9CS2OY5z+5F8gI7AI=\">AAAB+XicbVC7TsNAEDzzDOEVoKQ5ESFRRTYCQRlBkzJI5CElVrS+rMMp54fu1kiRlY+ghYoO0fI1FPwLtnEBCVONZna1s+PFShqy7U9rZXVtfWOzslXd3tnd268dHHZNlGiBHRGpSPc9MKhkiB2SpLAfa4TAU9jzpre533tEbWQU3tMsRjeASSh9KYAyqdcapej781GtbjfsAnyZOCWpsxLtUe1rOI5EEmBIQoExA8eOyU1BkxQK59VhYjAGMYUJDjIaQoDGTYu4c36aGKCIx6i5VLwQ8fdGCoExs8DLJgOgB7Po5eJ/3iAh/9pNZRgnhKHID5FUWBwyQsusB+RjqZEI8uTIZcgFaCBCLTkIkYlJVkw168NZ/H6ZdM8bzmXDvruoN2/KZirsmJ2wM+awK9ZkLdZmHSbYlD2xZ/Zipdar9Wa9/4yuWOXOEfsD6+MbFN6UGQ==</latexit> Heff <latexit sha1_base64=\"PhvF9KWEEbtX8mTFmdUdJszEF+U=\">AAAB/XicbVC7TsNAEDzzDOEVoKQ5ESFRRTYCQRlBQxmkvKQkROvLJpxyPlt3a6TIivgKWqjoEC3fQsG/YJsUkDDVaGZXOzt+pKQl1/10lpZXVtfWCxvFza3tnd3S3n7ThrER2BChCk3bB4tKamyQJIXtyCAEvsKWP77O/NYDGitDXadJhL0ARloOpQBKpU79LukOYDRCM+2Xym7FzcEXiTcjZTZDrV/66g5CEQeoSSiwtuO5EfUSMCSFwmmxG1uMQIxhhJ2UagjQ9pI88pQfxxYo5BEaLhXPRfy9kUBg7STw08kA6N7Oe5n4n9eJaXjZS6SOYkItskMkFeaHrDAy7QL5QBokgiw5cqm5AANEaCQHIVIxTssppn14898vkuZpxTuvuLdn5erVrJkCO2RH7IR57IJV2Q2rsQYTLGRP7Jm9OI/Oq/PmvP+MLjmznQP2B87HNz7HleE=</latexit> T † (a) MPI t=0 =2Δ1 MPI t=1 =0Δ1 MPI t=2 =2Δ1 MPI t=3 =2Δ1 MPI t=4 =3Δ1 MPI t=5 =3Δ1 MPI t=6 =3Δ1 MPI t=7 =3Δ1 MPI t=8 =1Δ1 =2Δ1 MPI t=9 MPI communication MPI t=0 =1Δ1 =0Δ1 =1Δ1 =2Δ1 MPI t=1 MPI t=2 MPI t=3 (b) (c) FIG. 6. Effective operators and parallel tensor networks. (a) Procedure for optimizing a TTN to find the ground state of a QMB system: the energy is computed by contracting the Hamiltonian H (yellow tensor) with the TTN, representing the state |ψ⟩, and its hermitian conjugate, representing ⟨ψ|. The variational optimization starts from a target tensor T (red tensor), by computing its effective Hamiltonian Heff and then solving the local eigenvalue problem for the latter. The tensor T is then updated with the newly found ground state, and the procedure iterates over all the tensors in the network (sweep). (b) The workload itself consists of optimizing each tensor held by the MPI thread t, which requires effective operators calculated by other MPI threads. We dub delays ∆i the number of optimization cycles needed to obtain the information of tensors in the i-th MPI thread in another MPI thread via MPI communication. Matrix product states naturally split into sub-chains which communicate with one or two neighboring MPI threads to obtain updated effective operators. Delays for updates scale with the distance between two MPI threads along the chain. Each MPI thread can use threading or openMP, e.g. in a hybrid openMP-MPI approach. (c) Similarly, TTNs can be split into sub-trees for each MPI thread allowing for optimizing the sub- tree without communication with other MPI threads. Delays due to updating scale logarithmically as any distance in a tree network. System size χ Factor Estimated walltime 64 × 64 450 Tbase 4.16 days 64 × 64 900 16 · Tbase 66.6 days 256 × 256 450 28 · Tbase 116.5 days 256 × 256 900 448 · Tbase 5.1 years 16 × 16 × 16 450 4 · Tbase 16.6 days 16 × 16 × 16 900 64 · Tbase 266 days 64 × 64 × 64 450 1984 · Tbase 23 years 64 × 64 × 64 900 31744 · Tbase 362 years TABLE II. Estimated simulation time. We derive the baseline from a single-tensor optimization of a 64 × 64 quantum Ising simulation with Z2 symmetry taking 7192s on a A100 GPU. Further, we assume that single-tensor update, one tensor and one GPU per MPI thread, and 50 sweeps for the baseline. To extrapolate to larger systems, we assume a scaling with O(χ4ND−1) as well as seven (thirty-one) tensors per MPI thread for 256 × 256 (64 × 64 × 64) systems. The empirical scalings are approximately a factor of 2 .3 for doubling the system size and 13 for doubling the bond dimension, which we obtain from smaller simulations with χ = 225 and for 32 × 32 qubits. The times are valid for any d < χ. perature. In its original formulation, both approaches are one-dimensional chains. Matrix product density opera- tor can be formulated as TTN but faces some challenges in terms of the question of positivity [171] or integrating symmetries. In contrast, tree tensor operators (TTO) are the tree-equivalent of an LPTN; they are also a posi- tive loopless representation of density matrices, recently introduced in [172]. However, TTOs cannot represent the infinite temperature state necessary for the imagi- nary time evolution approach. Instead, a possible use of the TTO employed in LGT simulations consists of a variational algorithm to target finite-temperature states or reconstruct open-system quantum dynamics, by effi- ciently compressing the relevant information. The TTO enables useful measures, e.g. computing the entangle- ment of formation as already shown for representative one-dimensional models at finite temperature [172]. IV. CONCLUSIONS The field of TN methods for LGTs has shown great potential in the last decade, during which significant ef- forts have been devoted to developing numerical algo- rithms and strategies that can be seen as a complemen- tary approach to MC simulations for high-energy physics. The validity of sign-problem-free TN algorithms has been proven for one-dimensional LGT models for both Abelian14 and non-Abelian scenarios. Recently, TN methods have also been applied to higher-dimensional LGTs with sim- ple truncated gauge groups and small-to-intermediate lattice sizes. On the one hand, these results prove the effectiveness of TN methods for simulating LGTs even in regimes that are problematic for other numerical meth- ods; on the other hand, they highlight the challenges that one needs to tackle to address state-of-the-art research problems, such as accessing the continuum limits or sim- ulating high-dimensional QCD on large lattices. In this work, after a general overview of TN meth- ods and their use for LGT simulations, we have de- scribed these challenges, starting from the problem of the very large local basis required for complex gauge groups and then discussing several computational bottlenecks in terms of bond dimensions and system sizes. To mitigate and possibly overcome these problems soon, we have presented a feasible roadmap, in terms of algorithmic developments and numerical strategies that might have a concrete impact in extending the range of applicability of TN algorithms to current research problems in high-energy physics. We foresee that all the presented steps could potentially have a key role in making the TN approach competitive as a comple- mentary method to MC techniques for simulating high- dimensional LGTs, such as large-scale QCD. ACKNOWLEDGMENTS The authors acknowledge financial support from the following institutions: the European Union (EU) via the Horizon 2020 research and innovation program (Quan- tum Flagship) projects PASQuanS2 and Euryqa, via the NextGenerationEU project CN00000013 - Italian Re- search Center on HPC, Big Data and Quantum Comput- ing (ICSC), and via the QuantERA projects T-NiSQ and QuantHEP; the Italian Ministry of University and Re- search (MUR) via the Departments of Excellence grants 2023-2027 projects Quantum Frontiers and Quantum Sensing and Modelling for One-Health (QuaSiModO), and via PRIN2022 project TANQU; the Italian Istituto Nazionale di Fisica Nucleare (INFN) via specific initia- tives IS-QUANTUM and IS-NPQCD; the German Fed- eral Ministry of Education and Research (BMBF) via project QRydDemo; the University of Bari via the 2023- UNBACLE-0244025 grant; the World-Class Research Infrastructure − Quantum Computing and Simulation Center (QCSC) of Padova University. We acknowledge computational resources by the Cloud Veneto, CINECA, the BwUniCluster, the University of Padova Strategic Research Infrastructure Grant 2017: “CAPRI: Calcolo ad Alte Prestazioni per la Ricerca e l’Innovazione”, and ReCaS Bari. [1] H. Kleinert, Gauge Fields in Condensed Matter (WORLD SCIENTIFIC, 1989). [2] E. Fradkin, Field Theories of Condensed Matter Physics, 2nd ed. (Cambridge University Press, Cam- bridge, 2013). [3] I. Ichinose and T. Matsui, Lattice gauge theory for condensed matter physics: ferromagnetic superconduc- tivity as its example, Modern Physics Letters B 28, 1430012 (2014). [4] M. E. Peskin and D. V. Schroeder, An introduction to quantum field theory (Westview, Boulder, CO, 1995). [5] M. D. Schwartz, Quantum Field Theory and the Stan- dard Model (Cambridge University Press, 2013). [6] K. G. Wilson, Confinement of quarks, Phys. Rev. D 10, 2445 (1974). [7] J. B. Kogut, An introduction to lattice gauge theory and spin systems, Reviews of Modern Physics 51, 659 (1979). [8] M. Creutz, L. Jacobs, and C. Rebbi, Monte Carlo com- putations in lattice gauge theories, Physics Reports 95, 201 (1983). [9] Z. Davoudi, E. T. Neil, C. W. Bauer, T. Bhattacharya, T. Blum, P. Boyle, R. C. Brower, S. Catterall, N. H. Christ, V. Cirigliano, G. Colangelo, C. DeTar, W. Det- mold, R. G. Edwards, A. X. El-Khadra, S. Gottlieb, R. Gupta, D. C. Hackett, A. Hasenfratz, T. Izubuchi, W. I. Jay, L. Jin, C. Kelly, A. S. Kronfeld, C. Lehner, H.-W. Lin, M. Lin, A. T. Lytle, S. Meinel, Y. Meurice, S. Mukherjee, A. Nicholson, S. Prelovsek, M. J. Sav- age, P. E. Shanahan, R. S. Van De Water, M. L. Wag- man, and O. Witzel, Report of the Snowmass 2021 Top- ical Group on Lattice Gauge Theory, arXiv e-prints , arXiv:2209.10758 (2022), arXiv:2209.10758 [hep-lat]. [10] K. Nagata, Finite-density lattice qcd and sign problem: Current status and open problems, Progress in Particle and Nuclear Physics 127, 103991 (2022). [11] M. C. Banuls, K. Cichy, J. I. Cirac, K. Jansen, and S. K¨ uhn, Tensor networks and their use for lattice gauge theories, in Proceedings of The 36th Annual International Symposium on Lattice Field Theory — PoS(LATTICE2018), Vol. 334 (Sissa Medialab, 2019) p. 022. [12] S. Montangero, Introduction to Tensor Network Meth- ods: Numerical Simulations of Low-Dimensional Many- Body Quantum Systems (Springer International Pub- lishing, Cham, 2018). [13] R. Or´ us, Tensor networks for complex quantum systems, Nature Reviews Physics 1, 538 (2019). [14] M. C. Ba˜ nuls, Tensor network algorithms: A route map, Annual Review of Condensed Matter Physics 14, null (2023). [15] T. M. R. Byrnes, P. Sriganesh, R. J. Bursill, and C. J. Hamer, Density matrix renormalization group approach to the massive schwinger model, Phys. Rev. D 66, 013002 (2002). [16] P. Silvi, E. Rico, T. Calarco, and S. Montangero, Lat- tice gauge tensor networks, New Journal of Physics 16, 103015 (2014). [17] T. Pichler, M. Dalmonte, E. Rico, P. Zoller, and S. Mon- tangero, Real-Time Dynamics in U(1) Lattice Gauge Theories with Tensor Networks, Physical Review X 6, 011023 (2016).15 [18] P. Silvi, F. Tschirsich, M. Gerster, J. J¨ unemann, D. Jaschke, M. Rizzi, and S. Montangero, The Tensor Networks Anthology: Simulation techniques for many- body quantum lattice systems, SciPost Physics Lecture Notes , 008 (2019). [19] L. Funcke, K. Jansen, and S. K¨ uhn, Topological vacuum structure of the schwinger model with matrix product states, Phys. Rev. D 101, 054507 (2020). [20] B. Buyens, J. Haegeman, K. Van Acoleyen, H. Ver- schelde, and F. Verstraete, Matrix Product States for Gauge Field Theories, Physical Review Letters 113, 091601 (2014). [21] E. Rico, T. Pichler, M. Dalmonte, P. Zoller, and S. Mon- tangero, Tensor Networks for Lattice Gauge Theories and Atomic Quantum Simulation, Physical Review Let- ters 112, 201601 (2014). [22] J. Haegeman, K. Van Acoleyen, N. Schuch, J. I. Cirac, and F. Verstraete, Gauging quantum states: From global to local symmetries in many-body systems, Phys. Rev. X 5, 011024 (2015). [23] P. Silvi, E. Rico, M. Dalmonte, F. Tschirsich, and S. Montangero, Finite-density phase diagram of a (1 + 1) −d non-abelian lattice gauge theory with tensor net- works, Quantum 1, 9 (2017). [24] B. Buyens, S. Montangero, J. Haegeman, F. Verstraete, and K. Van Acoleyen, Finite-representation approxima- tion of lattice gauge theories at the continuum limit with tensor networks, Phys. Rev. D 95, 094509 (2017). [25] M. C. Ba˜ nuls, K. Cichy, J. I. Cirac, K. Jansen, and S. K¨ uhn, Density induced phase transitions in the schwinger model: A study with matrix product states, Phys. Rev. Lett. 118, 071601 (2017). [26] E. Ercolessi, P. Facchi, G. Magnifico, S. Pascazio, and F. V. Pepe, Phase transitions in Zn gauge models: To- wards quantum simulations of the schwinger-weyl qed, Physical Review D 98, 074503 (2018). [27] G. Magnifico, D. Vodola, E. Ercolessi, S. P. Ku- mar, M. M¨ uller, and A. Bermudez, Symmetry-protected topological phases in lattice gauge theories: Topological qed2, Phys. Rev. D 99, 014503 (2019). [28] G. Magnifico, D. Vodola, E. Ercolessi, S. P. Kumar, M. M¨ uller, and A. Bermudez,zN gauge theories coupled to topological fermions: qed 2 with a quantum mechan- ical θ angle, Phys. Rev. B 100, 115152 (2019). [29] P. Sala, T. Shi, S. K¨ uhn, M. C. Banuls, E. Demler, and J. I. Cirac, Gaussian states for the variational study of (1+1)-dimensional lattice gauge models, in The 36th Annual International Symposium on Lattice Field The- ory. 22-28 July (2018) p. 230. [30] G. Magnifico, M. Dalmonte, P. Facchi, S. Pascazio, F. V. Pepe, and E. Ercolessi, Real Time Dynamics and Con- finement in the Zn Schwinger-Weyl lattice model for 1+1 QED, Quantum 4, 281 (2020). [31] M. C. Ba˜ nuls, K. Cichy, J. I. Cirac, K. Jansen, and S. K¨ uhn, Efficient basis formulation for (1 + 1)- dimensional su(2) lattice gauge theory: Spectral cal- culations with matrix product states, Phys. Rev. X 7, 041046 (2017). [32] M. Rigobello, S. Notarnicola, G. Magnifico, and S. Mon- tangero, Entanglement generation in (1 + 1)D qed scat- tering processes, Phys. Rev. D 104, 114501 (2021). [33] L. Funcke, K. Jansen, and S. K¨ uhn, Exploring the CP- violating Dashen phase in the Schwinger model with tensor networks, Physical Review D108, 014504 (2023), arXiv:2303.03799 [hep-lat]. [34] T. Angelides, L. Funcke, K. Jansen, and S. K¨ uhn, Computing the Mass Shift of Wilson and Staggered Fermions in the Lattice Schwinger Model with Matrix Product States, Physical Review D 108, 014516 (2023), arXiv:2303.11016 [hep-lat]. [35] T. Chanda, M. Dalmonte, M. Lewenstein, J. Za- krzewski, and L. Tagliacozzo, Spectral Properties of Critical 1+1D Abelian-Higgs Model , Tech. Rep. arXiv:2304.01030 (2023) arXiv:2304.01030 [hep-th]. [36] P. Schmoll, J. Naumann, A. Nietner, J. Eisert, and S. Sotiriadis, Hamiltonian truncation tensor networks for quantum field theories (2023), arXiv:2312.12506 [quant-ph]. [37] T. Hayata, Y. Hidaka, and K. Nishimura, Dense QCD 2 with matrix product states (2023), arXiv:2311.11643 [hep-lat]. [38] A. Florio, A. Weichselbaum, S. Valgushev, and R. D. Pisarski, Mass gaps of a Z3 gauge theory with three fermion flavors in 1 + 1 dimensions (2023), arXiv:2310.18312 [hep-th]. [39] J. Osborne, I. P. McCulloch, and J. C. Halimeh, Probing confinement through dynamical quantum phase transi- tions: From quantum spin models to lattice gauge the- ories (2023), arXiv:2310.12210 [cond-mat.quant-gas]. [40] M. Kebriˇ c, J. C. Halimeh, U. Schollw¨ ock, and F. Grusdt, Confinement in 1+1d Z2 lattice gauge theories at finite temperature (2024), arXiv:2308.08592 [cond-mat.quant- gas]. [41] R. Belyansky, S. Whitsitt, N. Mueller, A. Fahimniya, E. R. Bennewitz, Z. Davoudi, and A. V. Gorshkov, High- energy collision of quarks and mesons in the schwinger model: From tensor networks to circuit qed, Phys. Rev. Lett. 132, 091903 (2024). [42] I. Papaefstathiou, J. Knolle, and M. C. Ba˜ nuls, Real- time scattering in the lattice schwinger model (2024), arXiv:2402.18429 [hep-lat]. [43] G. Calaj` o, G. Cataldi, M. Rigobello, D. Wanisch, G. Magnifico, P. Silvi, S. Montangero, and J. C. Hal- imeh, Quantum Many-Body Scarring in a Non-Abelian Lattice Gauge Theory (2024), arxiv:2405.13112 [cond- mat, physics:hep-lat, physics:quant-ph]. [44] T. Felser, P. Silvi, M. Collura, and S. Montangero, Two- Dimensional Quantum-Link Lattice Quantum Electro- dynamics at Finite Density, Physical Review X 10, 041040 (2020). [45] P. Emonts, A. Kelman, U. Borla, S. Moroz, S. Gazit, and E. Zohar, Finding the ground state of a lattice gauge theory with fermionic tensor networks: a 2 + 1 d Z2 demonstrationfinding the ground state of a lattice gauge theory with fermionic tensor networks: a 2+1d Z2 demonstration, Physical Review D 107, 014505 (2023). [46] G. Magnifico, T. Felser, P. Silvi, and S. Montangero, Lattice quantum electrodynamics in (3+1)-dimensions at finite density with tensor networks, Nature Commu- nications 12, 3600 (2021). [47] J. Knaute, M. Feuerstein, and E. Zohar, Entangle- ment and confinement in lattice gauge theory ten- sor networks, Journal of High Energy Physics 2024, 10.1007/jhep02(2024)174 (2024). [48] S. Pradhan, A. Maroncelli, and E. Ercolessi, Discrete abelian lattice gauge theories on a ladder and their du- alities with quantum clock models, Physical Review B 109, 10.1103/physrevb.109.064410 (2024).16 [49] G.-X. Su, J. Osborne, and J. C. Halimeh, A cold- atom particle collider (2024), arXiv:2401.05489 [cond- mat.quant-gas]. [50] G. Cataldi, G. Magnifico, P. Silvi, and S. Mon- tangero, (2+1)D SU(2) Yang-Mills Lattice Gauge Theory at finite density via tensor networks (2023), arxiv:2307.09396 [cond-mat, physics:hep-lat, physics:hep-th, physics:quant-ph]. [51] B. Villalonga, S. Boixo, B. Nelson, C. Henze, E. Rieffel, R. Biswas, and S. Mandr` a, A flexible high-performance simulator for verifying and benchmarking quantum cir- cuits implemented on real hardware, npj Quantum In- formation 5, 10.1038/s41534-019-0196-1 (2019). [52] S. V. Mathis, G. Mazzola, and I. Tavernelli, Toward scalable simulations of lattice gauge theories on quan- tum computers, Physical Review D 102, 094501 (2020). [53] Y. Zhou, E. M. Stoudenmire, and X. Waintal, What lim- its the simulation of quantum computers?, Phys. Rev. X 10, 041038 (2020). [54] C. Huang, F. Zhang, M. Newman, X. Ni, D. Ding, J. Cai, X. Gao, T. Wang, F. Wu, G. Zhang, H.-S. Ku, Z. Tian, J. Wu, H. Xu, H. Yu, B. Yuan, M. Szegedy, Y. Shi, H.-H. Zhao, C. Deng, and J. Chen, Efficient par- allelization of tensor network contraction for simulating quantum computation, Nature Computational Science 1, 578 (2021). [55] R. Haghshenas, J. Gray, A. C. Potter, and G. K.-L. Chan, Variational power of quantum circuit tensor net- works, Phys. Rev. X 12, 011047 (2022). [56] E. Zohar, Quantum simulation of lattice gauge theories in more than one space dimension—requirements, chal- lenges and methods, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineer- ing Sciences 380, 20210069 (2021). [57] S. Catterall, R. Harnik, V. E. Hubeny, C. W. Bauer, A. Berlin, Z. Davoudi, T. Faulkner, T. Hartman, M. Headrick, Y. F. Kahn, H. Lamm, Y. Meurice, S. Rajendran, M. Rangamani, and B. Swingle, Re- port of the Snowmass 2021 Theory Frontier Topical Group on Quantum Information Science , Tech. Rep. FERMILAB-FN-1199-T; arXiv:2209.14839 (2022). [58] D. Pomarico, L. Cosmai, P. Facchi, C. Lupo, S. Pas- cazio, and F. V. Pepe, Dynamical Quantum Phase Transitions of the Schwinger Model: Real-Time Dy- namics on IBM Quantum, Entropy 25, 608 (2023), arxiv:2302.01151 [quant-ph]. [59] L. Funcke, T. Hartung, K. Jansen, and S. K¨ uhn, Re- view on Quantum Computing for Lattice Field Theory, in Proceedings of The 39th International Symposium on Lattice Field Theory — PoS(LATTICE2022) , Vol. 430 (SISSA Medialab, 2023) p. 228, arxiv:2302.00467. [60] A. Mariani, S. Pradhan, and E. Ercolessi, Hamiltonians and gauge-invariant Hilbert space for lattice Yang-Mills- like theories with finite gauge group, Physical Review D 107, 114513 (2023), arXiv:2301.12224 [quant-ph]. [61] T. V. Zache, D. Gonz´ alez-Cuadra, and P. Zoller, Fermion-qudit quantum processors for simulating lattice gauge theories with matter, Quantum 7, 1140 (2023). [62] J. Eisert, M. Cramer, and M. B. Plenio, Colloquium: Area laws for the entanglement entropy, Rev. Mod. Phys. 82, 277 (2010). [63] P. Calabrese and J. Cardy, Entanglement entropy and quantum field theory, Journal of Statistical Mechanics: Theory and Experiment 2004, P06002 (2004). [64] M. B. Hastings, An area law for one-dimensional quan- tum systems, Journal of Statistical Mechanics: Theory and Experiment 2007, P08024 (2007). [65] T. Kuwahara and K. Saito, Area law of noncritical ground states in 1d long-range interacting systems, Na- ture Communications 11, 4478 (2020). [66] J. Cho, Realistic area-law bound on entanglement from exponentially decaying correlations, Phys. Rev. X 8, 031009 (2018). [67] M. M. Wolf, F. Verstraete, M. B. Hastings, and J. I. Cirac, Area laws in quantum systems: Mutual infor- mation and correlations, Phys. Rev. Lett. 100, 070502 (2008). [68] L. Masanes, Area law for the entropy of low-energy states, Physical Review A 80, 052104 (2009). [69] E. Hamza, S. Michalakis, B. Nachtergaele, and R. Sims, Approximating the ground state of gapped quantum spin systems, Journal of Mathematical Physics 50, 095213 (2009). [70] M. J. Kastoryano, A. Lucia, and D. Perez-Garcia, Lo- cality at the Boundary Implies Gap in the Bulk for 2D PEPS, Communications in Mathematical Physics 366, 895 (2019). [71] J. I. Cirac, J. Garre-Rubio, and D. P´ erez-Garc´ ıa, Mathematical open problems in projected entangled pair states, Revista Matem´ atica Complutense 32, 579 (2019). [72] J. I. Cirac, D. P´ erez-Garc´ ıa, N. Schuch, and F. Ver- straete, Matrix product states and projected entangled pair states: Concepts, symmetries, theorems, Rev. Mod. Phys. 93, 045003 (2021). [73] D. Perez-Garcia, F. Verstraete, M. M. Wolf, and J. I. Cirac, Matrix product state representations, Quantum Info. Comput. 7, 401 (2007). [74] U. Schollw¨ ock, The density-matrix renormalization group in the age of matrix product states, Annals of Physics 326, 96 (2011). [75] G. K.-L. Chan, A. Keselman, N. Nakatani, Z. Li, and S. R. White, Matrix Product Operators, Matrix Prod- uct States, and Ab Initio Density Matrix Renormaliza- tion Group Algorithms , Tech. Rep. arXiv:1605.02611 (2016) arxiv:1605.02611 [cond-mat, physics:physics, physics:quant-ph]. [76] The scaling presented here does not consider the bond dimension of the mpo representing the hamiltonian. the bond dimension of the mpo depends on the dimension- ality of the system. in a simplified argument holding for the ttn, the maximum number of interactions cut for any bipartition gives a first intuition. [77] A. Gleis, J.-W. Li, and J. von Delft, Controlled Bond Expansion for Density Matrix Renormalization Group Ground State Search at Single-Site Costs, Physical Re- view Letters 130, 246402 (2023). [78] N. Schuch, M. M. Wolf, F. Verstraete, and J. I. Cirac, Computational complexity of projected entangled pair states, Phys. Rev. Lett. 98, 140506 (2007). [79] J. Eisert, Entanglement and tensor network states , Tech. Rep. arXiv:1308.3318 (2013) arxiv:1308.3318 [cond-mat, physics:quant-ph]. [80] L. Vanderstraeten, L. Burgelman, B. Ponsioen, M. Van Damme, B. Vanhecke, P. Corboz, J. Haege- man, and F. Verstraete, Variational methods for con- tracting projected entangled-pair states, Phys. Rev. B 105, 195140 (2022).17 [81] Y.-Y. Shi, L.-M. Duan, and G. Vidal, Classical simula- tion of quantum many-body systems with a tree tensor network, Phys. Rev. A 74, 022320 (2006). [82] X. Qian and M. Qin, From tree tensor network to multi- scale entanglement renormalization ansatz, Phys. Rev. B 105, 205102 (2022). [83] A. J. Ferris, Area law and real-space renormalization, Phys. Rev. B 87, 125139 (2013). [84] T. Felser, S. Notarnicola, and S. Montangero, Efficient tensor network ansatz for high-dimensional quantum many-body problems, Phys. Rev. Lett. 126, 170603 (2021). [85] D. Jaschke, M. L. Wall, and L. D. Carr, Open source matrix product states: Opening ways to simulate en- tangled many-body quantum systems in one dimension, Computer Physics Communications 225, 59 (2018). [86] G. Vidal, Efficient simulation of one-dimensional quan- tum many-body systems, Phys. Rev. Lett. 93, 040502 (2004). [87] J. Haegeman, J. I. Cirac, T. J. Osborne, I. Piˇ zorn, H. Verschelde, and F. Verstraete, Time-dependent vari- ational principle for quantum lattices, Phys. Rev. Lett. 107, 070601 (2011). [88] D. Bauernfeind and M. Aichhorn, Time dependent vari- ational principle for tree Tensor Networks, SciPost Phys. 8, 024 (2020). [89] L. Kohn, P. Silvi, M. Gerster, M. Keck, R. Fazio, G. E. Santoro, and S. Montangero, Superfluid-to-mott tran- sition in a bose-hubbard ring: Persistent currents and defect formation, Phys. Rev. A 101, 023617 (2020). [90] N. Schuch, M. M. Wolf, F. Verstraete, and J. I. Cirac, Entropy scaling and simulability by matrix product states, Phys. Rev. Lett. 100, 030504 (2008). [91] S. Paeckel, T. K¨ ohler, A. Swoboda, S. R. Manmana, U. Schollw¨ ock, and C. Hubig, Time-evolution meth- ods for matrix-product states, Annals of Physics 411, 167998 (2019). [92] C. D. White, M. Zaletel, R. S. K. Mong, and G. Refael, Quantum dynamics of thermalizing systems, Phys. Rev. B 97, 035127 (2018). [93] J. Surace, M. Piani, and L. Tagliacozzo, Simulating the out-of-equilibrium dynamics of local observables by trading entanglement for mixture, Phys. Rev. B 99, 235115 (2019). [94] L. Susskind, Lattice fermions, Physical Review D 16, 3031 (1977). [95] E. Zohar and M. Burrello, Formulation of lattice gauge theories for quantum simulations, Physical Review D 91, 054506 (2015). [96] P. Sala, T. Shi, S. K¨ uhn, M. C. Ba˜ nuls, E. Demler, and J. I. Cirac, Variational study of u(1) and su(2) lattice gauge theories with gaussian states in 1 + 1 dimensions, Phys. Rev. D 98, 034505 (2018). [97] J. Bender and E. Zohar, Gauge redundancy-free for- mulation of compact qed with dynamical matter for quantum and classical computations, Phys. Rev. D102, 114517 (2020). [98] D. Horn, Finite matrix models with continuous local gauge invariance, Physics Letters B 100, 149 (1981). [99] P. Orland and D. Rohrlich, Lattice gauge magnets: Local isospin from spin, Nuclear Physics B 338, 647 (1990). [100] S. Chandrasekharan and U. J. Wiese, Quantum link models: A discrete approach to gauge theories, Nuclear Physics B 492, 455 (1997). [101] R. Brower, S. Chandrasekharan, and U.-J. Wiese, QCD as a quantum link model, Physical Review D60, 094502 (1999). [102] T. Byrnes and Y. Yamamoto, Simulating lattice gauge theories on a quantum computer, Physical Review A73, 022328 (2006). [103] Z. Davoudi, M. Hafezi, C. Monroe, G. Pagano, A. Seif, and A. Shaw, Towards analog quantum simulations of lattice gauge theories with trapped ions, Physical Re- view Research 2, 023015 (2020). [104] G. Mazzola, S. V. Mathis, G. Mazzola, and I. Tavernelli, Gauge-invariant quantum circuits for$U$(1) and Yang- Mills lattice gauge theories, Physical Review Research 3, 043209 (2021). [105] A. Kan, L. Funcke, S. K¨ uhn, L. Dellantonio, J. Zhang, J. F. Haase, C. A. Muschik, and K. Jansen, Investigating a $(3+1)\\mathrm{D}$ topo- logical $\\ensuremath{\\theta}$-term in the Hamiltonian formulation of lattice gauge theories for quantum and classical simulations, Physical Review D 104, 034504 (2021). [106] C. W. Bauer, Z. Davoudi, N. Klco, and M. J. Sav- age, Quantum simulation of fundamental particles and forces, Nature Reviews Physics 5, 420 (2023). [107] J. F. Haase, L. Dellantonio, A. Celi, D. Paulson, A. Kan, K. Jansen, and C. A. Muschik, A resource efficient ap- proach for quantum and classical simulations of gauge theories in particle physics, Quantum 5, 393 (2021). [108] D. C. Hackett, K. Howe, C. Hughes, W. Jay, E. T. Neil, and J. N. Simone, Digitizing gauge fields: Lattice monte carlo results for future quantum computers, Phys. Rev. A 99, 062341 (2019). [109] T. V. Zache, D. Gonz´ alez-Cuadra, and P. Zoller, Quantum and classical spin network algorithms for $q$-deformed Kogut-Susskind gauge theories (2023), arxiv:2304.02527 [cond-mat, physics:hep-lat, physics:quant-ph]. [110] M. Rigobello, G. Magnifico, P. Silvi, and S. Montangero, Hadrons in (1+1)D Hamiltonian hardcore lattice QCD (2023), arxiv:2308.04488 [cond-mat, physics:hep-lat, physics:hep-ph, physics:physics, physics:quant-ph]. [111] F. Strocchi, An Introduction to Non-Perturbative Foun- dations of Quantum Field Theory , International Series of Monographs on Physics (Oxford University Press, Oxford, New York, 2013). [112] E. Zohar and J. I. Cirac, Removing staggered fermionic matter in U(N) and SU (N) lattice gauge theories, Physical Review D 99, 114511 (2019). [113] E. Zohar and J. I. Cirac, Eliminating fermionic matter fields in lattice gauge theories, Physical Review B 98, 075119 (2018). [114] M. Ballarin, G. Cataldi, G. Magnifico, D. Jaschke, M. Di Liberto, I. Siloi, S. Montangero, and P. Silvi, Scalable digital quantum simulation of lattice fermion theories with local encoding , Tech. Rep. (2023) arxiv:2310.15091. [115] F. Verstraete, V. Murg, and J. I. Cirac, Matrix product states, projected entangled pair states, and variational renormalization group methods for quantum spin sys- tems, Advances in Physics 57, 143 (2008). [116] A. Ciavarella, N. Klco, and M. J. Savage, Trailhead for quantum simulation of su(3) yang-mills lattice gauge theory in the local multiplet basis, Phys. Rev. D 103,18 094501 (2021). [117] Z. Davoudi, I. Raychowdhury, and A. Shaw, Search for efficient formulations for hamiltonian simulation of non- abelian lattice gauge theories, Phys. Rev. D104, 074505 (2021). [118] Y. Tong, V. V. Albert, J. R. McClean, J. Preskill, and Y. Su, Provably accurate simulation of gauge theories and bosonic systems, Quantum 6, 816 (2022). [119] R. Fiore, P. Giudice, D. Giuliano, D. Marmottini, A. Papa, and P. Sodano, QED 3 on a space-time lattice: A comparison between compact and noncompact formu- lation, in Proceedings of XXIIIrd International Sympo- sium on Lattice Field Theory — PoS(LAT2005), Vol. 20 (SISSA Medialab, 2005) p. 243. [120] O. Raviv, Y. Shamir, and B. Svetitsky, Nonperturba- tive beta function in three-dimensional electrodynamics, Physical Review D 90, 014512 (2014). [121] B. Svetitsky, O. Raviv, and Y. Shamir, Beta function of three-dimensional QED, in Proceedings of The 32nd International Symposium on Lattice Field Theory — PoS(LATTICE2014), Vol. 214 (SISSA Medialab, 2015) p. 051. [122] X. Y. Xu, Y. Qi, L. Zhang, F. F. Assaad, C. Xu, and Z. Y. Meng, Monte Carlo Study of Lattice Compact Quantum Electrodynamics with Fermionic Matter: The Parent State of Quantum Phases, Physical Review X 9, 021022 (2019). [123] M. Creutz, Lattice Gauge Theory and Monte Carlo Methods, Tech. Rep. BNL-42086 (1988). [124] M. Creutz, Lattice gauge theories and Monte Carlo al- gorithms, Nuclear Physics B - Proceedings Supplements 10, 1 (1989). [125] M. Loan, M. Brunner, C. Sloggett, and C. Hamer, Path integral Monte Carlo approach to the U(1) lat- tice gauge theory in 2+1 dimensions, Physical Review D 68, 034504 (2003). [126] H. J. Rothe, Lattice Gauge Theories: An Introduction (Fourth Edition) (World Scientific Publishing Company, 2012). [127] L. Funcke, C. F. Groß, K. Jansen, S. K¨ uhn, S. Romiti, and C. Urbach, Hamiltonian limit of lattice QED in 2+1 dimensions, in Proceedings of The 39th In- ternational Symposium on Lattice Field Theory — PoS(LATTICE2022), Vol. 430 (SISSA Medialab, 2023) p. 292. [128] C. Strouthos and J. B. Kogut, The Phases of Non- Compact QED(3) (2008), arxiv:0804.0300 [cond-mat, physics:hep-lat, physics:hep-ph]. [129] J. Bender, Quantum and Classical Methods for Lattice Gauge Theories in Higher Dimensions , Ph.D. thesis, Technische Universit¨ at M¨ unchen (2023). [130] G. Clemente, A. Crippa, and K. Jansen, Strategies for the determination of the running coupling of ( $2+1$)- dimensional QED with quantum computing, Physical Review D 106, 114511 (2022). [131] M. P. Hern´ andez, Lattice field theory fundamentals (Oxford University Press, 2011) p. 20. [132] C. Kang, M. B. Soley, E. Crane, S. M. Girvin, and N. Wiebe, Leveraging hamiltonian simulation tech- niques to compile operations on bosonic devices (2023), arXiv:2303.15542 [quant-ph]. [133] R. L. Workman and Others (Particle Data Group), Re- view of Particle Physics, PTEP 2022, 083C01 (2022). [134] A. Kelman, U. Borla, I. Gomelski, J. Elyovich, G. Roose, P. Emonts, and E. Zohar, Gauged gaussian peps – a high dimensional tensor network formulation for lattice gauge theories (2024), arXiv:2404.13123 [hep- lat]. [135] T. Holstein, Studies of polaron motion: Part i. the molecular-crystal model, Annals of Physics 8, 325 (1959). [136] I. Bloch, Ultracold quantum gases in optical lattices, Nature Physics 1, 23 (2005). [137] F. Schlawin, D. M. Kennes, and M. A. Sentef, Cavity quantum materials, Applied Physics Reviews 9, 011312 (2022). [138] J. Stolpp, T. K¨ ohler, S. R. Manmana, E. Jeckelmann, F. Heidrich-Meisner, and S. Paeckel, Comparative study of state-of-the-art matrix-product-state methods for lat- tice models with large local hilbert spaces without u(1) symmetry, Computer Physics Communications 269, 108106 (2021). [139] E. Jeckelmann and S. R. White, Density-matrix renormalization-group study of the polaron problem in the holstein model, Phys. Rev. B 57, 6376 (1998). [140] C. Guo, A. Weichselbaum, J. von Delft, and M. Vojta, Critical and strong-coupling phases in one- and two- bath spin-boson models, Phys. Rev. Lett. 108, 160401 (2012). [141] C. Brockt, F. Dorfner, L. Vidmar, F. Heidrich-Meisner, and E. Jeckelmann, Matrix-product-state method with a dynamical local basis optimization for bosonic systems out of equilibrium, Phys. Rev. B 92, 241106 (2015). [142] J. Stolpp, J. Herbrych, F. Dorfner, E. Dagotto, and F. Heidrich-Meisner, Charge-density-wave melting in the one-dimensional holstein model, Phys. Rev. B 101, 035134 (2020). [143] C. Zhang, E. Jeckelmann, and S. R. White, Density matrix approach to local hilbert space reduction, Phys. Rev. Lett. 80, 2661 (1998). [144] F. A. Y. N. Schr¨ oder and A. W. Chin, Simulating open quantum dynamics with time-dependent varia- tional matrix product states: Towards microscopic cor- relation of environment dynamics and reduced system evolution, Phys. Rev. B 93, 075105 (2016). [145] M. Mathur and A. Rathor, Exact duality and lo- cal dynamics in su(n) lattice gauge theory (2023), arXiv:2109.00992 [hep-lat]. [146] C. W. Bauer, I. D’Andrea, M. Freytsis, and D. M. Grabowska, A new basis for hamiltonian su(2) simu- lations (2023), arXiv:2307.11829 [hep-ph]. [147] J. Kessler, F. Calcavecchia, and T. D. K¨ uhne, Artifi- cial Neural Networks as Trial Wave Functions for Quan- tum Monte Carlo, Advanced Theory and Simulations4, 2000269 (2021). [148] F. A. Y. N. Schr¨ oder, D. H. P. Turban, A. J. Musser, N. D. M. Hine, and A. W. Chin, Tensor network simu- lation of multi-environmental open quantum dynamics via machine learning and entanglement renormalisation, Nature Communications 10, 1062 (2019). [149] W. E. Arnoldi, The principle of minimized iterations in the solution of the matrix eigenvalue problem, Quarterly of Applied Mathematics 9, 17 (1951). [150] R. B. Lehoucq, D. C. Sorensen, and C. Yang, ARPACK: Solution of Large Scale Eigenvalue Problems by Im- plicitly Restarted Arnoldi Methods , Available from netlib@ornl.gov (1997). [151] L. Dagum and R. Menon, Openmp: an industry stan-19 dard api for shared-memory programming, Computa- tional Science & Engineering, IEEE 5, 46 (1998). [152] Y. Shi, U. N. Niranjan, A. Anandkumar, and C. Cecka, Tensor Contractions with Extended BLAS Kernels on CPU and GPU, in 2016 IEEE 23rd International Conference on High Performance Computing (HiPC) (IEEE, 2016) pp. 193–202. [153] A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, and S. Tomov, High-performance Tensor Contractions for GPUs, Procedia Computer Science In- ternational Conference on Computational Science 2016, ICCS 2016, 6-8 June 2016, San Diego, California, USA, 80, 108 (2016). [154] T. Vincent, L. J. O’Riordan, M. Andrenkov, J. Brown, N. Killoran, H. Qi, and I. Dhand, Jet: Fast quantum cir- cuit simulations with parallel task-based tensor-network contraction, Quantum 6, 709 (2022). [155] F. Pan and P. Zhang, Simulation of quantum circuits using the big-batch tensor network method, Phys. Rev. Lett. 128, 030501 (2022). [156] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden, A. Borchers, R. Boyle, P.-l. Cantin, C. Chao, C. Clark, J. Coriell, M. Daley, M. Dau, J. Dean, B. Gelb, T. V. Ghaemmaghami, R. Gottipati, W. Gulland, R. Hag- mann, C. R. Ho, D. Hogberg, J. Hu, R. Hundt, D. Hurt, J. Ibarz, A. Jaffey, A. Jaworski, A. Kaplan, H. Khai- tan, A. Koch, N. Kumar, S. Lacy, J. Laudon, J. Law, D. Le, C. Leary, Z. Liu, K. Lucke, A. Lundin, G. MacK- ean, A. Maggiore, M. Mahony, K. Miller, R. Na- garajan, R. Narayanaswami, R. Ni, K. Nix, T. Nor- rie, M. Omernick, N. Penukonda, A. Phelps, J. Ross, M. Ross, A. Salek, E. Samadiani, C. Severn, G. Sizikov, M. Snelham, J. Souter, D. Steinberg, A. Swing, M. Tan, G. Thorson, B. Tian, H. Toma, E. Tuttle, V. Vasude- van, R. Walter, W. Wang, E. Wilcox, and D. H. Yoon, In-Datacenter Performance Analysis of a Tensor Pro- cessing Unit (2017), arXiv:1704.04760 [cs.AR]. [157] M. Hauru, A. Morningstar, J. Beall, M. Ganahl, A. Lewis, and G. Vidal, Simulation of quantum physics with tensor processing units: brute-force computa- tion of ground states and time evolution (2021), arXiv:2111.10466 [quant-ph]. [158] A. Morningstar, M. Hauru, J. Beall, M. Ganahl, A. G. M. Lewis, V. Khemani, and G. Vidal, Simulation of Quantum Many-Body Dynamics with Tensor Pro- cessing Units: Floquet Prethermalization, PRX Quan- tum 3, 020331 (2022). [159] M. Ganahl, J. Beall, M. Hauru, A. G. M. Lewis, T. Wo- jno, J. H. Yoo, Y. Zou, and G. Vidal, Density Matrix Renormalization Group with Tensor Processing Units, PRX Quantum 4, 010317 (2023). [160] L. S. Blackford, J. Choi, A. Cleary, E. D’Azevedo, J. Demmel, I. Dhillon, J. Dongarra, S. Hammarling, G. Henry, A. Petitet, K. Stanley, D. Walker, and R. C. Whaley, ScaLAPACK Users’ Guide, Software, Environ- ments, and Tools (Society for Industrial and Applied Mathematics, Philadelphia, PA, 1997). [161] Y. Lu, F. Ino, and Y. Matsushita, High-performance out-of-core block randomized singular value decomposi- tion on gpu (2017), arXiv:1706.07191 [cs]. [162] E. Gabriel, G. E. Fagg, G. Bosilca, T. Angskun, J. J. Dongarra, J. M. Squyres, V. Sahay, P. Kambadur, B. Barrett, A. Lumsdaine, R. H. Castain, D. J. Daniel, R. L. Graham, and T. S. Woodall, Open MPI: Goals, concept, and design of a next generation MPI implemen- tation, in Proceedings, 11th European PVM/MPI Users’ Group Meeting (Budapest, Hungary, 2004) pp. 97–104. [163] E. M. Stoudenmire and S. R. White, Real-space parallel density matrix renormalization group, Phys. Rev. B 87, 155137 (2013). [164] P. Secular, N. Gourianov, M. Lubasch, S. Dolgov, S. R. Clark, and D. Jaksch, Parallel time-dependent varia- tional principle algorithm for matrix product states, Phys. Rev. B 101, 235123 (2020). [165] G. Cataldi, A. Abedi, G. Magnifico, S. Notarnicola, N. D. Pozza, V. Giovannetti, and S. Montangero, Hilbert curve vs Hilbert space: exploiting fractal 2D covering to increase tensor network efficiency, Quantum 5, 556 (2021). [166] Quantum tea leaves: Customized version b5bc166c which is not public yet; optimized towards large bond dimensions moving data between CPU and GPU on more levels than available versions., https://baltig.infn.it/quantum_tea_leaves/ py_api_quantum_tea_leaves. [167] Textarossa Dibona: using node of the EuroHPC project with Nvidia A100 GPU with 40GB memory. [168] F. Verstraete, J. J. Garc´ ıa-Ripoll, and J. I. Cirac, Ma- trix product density operators: Simulation of finite- temperature and dissipative systems, Phys. Rev. Lett. 93, 207204 (2004). [169] M. Zwolak and G. Vidal, Mixed-state dynamics in one-dimensional quantum lattice systems: A time- dependent superoperator renormalization algorithm, Phys. Rev. Lett. 93, 207205 (2004). [170] A. H. Werner, D. Jaschke, P. Silvi, M. Kliesch, T. Calarco, J. Eisert, and S. Montangero, Positive tensor network approach for simulating open quan- tum many-body systems, Phys. Rev. Lett. 116, 237201 (2016). [171] M. Kliesch, D. Gross, and J. Eisert, Matrix-product operators and states: Np-hardness and undecidability, Phys. Rev. Lett. 113, 160503 (2014). [172] L. Arceci, P. Silvi, and S. Montangero, Entanglement of Formation of Mixed Many-Body Quantum States via Tree Tensor Operators, Physical Review Letters 128, 040501 (2022).",
      "references": [
        "Gauge Fields in Condensed Matter",
        "Field Theories of Condensed Matter Physics, 2nd ed.",
        "Lattice gauge theory for condensed matter physics: ferromagnetic superconductivity as its example",
        "An introduction to quantum field theory",
        "Quantum Field Theory and the Standard Model",
        "Confinement of quarks",
        "An introduction to lattice gauge theory and spin systems",
        "Monte Carlo computations in lattice gauge theories",
        "Report of the Snowmass 2021 Topical Group on Lattice Gauge Theory",
        "Finite-density lattice qcd and sign problem: Current status and open problems",
        "Tensor networks and their use for lattice gauge theories",
        "Introduction to Tensor Network Methods: Numerical Simulations of Low-Dimensional Many-Body Quantum Systems",
        "Tensor networks for complex quantum systems",
        "Tensor network algorithms: A route map",
        "Density matrix renormalization group approach to the massive schwinger model",
        "Lattice gauge tensor networks",
        "Real-Time Dynamics in U(1) Lattice Gauge Theories with Tensor Networks",
        "The Tensor Networks Anthology: Simulation techniques for many-body quantum lattice systems",
        "Topological vacuum structure of the schwinger model with matrix product states",
        "Matrix Product States for Gauge Field Theories",
        "Tensor Networks for Lattice Gauge Theories and Atomic Quantum Simulation",
        "Gauging quantum states: From global to local symmetries in many-body systems",
        "Finite-density phase diagram of a (1 + 1) −d non-abelian lattice gauge theory with tensor networks",
        "Finite-representation approximation of lattice gauge theories at the continuum limit with tensor networks",
        "Density induced phase transitions in the schwinger model: A study with matrix product states",
        "Phase transitions in Zn gauge models: Towards quantum simulations of the schwinger-weyl qed",
        "Symmetry-protected topological phases in lattice gauge theories: Topological qed2",
        "zN gauge theories coupled to topological fermions: qed 2 with a quantum mechanical θ angle",
        "Gaussian states for the variational study of (1+1)-dimensional lattice gauge models",
        "Real Time Dynamics and Confinement in the Zn Schwinger-Weyl lattice model for 1+1 QED",
        "Efficient basis formulation for (1 + 1)-dimensional su(2) lattice gauge theory: Spectral calculations with matrix product states",
        "Entanglement generation in (1 + 1)D qed scattering processes",
        "Exploring the CP-violating Dashen phase in the Schwinger model with tensor networks",
        "Computing the Mass Shift of Wilson and Staggered Fermions in the Lattice Schwinger Model with Matrix Product States",
        "Spectral Properties of Critical 1+1D Abelian-Higgs Model",
        "Hamiltonian truncation tensor networks for quantum field theories",
        "Dense QCD 2 with matrix product states",
        "Mass gaps of a Z3 gauge theory with three fermion flavors in 1 + 1 dimensions",
        "Probing confinement through dynamical quantum phase transitions: From quantum spin models to lattice gauge theories",
        "Confinement in 1+1d Z2 lattice gauge theories at finite temperature",
        "High-energy collision of quarks and mesons in the schwinger model: From tensor networks to circuit qed",
        "Real-time scattering in the lattice schwinger model",
        "Quantum Many-Body Scarring in a Non-Abelian Lattice Gauge Theory",
        "Two-Dimensional Quantum-Link Lattice Quantum Electrodynamics at Finite Density",
        "Finding the ground state of a lattice gauge theory with fermionic tensor networks: a 2 + 1 d Z2 demonstrationfinding the ground state of a lattice gauge theory with fermionic tensor networks: a 2+1d Z2 demonstration",
        "Lattice quantum electrodynamics in (3+1)-dimensions at finite density with tensor networks",
        "Entanglement and confinement in lattice gauge theory tensor networks",
        "Discrete abelian lattice gauge theories on a ladder and their dualities with quantum clock models",
        "A cold-atom particle collider",
        "(2+1)D SU(2) Yang-Mills Lattice Gauge Theory at finite density via tensor networks",
        "A flexible high-performance simulator for verifying and benchmarking quantum circuits implemented on real hardware",
        "Toward scalable simulations of lattice gauge theories on quantum computers",
        "What limits the simulation of quantum computers?",
        "Efficient parallelization of tensor network contraction for simulating quantum computation",
        "Variational power of quantum circuit tensor networks",
        "Quantum simulation of lattice gauge theories in more than one space dimension—requirements, challenges and methods",
        "Report of the Snowmass 2021 Theory Frontier Topical Group on Quantum Information Science",
        "Dynamical Quantum Phase Transitions of the Schwinger Model: Real-Time Dynamics on IBM Quantum",
        "Review on Quantum Computing for Lattice Field Theory",
        "Hamiltonians and gauge-invariant Hilbert space for lattice Yang-Mills-like theories with finite gauge group",
        "Fermion-qudit quantum processors for simulating lattice gauge theories with matter",
        "Colloquium: Area laws for the entanglement entropy",
        "Entanglement entropy and quantum field theory",
        "An area law for one-dimensional quantum systems",
        "Area law of noncritical ground states in 1d long-range interacting systems",
        "Realistic area-law bound on entanglement from exponentially decaying correlations",
        "Area law for the entropy of low-energy states",
        "Approximating the ground state of gapped quantum spin systems",
        "Locality at the Boundary Implies Gap in the Bulk for 2D PEPS",
        "Mathematical open problems in projected entangled pair states",
        "Matrix product states and projected entangled pair states: Concepts, symmetries, theorems",
        "Matrix product state representations",
        "The density-matrix renormalization group in the age of matrix product states",
        "Matrix Product Operators, Matrix Product States, and Ab Initio Density Matrix Renormalization Group Algorithms",
        "Computational complexity of projected entangled pair states",
        "Entanglement and tensor network states",
        "Variational methods for contracting projected entangled-pair states",
        "Classical simulation of quantum many-body systems with a tree tensor network",
        "From tree tensor network to multi-scale entanglement renormalization ansatz",
        "Area law and real-space renormalization",
        "Efficient tensor network ansatz for high-dimensional quantum many-body problems",
        "Open source matrix product states: Opening ways to simulate entangled many-body quantum systems in one dimension",
        "Efficient simulation of one-dimensional quantum many-body systems",
        "Time-dependent variational principle for quantum lattices",
        "Time dependent variational principle for tree Tensor Networks",
        "Superfluid-to-mott transition in a bose-hubbard ring: Persistent currents and defect formation",
        "Entropy scaling and simulability by matrix product states",
        "Time-evolution methods for matrix-product states",
        "Quantum dynamics of thermalizing systems",
        "Simulating the out-of-equilibrium dynamics of local observables by trading entanglement for mixture",
        "Lattice fermions",
        "Formulation of lattice gauge theories for quantum simulations",
        "Removing staggered fermionic matter in U(N) and SU (N) lattice gauge theories",
        "Eliminating fermionic matter fields in lattice gauge theories",
        "Scalable digital quantum simulation of lattice fermion theories with local encoding",
        "Hadrons in (1+1)D Hamiltonian hardcore lattice QCD",
        "An Introduction to Non-Perturbative Foundations of Quantum Field Theory",
        "Gauge redundancy-free formulation of compact qed with dynamical matter for quantum and classical computations",
        "Quantum link models: A discrete approach to gauge theories",
        "Lattice gauge magnets: Local isospin from spin",
        "QCD as a quantum link model",
        "Simulating lattice gauge theories on a quantum computer",
        "Towards analog quantum simulations of lattice gauge theories with trapped ions",
        "Gauge-invariant quantum circuits for$U$(1) and Yang-Mills lattice gauge theories",
        "Investigating a $(3+1)\text{D}$ topological $\theta$-term in the Hamiltonian formulation of lattice gauge theories for quantum and classical simulations",
        "Quantum simulation of fundamental particles and forces",
        "A resource efficient approach for quantum and classical simulations of gauge theories in particle physics",
        "Digitizing gauge fields: Lattice monte carlo results for future quantum computers",
        "Quantum and classical spin network algorithms for $q$-deformed Kogut-Susskind gauge theories",
        "Critical and strong-coupling phases in one- and two-bath spin-boson models",
        "Matrix-product-state method with a dynamical local basis optimization for bosonic systems out of equilibrium",
        "Charge-density-wave melting in the one-dimensional holstein model",
        "Density matrix approach to local hilbert space reduction",
        "Simulating open quantum dynamics with time-dependent variational matrix product states: Towards microscopic correlation of environment dynamics and reduced system evolution",
        "Exact duality and local dynamics in su(n) lattice gauge theory",
        "A new basis for hamiltonian su(2) simulations",
        "Artificial Neural Networks as Trial Wave Functions for Quantum Monte Carlo",
        "Tensor network simulation of multi-environmental open quantum dynamics via machine learning and entanglement renormalisation",
        "The principle of minimized iterations in the solution of the matrix eigenvalue problem",
        "ARPACK: Solution of Large Scale Eigenvalue Problems by Implicitly Restarted Arnoldi Methods",
        "Openmp: an industry standard api for shared-memory programming",
        "Tensor Contractions with Extended BLAS Kernels on CPU and GPU",
        "High-performance Tensor Contractions for GPUs",
        "Jet: Fast quantum circuit simulations with parallel task-based tensor-network contraction",
        "Simulation of quantum circuits using the big-batch tensor network method",
        "In-Datacenter Performance Analysis of a Tensor Processing Unit",
        "Simulation of quantum physics with tensor processing units: brute-force computation of ground states and time evolution",
        "Simulation of Quantum Many-Body Dynamics with Tensor Processing Units: Floquet Prethermalization",
        "Density Matrix Renormalization Group with Tensor Processing Units",
        "ScaLAPACK Users’ Guide, Software, Environments, and Tools",
        "High-performance out-of-core block randomized singular value decomposition on gpu",
        "Open MPI: Goals, concept, and design of a next generation MPI implementation",
        "Real-space parallel density matrix renormalization group",
        "Parallel time-dependent variational principle algorithm for matrix product states",
        "Hilbert curve vs Hilbert space: exploiting fractal 2D covering to increase tensor network efficiency",
        "Matrix product density operators: Simulation of finite-temperature and dissipative systems",
        "Mixed-state dynamics in one-dimensional quantum lattice systems: A time-dependent superoperator renormalization algorithm",
        "Positive tensor network approach for simulating open quantum many-body systems",
        "Matrix-product operators and states: Np-hardness and undecidability",
        "Entanglement of Formation of Mixed Many-Body Quantum States via Tree Tensor Operators",
        "QED 3 on a space-time lattice: A comparison between compact and noncompact formulation",
        "Nonperturbative beta function in three-dimensional electrodynamics",
        "Beta function of three-dimensional QED",
        "Monte Carlo Study of Lattice Compact Quantum Electrodynamics with Fermionic Matter: The Parent State of Quantum Phases",
        "Lattice Gauge Theory and Monte Carlo Methods",
        "Lattice gauge theories and Monte Carlo algorithms",
        "Path integral Monte Carlo approach to the U(1) lattice gauge theory in 2+1 dimensions",
        "Lattice Gauge Theories: An Introduction (Fourth Edition)",
        "Hamiltonian limit of lattice QED in 2+1 dimensions",
        "The Phases of Non-Compact QED(3)",
        "Quantum and Classical Methods for Lattice Gauge Theories in Higher Dimensions",
        "Strategies for the determination of the running coupling of ( $2+1$)-dimensional QED with quantum computing",
        "Lattice field theory fundamentals",
        "Leveraging hamiltonian simulation techniques to compile operations on bosonic devices",
        "Review of Particle Physics",
        "Gauged gaussian peps – a high dimensional tensor network formulation for lattice gauge theories",
        "Studies of polaron motion: Part i. the molecular-crystal model",
        "Ultracold quantum gases in optical lattices",
        "Cavity quantum materials",
        "Comparative study of state-of-the-art matrix-product-state methods for lattice models with large local hilbert spaces without u(1) symmetry",
        "Density-matrix renormalization-group study of the polaron problem in the holstein model"
      ],
      "meta_data": {
        "arxiv_id": "2407.03058v1",
        "doi": "10.1038/s42005-025-02125-x",
        "authors": [
          "Giuseppe Magnifico",
          "Giovanni Cataldi",
          "Marco Rigobello",
          "Peter Majcen",
          "Daniel Jaschke",
          "Pietro Silvi",
          "Simone Montangero"
        ],
        "published_date": "2024-07-03T12:30:04Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper reviews the state-of-the-art of Tensor Network (TN) methods for Lattice Gauge Theories (LGTs) and proposes a roadmap for their algorithmic development and optimization to tackle challenging high-energy physics problems beyond one dimension, such as accessing continuum limits and simulating large-scale Quantum Chromodynamics (QCD). A key contribution is highlighting that TN methods are intrinsically free from the sign problem, which limits traditional Monte Carlo (MC) methods in regimes like finite chemical potentials or real-time dynamics. The work also provides tailored estimates of theoretical and computational resource scaling for large-scale LGTs.",
        "methodology": "The research primarily reviews and proposes advancements for Tensor Network (TN) methods, including Matrix Product States (MPS), Projected Entangled Pair States (PEPS), and Tree Tensor Networks (TTN), with a particular focus on TTNs for higher dimensions. It details the Hamiltonian formalism for LGTs, including matter and gauge fields on a lattice, and the Kogut-Susskind Hamiltonian. Key techniques for TN application to LGTs involve gauge field truncation (e.g., Quantum Link Models, electric energy density operator truncation), and enforcing Gauss law through a 'dressed site' construction (involving rishons, Abelian link symmetries, gauge-invariant computational basis, and defermionization). Algorithms discussed for ground state searching include Density Matrix Renormalization Group (DMRG) for MPS and variational optimization for TTNs. For real-time dynamics, Time Evolved Block Decimation (TEBD) and Time-Dependent Variational Principle (TDVP) are mentioned.",
        "experimental_setup": "The paper uses a (2+1)-dimensional U(1) LGT (QED) with dynamical matter as a paradigmatic model to analyze the scaling of the local basis dimension for various gauge groups (U(1), SU(2), SU(3)) in 2 and 3 space dimensions. It benchmarks the convergence in gauge truncation by performing exact diagonalization on a single plaquette in open boundary conditions, computing the ground state expectation value of the magnetic energy operator (Re U_box) until the relative deviation between consecutive truncations drops below a threshold of 10^-5. Computational resource estimates are derived from a 2D quantum Ising model with Z2 symmetry, using a single-tensor optimization on an A100 GPU on Cineca's Leonardo pre-exascale cluster, with extrapolations for larger system sizes and bond dimensions.",
        "limitations": "Current TN simulations for LGTs face several limitations: the local basis dimension (d) can become extremely large (O(10^6)) for higher-dimensional non-Abelian models and large gauge group representations, making simulations practically infeasible. PEPS suffer from exponentially hard exact contraction and high computational complexity (O(Nd^2χ^8)) even with approximate methods. Loopless TTN structures may not explicitly reproduce the area law in dimensions higher than one without augmentation (aTTN), which then introduces severe scaling with local basis dimension (O(Nχ^4d^4 + Nχd^7)). Real-time dynamics generate linear entanglement growth, requiring exponential growth of bond dimension, thus limiting simulations to low-to-moderate times. Furthermore, continuum limits of LGTs correspond to critical points, where enhanced quantum correlations and entanglement area law violations are expected, posing a challenge for TN methods. The dependence of optimal gauge truncation on Hamiltonian parameters (m and g) also presents a constraint.",
        "future_research_directions": "The paper proposes a roadmap for future research directions. These include: 1. **Local Basis Truncation:** Developing and testing efficient algorithms like pseudo-site DMRG (PS-DMRG) and DMRG with local basis optimization (DMRG-LBO) for LGTs, employing exact diagonalization for 1D and (2+1)D systems, and integrating LBO protocols into high-dimensional TN ansätze like aTTN. Also, incorporating resource-efficient protocols (e.g., canonical transformations) for optimal basis construction. 2. **Tailored Initial States:** Constructing initial guesses based on physical intuition, utilizing machine learning-assisted protocols, or leveraging neighboring ground states and quenches to speed up convergence. 3. **Leveraging HPC Techniques for Local Optimization:** Enhancing performance through efficient OpenMP implementations, utilizing accelerators like GPUs and TPUs, exploring multi-node approaches (ScaLAPACK, MAGMA), and optimizing parameters and algorithms (e.g., precision tuning, random SVD). 4. **Sweeps and HPC Parallelization:** Developing parallel versions of MPS and TTN algorithms using MPI to manage delays and optimize sweeps, particularly for higher-dimensional systems. 5. **Finite Temperature Regime:** Devising and testing efficient TN algorithms for finite-temperature states, such as improving Matrix Product Density Operators (MPDOs), Locally Purified Tensor Networks (LPTNs), or variational Tree Tensor Operators (TTOs) to explore phenomena like the QCD phase diagram.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Indirectly Supervised Natural Language Processing",
      "full_text": "Adversarial Light Projection Attacks on Face Recognition Systems: A Feasibility Study Dinh-Luan Nguyen1,2, Sunpreet S. Arora1, Yuhang Wu1, and Hao Yang1 1Visa Research, Palo Alto CA USA 94306 2Michigan State University, East Lansing MI USA 48824 Corresponding author email: sunarora@visa.com Abstract Deep learning-based systems have been shown to be vul- nerable to adversarial attacks in both digital and physi- cal domains. While feasible, digital attacks have limited applicability in attacking deployed systems, including face recognition systems, where an adversary typically has ac- cess to the input and not the transmission channel. In such setting, physical attacks that directly provide a malicious input through the input channel pose a bigger threat. We investigate the feasibility of conducting real-time physical attacks on face recognition systems using adversarial light projections. A setup comprising a commercially available web camera and a projector is used to conduct the attack. The adversary uses a transformation-invariant adversarial pattern generation method to generate a digital adversarial pattern using one or more images of the target available to the adversary. The digital adversarial pattern is then pro- jected onto the adversary’s face in the physical domain to either impersonate a target (impersonation) or evade recog- nition (obfuscation). We conduct preliminary experiments using two open-source and one commercial face recognition system on a pool of 50 subjects. Our experimental results demonstrate the vulnerability of face recognition systems to light projection attacks in both white-box and black-box at- tack settings. 1. Introduction Deep learning-based systems are typically designed un- der the assumption that the inputs/examples presented to the system during the test/operational phase follow the same underlying distribution as the examples used to train the system. However, recent research has shown security vul- nerabilities of such systems when input test examples are intentionally crafted to cause the system to produce incor- rect results (called adversarial examples) [22, 8]. Most adversarial examples on convolutional neural network ar- (a)  (b) Figure 1: Example of impersonation attack on FaceNet [18] in white-box setting. Shown in (a) is the captured image of the adversary’s face with adversarial light projected in the physical domain that is recognized as the target (b). chitectures (typically used in image classiﬁcation scenarios including face recognition) are generated by perturbing the pixel intensities directly in the digital domain [13, 16, 3]. These digital attacks, however, do not directly translate into the physical domain where an adversary has access to the open-camera channel. In such setting, the adversary usu- ally does not have access to the image captured using the camera that is input to the convolutional neural network. Speciﬁcally, consider a face recognition system, that is de- ployed, such that it captures a face image of a subject and compares it to the enrolled faces to validate or establish the identity of the subject. While security mechanisms can be enforced to safeguard the digital storage and transmission of facial data captured using the camera, an adversary can potentially trick the system by providing a malicious input to the camera directly [10, 7]. A subclass of physical attacks on face recognition sys- tems called presentation or spooﬁng attacks achieve this by creating physical spoofs using one or more face images of the target ( e.g., 2D-printed face photos, 3D masks) [14]. The same objective can also be achieved by crafting phys- ical adversarial artifacts such as glasses that an adversary can wear to either evade recognition or mimic a target [19]. However, fabrication of physical adversarial artifacts gener- 1 arXiv:2003.11145v2  [cs.CV]  17 Apr 2020Adversarial Pattern Generator Adversary  digital face Face recognition  neural network Camera Adversary  physical face Light projector Digitally generated projectable  adversarial light pattern Target digital face Figure 2: Setup used for conducting real-time adversarial light projection attacks on face recognition systems. First, the adversary captures his/her facial image using a camera, and uses one or more images of the target to (i) calibrate the camera-projector setup based on the attack environment, and (ii) generate a digital adversarial pattern. Next, the ad- versary projects the digital pattern onto the adversary’s face in the physical domain using a projector to either imperson- ate a target or evade recognition. ally requires a manufacturing method (e.g., 2D or 3D print- ing). In addition, the utility of physical artifacts is limited in conducting at-scale physical attacks targeting multiple users of a face recognition system by the type of physical speci- mens that can be fabricated in typical resource-constrained settings. We investigate the feasibility of conducting a real-time physical attack on face recognition systems using adver- sarial light projections that can be used for imperson- ating different enrolled users (called impersonation), or evading recognition (called obfuscation). The adversary ﬁrst calibrates the camera-projector setup and then uses a transformation-invariant adversarial pattern generation method to generate adversarial patterns in the digital do- main. These digital patterns are subsequently projected onto the adversary’s face to conduct impersonation or ob- fuscation attack. We refer to this attack as adversarial light projection attack. As an example, impersonation is the goal when an adversary intends to obtain access to a resource, e.g., personal device protected with a target’s face. Obfus- cation, on the other hand, is the goal of an adversary black- listed by law enforcement agencies who wants to evade recognition in scenarios such as border crossing. A similar idea was recently proposed for fooling deep learning classiﬁers designed for image classiﬁcation sys- tems [17]. However, the authors did not evaluate the utility of their method in the context of face recognition systems. Another recent work [28] fabricated a wearable cap with in- frared LEDs to attack face recognition systems. Although this work is similar in terms of its objective, our method does not require a wearable artifact and thus offers an easier alternative using off-the-shelf camera-projector setup (e.g., a portable mini projector [1]) for conducting physical at- tacks on facial recognition systems. Preliminary experi- ments conducted on 50 subjects show the vulnerability of state-of-the-art face recognition systems to adversarial light projection attacks in both white-box and black-box attack settings. 1.1. Contributions The major contributions of this work include: •Investigation of real-time adversarial light projection attacks using off-the-shelf camera-projector setup on state-of-the-art face recognition systems. •An efﬁcient transformation-invariant adversarial pat- tern generation method suitable for conducting real- time adversarial light projection attacks. •Demonstration of vulnerability of state-of-the-art face recognition systems to adversarial light projection at- tacks in both white-box and black-box settings. 2. Related Work Existing research on adversarial attacks can be broadly classiﬁed into two major categories: digital and physical at- tacks. Given one or more examples from source and target class, digital attack methods generate adversarial pattern(s) in the digital domain such that the pattern(s) results in a source class example being misclassiﬁed as a target class example (called targeted attack), or the source class exam- ple being incorrectly classiﬁed as an example from a dif- ferent class ( called untargeted attack), typically with high conﬁdence of being in the target class. Physical attacks extend this notion into the physical domain by using spe- cially crafted adversarial artifacts for targeted or untargeted attacks. Below we summarize the major research in the two categories and contrast existing physical attack meth- ods from the method presented here. 2.1. Digital Attacks One of the ﬁrst digital attack methods proposed by Szegedy et al. in 2013 called L-BFGS [22] formulates the goal of adversarial pattern generation as an optimization problem, and uses a box-constrained optimizer and linear search to ﬁnd the optimal solution. In 2014, Goodfellow et al. [8] proposed Fast Gradient Sign Method (FGSM), a single-step adversarial pattern generation method that uses gradients computed from neural network parameters for ad- versarial pattern generation. Following this, multiple exten- sions of FGSM were introduced [10, 5, 24, 25]. Shi et al. [20] combine gradient ascent and descent with binary search to ﬁnd the adversarial pattern with the leastℓ2norm. One of the most popular adversarial pattern genera- tion methods Projected Gradient Descent (PGD) [13] uses gradient projection space as a bound to generate ℓinf ad- versarial patterns. Other prominent methods include Deep- Fool [16] which was proposed for conducting untargeted at- tacks with ℓp norm, and models adversarial pattern genera- tion as a linear approximation problem, and SparseFool [15] that aims to generate adversarial patterns by modifying a minimal number of pixels. Another popular method pro- posed by Carlini and Wagner [3] uses gradient descent with a custom loss function to minimize the ℓp norm during ad- versarial pattern generation. 2.2. Physical Attacks Kurakin et al. [10] printed 2D adversarial patches con- taining objects overlaid by adversarial patterns to attack deep networks trained for the object recognition task. Sev- eral other researchers directly printed 2D adversarial pat- terns which are then manually attached to physical ob- jects to attack object detection and classiﬁcation algorithms [4, 21, 7, 27]. Similarly, Thys et al. [23] printed 2D ad- versarial patches to circumvent pedestrian detection classi- ﬁers. Athalye et al. [2] proposed a transformation-invariant adversarial pattern generation scheme called Expectation of Transformations (EOT) to fabricate 3D adversarial ob- jects designed to fool object classiﬁers. More recently, Li et al. [11] printed adversarial dots on a 2D transparent pa- per to provide adversarial input via the camera to an object recognition system. Although the aforementioned methods succeed in achieving their stated objective, they usually re- quire extensive calibration of each 2D or 3D-printed artifact before fabrication. In addition, they also require fabrica- tion of physical artifacts. On the other hand, the camera- projector setup used in the method presented here can be calibrated once based on the attack environment, and then subsequently used for conducting multiple real-time attacks targeting different enrolled users of a face recognition sys- tem. Similar to this work, Nichols and Jasper [17] used a camera-projector setup to generate 2D adversarial dot pat- terns that are then projected onto the physical scene to at- tack object recognition systems. However, they did not use the setup for conducting impersonation or obfuscation at- tacks on face recognition systems. Zhou et al. [28], on the other hand, fabricated a wearable cap with infrared LEDs to fool face recognition systems. Although this work is identi- cal to the method presented here in terms of its objective, our method does not require creation of a wearable arti- fact and thus offers an easier alternative using off-the-shelf camera-projector setup for conducting physical attacks on facial recognition systems. 3. Adversarial Light Projection Attack The proposed adversarial light projection attack is per- formed in two steps: the ﬁrst step is to calibrate the camera- projector setup based on the attack environment and com- pute the adversarial pattern in the digital domain that can be used to either evade recognition or impersonate a target, and the second step is to project the computed digital adver- sarial pattern onto the adversary’s face using the projector to attack the deployed face recognition system (see Figure 2). 3.1. Assumptions In the ﬁrst step, the adversary is assumed to have either white-box access1 or black-box access2 to the deployed face recognition algorithm that the adversary intends to attack. White-box is a reasonable assumption for face recognition algorithms such as FaceNet [18] and SphereFace [12] that are available in open-source. On the other hand, commer- cial face recognition systems often only provide black-box access. So we assume that the adversary uses an open- source algorithm to generate adversarial patterns to attack the black-box system. This assumption exploits the prop- erty that adversarial patterns are highly transferable across deep network architectures. Additionally, we assume that the adversary has access to an image of the target (in case of impersonation attack). Further, the adversary has access to a camera to capture the adversary’s own face image in order to compute the adver- sarial light pattern, and a projector that is able to project light patterns on his/her own face in the physical domain in order to conduct the attack. In addition, the adversary is assumed to have either access to or reasonable prior knowl- edge of the environment where the face recognition system to be attacked is deployed. This is to ensure that the ad- versarial light pattern can be calibrated based on the attack environment before projection. 3.2. Practical Considerations Adversarial light projection attacks are inherently chal- lenging because of their unconstrained nature. Below we discuss key practical considerations critical to the success of such attacks: •Environmental factors, for example ambient and posi- tional lighting, and their interplay with the projected light. Calibration of the attack setup based on the at- tack environment is therefore integral to the success of the attack (see Section 4). 1In white-box setting, the adversary knows the internal details of the model including the architecture and trained weight parameters. 2In black-box setting, the adversary only knows the decision/output of the model for one or more inputs.Projector Camera Project  Capture Projection wall Figure 3: Different viewpoints in the camera-projector setup. The physical adversary is assumed to be in the view of both the camera and the projector. •Intra-adversary facial variations especially due to slight physical movements of the adversary, e.g., head movements and changes in the distance to the camera, while conducting the attack. Generation of adversar- ial patterns that are relatively invariant to such varia- tions is therefore critical to the success of the attack (see Section 5). •Intra-target facial variations because the adversary would typically not have access to the enrolled im- ages of the target in the deployed face recognition sys- tem. Instead, the adversary would have target images captured in a different context, such as social media. Hence it is important that the generated adversarial pattern be robust to the target’s facial variations (see Section 5). 4. Attack Setup Calibration There are two key calibration steps integral to success of the attack: (i) position calibration : to ensure that the adversarial pattern generated in the digital domain can be projected onto the appropriate region of the adversary’s face while conducting the attack, and (ii) color calibration: to ensure that the digital adversarial pattern is reproduced with high ﬁdelity by the projector as adversarial light. 4.1. Position calibration Assume that the adversary is in view of both the camera and the projector (Figure 3). There are two possible ways to perform position calibration: (i) manual: the adversary manually annotates a small number (3-4) of corresponding points between the two views; or (ii) automatic: a facial landmark detection algorithm is used to detect correspond- ing facial landmarks from the two views. Once the land- mark correspondences are determined, a calibration matrix is computed to perform position calibration (see Figure 4). 4.2. Color calibration Let Cand Gbe the color reproduction functions of the camera and projector, respectively, in the physical attack Camera view point Aligned projector content Step 1 Step 2 Step 3 Figure 4: Two possible methods for position calibration: (i) manual (top row) and (ii) automatic (bottom row). Step 1: Adversary’s face is captured using the camera, and facial landmarks are either manually annotated or automatically detected; Step 2: Projected scene (including the adversarys face) is captured using the camera, and facial landmarks are manually annotated or automatically detected; Step 3: Cor- responding landmarks from step 1 and step 2 are used to compute the calibration matrix to calibrate the adversarial pattern before projection on the adversary’s face. setting. The objective of color calibration is to ﬁnd the color transformation function Υ given Cand G. Let x = C(x0), where x0 is the physical adversary, x is the image of the adversary in the digital domain, and h is the method used to generate adversarial pattern in the digital domain such as the one presented in section 5.3. Also, assume addi- tive characteristic of the color reproduction functions, i.e. C(color1 + color2) = C(color1) + C(color2). The rela- tionship between the digital and physical domains can then be expressed as follows: C(x0 + G(Υ(h(x)))) = x+ h(x) ⇔C(x0) + C(G(Υ(h(x))))) = x+ h(x) ⇔C(G(Υ(h(x))))) = h(x) ⇔Υ = (C◦G )−1 (1) To empirically estimate the aforementioned relationship in the attack setting, (i) color calibration patterns are gen- erated in the digital domain, (ii) the generated calibration patterns are projected on a white background in the phys- ical domain, and (iii) the projected calibration patterns in the physical domain are captured in the digital domain us- ing the camera. The color transformation function Υ is then computed by performing regression on the generated and camera-captured color pairs. In practice, we found that per- forming regression in Labcolor space results in more accu- rate color reproduction in the physical domain than standard RGBcolor space.Algorithm 1Transformation-Invariant Adversarial Pattern Generation Input: Image of adversary x, target image y Output: Adversarial pattern xadv Notations: A: method to compute representative average image (equation 3); f: method to compute face embedding; M: distance metric (e.g., ℓp, cosine); clip: method to clip intensity values (0-255); F: fusion function (e.g. summation); α,β: hyper-parameters 1: procedure OPTIMIZE (x,y) 2: xtemp 0 ←0w×h, xavg 0 ←x, and x0 ←x ⊿ Initialization 3: while not converge do 4: xavg t−1 ←A(xavg t−1) ⊿Compute representative average image 5: xavg t ←clip(xavg t−1 + xtemp t−1 + ζN(0,σ2)) ⊿Clipped representative image 6: xt ←clip(xt−1 + xtemp t−1 ) ⊿Clipped image of adversary 7: Davg ←M(f(xavg t ),f(y)) and D←M (f(xt),f(y)) ⊿Distance computation using metric M 8: ∆η←∇xF(Davg,D)|x=xtemp t−1 ⊿Compute update with respect to both xand xavg 9: gt ←β×gt−1 + ∆η ∥∆η∥1 and xtemp t ←xtemp t−1 + α×sign(gt) ⊿Gradient update with momentum 10: xadv = xt −x ⊿ Adversarial pattern 5. Transformation-Invariant Adversarial Pat- tern Generation Generation of adversarial patterns that are relatively in- variant to intra-adversary facial variations is critical to the success of light projection attacks. Letxand y, respectively, be the images pertaining to the adversary and the target in the digital domain that are used in the adversarial pattern generation process. Existing methods ( e.g., [2], [6]) gen- erate a transformation-invariant adversarial pattern xadv by applying different transformations on the adversary’s im- age. In case of impersonation attack, the following opti- mization is solved: xadv = argmin ∆x k−1∑ i=0 (wiL(Ti(x) + ∆x,y)),s.t.∥∆x∥p ≤ε (2) Here, Ti corresponds to the ith transformation, k is the total number of transformations, and wi corresponds to the weight of Ti such that ∑k−1 i=0 wi = 1 . Also, Lis the loss function used in the adversarial pattern generation process. We assume that Lis computed using a distance metric M in the face embedding space, and hence needs to be min- imized. Alternatively, Lcould be computed using a simi- larity metric in which case it would need to be maximized. Equation 2 involves computation of loss functions with re- spect to each transformation in each iteration. This is com- putationally intensive, and limits the application of these methods in the setting where an adversary wants to generate adversarial patterns in real-time. 5.1. Computing representative adversary image Instead, our method computes an average representative image xavg of the adversary as follows: xavg = A(x) = w0x+ k−1∑ i=1 (wiTi(x)),s.t. k−1∑ i=0 wi = 1 (3) For the impersonation task, the following optimization is then solved: xadv = argmin ∆x L(xavg + ∆x,y),s.t.∥∆x∥p ≤ε (4) Equation 4 does not require explicit computation of loss functions for all possible transformations, yet explores a wide variety of transformation conﬁgurations in each iter- ation. The limitation though is that the optimization is per- formed with respect to a single representative image, and not with respect to expected loss pertaining to each transfor- mation conﬁguration. For real-time light projection attacks, this trade-off is desirable. Also note that although equations 2 and 4 pertain to im- personation, equations for obfuscation can be formulated and solved in a similar manner. 5.2. Using the original adversary image Optimizing with respect to the representative image xavg provides an efﬁcient way to achieve transformation- invariance. However, to ensure that the generated pattern re- tains adversarial characteristics not only for the representa- tive image xavg but for xas well, optimization with respect to both xavg and xis performed. Figure 5 illustrates exam- ple beneﬁt for translation and rotation-invariance. Similar beneﬁts were observed for other transformations.x y x y (a) Translation (b) Rotation Figure 5: Advantage of using the proposed transformation-invariant pattern generation method for (a) translation invariance and (b) rotation invariance. Left and right charts show, respectively, the similarity scores generated by FaceNet [18] between an adversary’s face image with projected adversarial light pattern and the target’s face image, without and with (a) translation- invariance (in xand ydirection in pixels) and (b) rotation-invariance (in degrees). 5.3. Proposed method Algorithm 1 summarizes the proposed transformation- invariant pattern generation method. The method takes as input the image xof the adversary and the image of the tar- get y, and outputs the adversarial pattern xadv. The trans- formations used depend on the invariance objective ( e.g., afﬁne, perspective, photo-metric or others). The conver- gence criteria is either predeﬁned number of steps or thresh- old on distance metric M (step 7). A brightness term sam- pled from a normal distribution is used during each iter- ative update for obtaining invariance to slight illumination changes (step 5). Furthermore, a binary mask can be used to constrain the facial region for which the adversarial pattern is generated similar to [19]. 5.4. Using multiple images of target While the method described above focuses on intra- adversary invariance, it is desirable to impart invariance to intra-target variations as well to increase likelihood of suc- cess. For this, multiple images of the target can be used. Instead of a single target image, algorithm 1 can then be op- timized with respect to target embedding ycomputed using multiple target images. 6. Experimental Evaluation To study the feasibility of adversarial light projection at- tacks, live subject experiments are performed with 50 sub- jects in total. Each experiment is conducted in a room with ﬁxed lighting. A Logitech web camera and a Panasonic or Epson projector are used for the experiments.(a)  (b) Figure 6: Example of impersonation attack on the commer- cial face recognition system in black-box setting. Shown in (a) is the captured image of the adversary’s face with ad- versarial light projected in the physical domain that is rec- ognized as target (b). This example is interesting because it appears that adversarial light resulted in failure of face detection (blue rectangle) yet the impersonation attack suc- ceeded. 6.1. Experimental setup A web-based user interface is designed that takes real- time input of a subject’s face using a web camera, and lets the subject select/upload face images of the target. The in- terface also lets the subject perform calibration based on the camera feed and the connected projector. Multi-task con- volutional neural network (MTCNN)-based face detection and landmark estimation [26] is used for automatic position calibration. For color calibration, the method described in section 4.2 is used. If necessary, the brightness and color in- tensity of the projector is manually tuned via the interface. Post calibration, a python script executes the transformation-invariant pattern generation method (implemented in Tensorﬂow 2.0) for 100 iterations. Cosine is used as distance metric and multiplication as the fusion function. For gradient update, the parameters αand β are set to 1 and 0.7 respectively. The following transformation conﬁgurations (assuming the origin is centered at midpoint of adversary’s image) are considered during generation of the transformation-invariant pattern: −40 to +40 pixels translation in both x and y directions, −π/3 to +π/3 rotation, and 0.5 −2 times scaling. Each transformation conﬁguration is assumed to be equally likely i.e. the weight corresponding to each transformation is set to 1 k where k= 3 is the number of transformations. The computed digital pattern is projected using the pro- jector in the form of adversarial light onto the subject’s face. The subject’s face with light projection is captured for about 30 seconds and used to attack a face recognition algorithm in real-time. The subject is instructed to make natural head movements (e.g. translation, rotation) during the duration of the attack. Similarity score between each captured im- age and the target face image is computed. If the com- Experiment #Subjects #Attempts Success Rate (%) imp-ﬁx-FN 25 25 92.00 imp-ﬁx-SF 25 25 84.00 imp-ﬁx-CO 25 25 60.00 imp-select-FN 15 15 93.33 imp-select-SF 15 15 80.00 imp-top5-FN 10 50 88.00 imp-top5-SF 10 50 78.00 obf-FN 10 10 100.00 obf-SF 10 10 100.00 obf-CO 10 10 70.00 Table 1: Impersonation (imp) and obfuscation (obf) exper- iments conducted on live subjects. FC, SF and CO refer to FaceNet, SphereFace and the commercial face recognition system respectively. Similarity score threshold correspond- ing to FAR=0.01% is used to determine if the attack was successful. puted score for any adversary-target image pair is above the threshold corresponding to False Accept Rate (FAR) of 0.01%, the attack attempt is considered successful. FaceNet [18] and SphereFace [12] are the two face recognition al- gorithms used in white-box setting. In black-box setting, FaceNet is used to generate adversarial pattern to attack a commercial face recognition algorithm. 6.2. Impersonation For impersonation, a face image of a subject (adversary) captured using the camera, and a face image of the target (obtained from the web or a database) are used to gener- ate the digital adversarial pattern. A different face image of the target (also obtained from the web or a database) is assumed to be enrolled in the face recognition system to be attacked. Impersonation attempts are made using different subject pools in the following scenarios. •Fixed target (imp-ﬁx) : Impersonating a ﬁxed high- proﬁle target (Rowan Atkinson). 25 subjects in total attempted this in both white-box and black-box set- ting. 23 and 21 attempts out of 25 on FaceNet and SphereFace, respectively, succeeded. In black-box set- ting, 15 out of 25 attempts on the commercial system succeeded. •Selected target (imp-select): Impersonating any one of the given targets (Taylor Swift, Michael Phelps, or Al- bert Einstein) at random. A total of 15 subjects at- tempted this in white-box setting. 14 and 12 attempts out of 15 on FaceNet and SphereFace, respectively, succeeded.(a)  (b) Figure 7: Example of impersonation attack on SphereFace [12] in white-box setting. Shown in (a) is the captured im- age of the adversary’s face with adversarial light projected in the physical domain that is recognized as target (b). •Top-5 similar targets (imp-top5) : Given a database of target images (Labelled Faces in the Wild (LFW) database [9]), impersonate top-5 most similar targets based on a face recognition algorithm. 10 different subjects attempted this on ﬁve most similar targets from LFW in white-box setting. 44 and 39 out of 50 at- tempts on FaceNet and SphereFace, respectively, suc- ceeded. Table 1 summarizes the experimental results. Figures 1, 7 and 6 show successful examples of impersonation, respec- tively, on FaceNet, SphereFace and the commercial face recognition system. 6.3. Obfuscation For obfuscation (obf), two face images of a subject (adversary-target pair) captured using the camera are used to generate digital adversarial pattern. A different face im- age of the same subject (target) is assumed to be enrolled. 10 different subjects attempted obfuscation attacks in both white-box and black-box setting. All 10 obfuscation at- tempts on FaceNet and SphereFace succeeded in white-box setting, whereas in black-box setting 7 out of 10 attempts on the commercial face recognition system succeeded. Ta- ble 1 summarizes the experimental results. Figure 8 shows a successful example of obfuscation on the commercial face recognition system. 6.4. Failure Cases While most impersonation and obfuscation attempts are successful, failure of adversarial light projection attacks is observed due to one or more of the following reasons: •Light projection either covering the entire face or sig- niﬁcantly occluding majority of the face resulting in failure of face detection. Projection of adversarial light on a particular part of the face, e.g., cheeks or fore- head, is found to result in higher likelihood of success in practice. Figure 8: Example of obfuscation attack on the commercial face recognition system in black-box setting. Adversarial light projected on the adversary’s face in the physical do- main results in successful obfuscation. •Strong ambient or directional lighting that overpowers the projected light. •Extreme facial pose of the adversary. In practice, how- ever, this is less likely as the adversary is cooperative. •Out of focus light projection on the adversary’s face when the projector lens is not tuned appropriately. Manual tuning of the projector lens to ensure the pro- jected light is properly focused on the adversary’s face is important for a successful attack attempt in practice. We plan to investigate failure cases in a more systematic manner in a follow-up study. 7. Conclusions and Future Work We show the feasibility of conducting impersonation and obfuscation attacks using adversarial light projections on two open-source face recognition systems in white-box set- ting and a commercial face recognition system in black-box setting. Furthermore, an efﬁcient transformation-invariant adversarial pattern generation method that enables an ad- versary to conduct light projection attacks in real-time is presented. While we have shown the feasibility of light projection attacks, we have not systematically tested the likelihood of success at different operating thresholds of face recognition systems in different environments. One of our immediate goals, therefore, is to systematically investigate the impact of environmental and subject-dependent covariates (such as lighting, subject orientation and pose) on the repeatability of light projection attacks. Furthermore, we suspect that presentation attack detection methods designed for static at- tacks using 2D or 3D fabricated artifacts will be inadequate in defending against dynamic adversarial attacks such as light projection attacks. Therefore, we also plan to conduct an evaluation of existing defense mechanisms and develop novel defense mechanisms for such dynamic attacks.References [1] Pico Mini Portable Projector. https://www.amazon.com/Portable-Projector-Haidiscool- Smartphone-Entertainment/dp/B07DWX5FGM/, 2019. [Online; accessed 10-September-2019]. 2 [2] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok. Syn- thesizing robust adversarial examples. arXiv preprint arXiv:1707.07397, 2017. 3, 5 [3] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pages 39–57. IEEE, 2017. 1, 3 [4] S. Chen, C. Cornelius, J. Martin, and D. Chau. Shapeshifter: Robust physical adversarial attack on faster r-cnn object de- tector. In Joint European Conference on Machine Learn- ing and Knowledge Discovery in Databases , pages 52–68. Springer, 2018. 3 [5] Y . Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li. Boosting adversarial attacks with momentum. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9185–9193, 2018. 2 [6] Y . Dong, T. Pang, H. Su, and J. Zhu. Evading defenses to transferable adversarial examples by translation-invariant at- tacks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4312–4321, 2019. 5 [7] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, and D. Song. Robust physical- world attacks on deep learning visual classiﬁcation. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1625–1634, 2018. 1, 3 [8] I. Goodfellow, J. Shlens, and C. Szegedy. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. 1, 2 [9] G. Huang, M. Mattar, T. Berg, and E. Learned-Miller. La- beled faces in the wild: A database for studying face recog- nition in unconstrained environments. 2008. 8 [10] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial exam- ples in the physical world. arXiv preprint arXiv:1607.02533, 2016. 1, 2, 3 [11] J. Li, F. Schmidt, and Z. Kolter. Adversarial camera stick- ers: A physical camera-based attack on deep learning sys- tems. In International Conference on Machine Learning , pages 3896–3904, 2019. 3 [12] W. Liu, Y . Wen, Z. Yu, M. Li, B. Raj, and L. Song. Sphereface: Deep hypersphere embedding for face recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 212–220, 2017. 3, 7, 8 [13] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In Proceedings of the International Conference on Learning Representations, 2018. 1, 3 [14] S. Marcel, M. Nixon, and S. Li. Handbook of biometric anti- spooﬁng, volume 1. Springer, 2014. 1 [15] A. Modas, S. Moosavi-Dezfooli, and P. Frossard. Sparsefool: a few pixels make a big difference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 9087–9096, 2019. 3 [16] S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2574–2582, 2016. 1, 3 [17] N. Nichols and R. Jasper. Projecting trouble: Light based ad- versarial attacks on deep learning classiﬁers. arXiv preprint arXiv:1810.10337, 2018. 2, 3 [18] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uniﬁed embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815–823, 2015. 1, 3, 6, 7 [19] M. Sharif, S. Bhagavatula, L. Bauer, and M. Reiter. Ac- cessorize to a crime: Real and stealthy attacks on state-of- the-art face recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Se- curity, pages 1528–1540. ACM, 2016. 1, 6 [20] Y . Shi, S. Wang, and Y . Han. Curls & whey: Boosting black- box adversarial attacks. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, 2019. 2 [21] D. Song, K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, F. Tramer, A. Prakash, and T. Kohno. Physical adversarial examples for object detectors. In12th {USENIX} Workshop on Offensive Technologies ({WOOT} 18), 2018. 3 [22] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In Proceedings of the International Conference on Learning Representations, 2014. 1, 2 [23] S. Thys, W. Van Ranst, and T. Goedem´e. Fooling automated surveillance cameras: adversarial patches to attack person detection. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition Workshops, pages 0–0, 2019. 3 [24] L. Wu, Z. Zhu, C. Tai, et al. Understanding and enhancing the transferability of adversarial examples. arXiv preprint arXiv:1802.09707, 2018. 2 [25] C. Xie, Z. Zhang, Y . Zhou, S. Bai, J. Wang, Z. Ren, and A. Yuille. Improving transferability of adversarial examples with input diversity. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 2730– 2739, 2019. 2 [26] K. Zhang, Z. Zhang, Z. Li, and Y . Qiao. Joint face detection and alignment using multitask cascaded convolutional net- works. IEEE Signal Processing Letters, 23(10):1499–1503, 2016. 7 [27] Y . Zhao, H. Zhu, Q. Shen, R. Liang, K. Chen, and S. Zhang. Practical adversarial attack against object detector. arXiv preprint arXiv:1812.10217, 2018. 3 [28] Z. Zhou, D. Tang, X. Wang, W. Han, X. Liu, and K. Zhang. Invisible mask: Practical attacks on face recognition with in- frared. arXiv preprint arXiv:1803.04683, 2018. 2, 3",
      "references": [
        "Synthesizing robust adversarial examples",
        "Towards evaluating the robustness of neural networks",
        "Shapeshifter: Robust physical adversarial attack on faster r-cnn object detector",
        "Boosting adversarial attacks with momentum",
        "Evading defenses to transferable adversarial examples by translation-invariant attacks",
        "Robust physical-world attacks on deep learning visual classiﬁcation",
        "Explaining and harnessing adversarial examples",
        "Labeled faces in the wild: A database for studying face recognition in unconstrained environments",
        "Adversarial examples in the physical world",
        "Adversarial camera stickers: A physical camera-based attack on deep learning systems",
        "Sphereface: Deep hypersphere embedding for face recognition",
        "Towards deep learning models resistant to adversarial attacks",
        "Handbook of biometric anti-spooﬁng, volume 1",
        "Sparsefool: a few pixels make a big difference",
        "Deepfool: a simple and accurate method to fool deep neural networks",
        "Projecting trouble: Light based adversarial attacks on deep learning classiﬁers",
        "Facenet: A uniﬁed embedding for face recognition and clustering",
        "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition",
        "Curls & whey: Boosting black-box adversarial attacks",
        "Physical adversarial examples for object detectors",
        "Intriguing properties of neural networks",
        "Fooling automated surveillance cameras: adversarial patches to attack person detection",
        "Understanding and enhancing the transferability of adversarial examples",
        "Improving transferability of adversarial examples with input diversity",
        "Joint face detection and alignment using multitask cascaded convolutional networks",
        "Practical adversarial attack against object detector",
        "Invisible mask: Practical attacks on face recognition with infrared"
      ],
      "meta_data": {
        "arxiv_id": "2003.11145v2",
        "authors": [
          "Dinh-Luan Nguyen",
          "Sunpreet S. Arora",
          "Yuhang Wu",
          "Hao Yang"
        ],
        "published_date": "2020-03-24T23:06:25Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Investigated the feasibility of real-time physical attacks on face recognition systems using adversarial light projections for impersonation and obfuscation. Developed an efficient transformation-invariant adversarial pattern generation method suitable for real-time application. Demonstrated the vulnerability of state-of-the-art face recognition systems to these attacks in both white-box and black-box settings.",
        "methodology": "The attack comprises two main steps: (1) calibration of the camera-projector setup and computation of a digital adversarial pattern, and (2) projection of the digital pattern onto the adversary's face. Calibration involves position calibration (manual or automatic using facial landmarks) and color calibration (estimating a color transformation function in Lab color space). The transformation-invariant adversarial pattern generation method optimizes patterns to be robust against intra-adversary facial variations (e.g., translation, rotation, scaling) by computing an average representative image and optimizing with respect to both this average and the original adversary image. It uses a distance metric (e.g., cosine) in the face embedding space and incorporates a brightness term for illumination invariance. The method can also utilize multiple target images for robustness against intra-target variations. Attacks can be for impersonation (being recognized as a target) or obfuscation (evading recognition).",
        "experimental_setup": "Live subject experiments were conducted with 50 subjects in a room with fixed lighting, using a Logitech web camera and Panasonic or Epson projectors. A web-based user interface facilitated real-time input, calibration (automatic position calibration via MTCNN, color calibration as described in the methodology), and pattern generation. The transformation-invariant pattern generation method (implemented in TensorFlow 2.0) ran for 100 iterations, using cosine as the distance metric and multiplication as the fusion function, with α=1 and β=0.7 for gradient updates. Transformation configurations included -40 to +40 pixels translation, -π/3 to +π/3 rotation, and 0.5-2 times scaling, each with equal weight. Attack attempts involved projecting the pattern onto the subject's face for 30 seconds while they made natural head movements. Success was determined if the similarity score between the captured image and the target face exceeded a threshold corresponding to a False Accept Rate (FAR) of 0.01%. FaceNet and SphereFace were used for white-box settings, while a commercial face recognition system was attacked in a black-box setting using patterns generated from FaceNet. Impersonation experiments included fixed target, selected target, and top-5 similar targets scenarios. Obfuscation experiments involved the adversary attempting to evade recognition.",
        "limitations": "The study did not systematically test the likelihood of attack success at different operating thresholds of face recognition systems across various environments. Observed failure cases included light projection covering the entire face or significantly occluding it (leading to face detection failure), strong ambient or directional lighting overpowering the projected light, extreme facial poses of the adversary, and out-of-focus light projection due to improper projector lens tuning.",
        "future_research_directions": "Future work includes systematically investigating the impact of environmental and subject-dependent covariates (such as lighting, subject orientation, and pose) on the repeatability of light projection attacks. Additionally, plans involve evaluating existing presentation attack detection methods (designed for static 2D/3D fabricated artifacts) against dynamic adversarial light projection attacks and developing novel defense mechanisms specifically for such dynamic attacks.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Investigating Mysteries of CoT-Augmented Distillation",
      "full_text": "Front. Comput. Sci., 2024, 0(0): 1–36 https://doi.org/10.1007/sxxxxx-yyy-zzzz-1 REVIEW ARTICLE The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends Mengqi Chen1, Bin Guo(B)1, Hao Wang1, Haoyu Li1, Qian Zhao1, Jingqi Liu1, Yasan Ding1, Yan Pan2, Zhiwen Yu1 1 Northwestern Polytechnical University, Xi’an, China 2 National University of Defense Technology, Changsha, China © Higher Education Press 2024 Abstract Persuasion, as one of the crucial abili- ties in human communication, has garnered exten- sive attention from researchers within the field of intelligent dialogue systems. Developing dialogue agents that can persuade others to accept certain standpoints is essential to achieving truly intelli- gent and anthropomorphic dialogue systems. Ben- efiting from the substantial progress of Large Lan- guage Models (LLMs), dialogue agents have ac- quired an exceptional capability in context under- standing and response generation. However, as a typical and complicated cognitive psychological sy- stem, persuasive dialogue agents also require knowl- edge from the domain of cognitive psychology to attain a level of human-like persuasion. Conse- quently, the cognitive strategy-enhanced persuasive dialogue agent (defined as CogAgent), which in- corporates cognitive strategies to achieve persua- sive targets through conversation, has become a pre- dominant research paradigm. To depict the research trends of CogAgent, in this paper, we first present several fundamental cognitive psychology theories and give the formalized definition of three typical cognitive strategies, including the persuasion strat- egy, the topic path planning strategy, and the argu- ment structure prediction strategy. Then we pro- pose a new system architecture by incorporating the formalized definition to lay the foundation of CogAgent. Representative works are detailed and investigated according to the combined cognitive strategy, followed by the summary of authoritative benchmarks and evaluation metrics. Finally, we summarize our insights on open issues and future directions of CogAgent for upcoming researchers. Keywords Persuasive dialogue, cognitive strat- egy, cognitive psychology, persuasion strategy Received month dd, yyyy; accepted month dd, yyyy E-mail: guob@nwpu.edu.cn arXiv:2402.04631v1  [cs.CL]  7 Feb 20242 Front. Comput. Sci., 2024, 0(0): 1–36 1 Introduction Dialog agents can engage in chitchat with humans to establish certain emotional connections or help us complete tasks through long-form conversations (e.g., restaurant reservation, travel time arrange- ment). Building intelligent human-machine dia- logue agents that can conduct natural and engag- ing conversations with humans is the long-standing goal of artificial intelligence (AI) [1, 2]. Moreover, the persuasive ability of dialogue agents has gar- nered extensive attention from researchers. Persua- sion is one of the crucial abilities in human commu- nication. The Elaboration Likelihood Model (ELM) theory [3] suggests that people tend to engage with persuasive messages when communicating with oth- ers. It is a prevalent phenomenon for individu- als to hold diverse perspectives on a given topic and endeavor to influence others in altering their viewpoints, attitudes, or behaviors through conver- sational interactions [4, 5]. There are massive per- suasive scenarios in the real world, such as bargain- ing prices of goods, debating on specific topics, and arguing in online comment sections [6, 7], and active online communities, such as Debate 1) and ChangeMyView2), for people to communicate and influence other people’s opinions by posting their views [8]. A persuasive conversion includes two distinct parties, corresponding to persuader and per- suadee, respectively [9]. The goal of the persuader is to change the persuadee’s viewpoint on a spe- cific topic by combining cognitive strategies, the personality of the persuadee, and other context fea- tures [10, 11]. The development of intelligent per- suasive dialogue agents that can persuade users to accept certain standpoints is emerging as a promis- 1)https://www.debate.org/ 2)https://www.reddit.com/r/changemyview/ ing research field [12, 13]. Modern dialogue agents have arrived at the era characterized by large language models (LLMs) [14, 15]. Driven by an immense scale of parameters and an abundance of training data, dialogue agents (e.g., ChatGPT3), LLaMA [16], Claude [17], Chat- GLM4)) have acquired an exceptional capability of context understanding and response generation [18, 19], reaching a satisfactory level of fluency, logic, emotional expression and personalization when con- versing with humans [20,21]. In addition to engag- ing in casual conversations with humans, existing dialogue agents, represented by ChatGPT, can as- sist humans in accomplishing intricate tasks, such as writing codes [22, 23], writing long and coher- ent academic papers [24, 25] and aiding in o ffice works (e.g., Microsoft Copilot5)), thereby substan- tially augmenting productivity and the quality of life. However, the persuasion process is an activity that involves human psychological cognition [10, 26, 27]. The design of persuasive dialogue agents needs to incorporate cognitive strategies to orga- nize content, logic, and presentation of dialogue re- sponse reasonably from the cognitive psychology perspective. There have been considerable works of persuasive dialogue systems, which mainly en- hance persuasiveness from three aspects, namely, integrating persuasion strategies, planning topic pa- ths, and extracting argument structures. For ex- ample, Wang et al.[11] provide an in-depth anal- ysis of the impact of persuasion strategies on the persuasive power of dialogue systems in the con- text of persuasion for donation scenarios. To en- able persuasion e fficiently, Qin et al. [28] drive a 3)https://openai.com/blog/chatgpt/ 4)https://github.com/THUDM/ChatGLM3 5)https://adoption.microsoft.com/en-us/ copilot/Mengqi Chen et al. 3 Fig. 1 The example of persuasive dialogue, where the dia- logue agent persuades the user to relieve for job crisis using various persuasion strategies. conversation towards a specified persuasive target with explicit topics /keywords planning over kno- wledge graphs. Arguments are also an important source of e ffective persuasive content. Prakken et al. [29] develop a dialogue system wherein an argu- ment graph serves as the persuasive knowledge for persuasive response generation. Different from ex- isting works that focus on the persuasiveness of di- alogue systems, in this paper, we argue that persua- sion is a cognitive psychology activity and that the persuasion strategy, the topic path planning strat- egy, and the argument structure prediction strategy can all be categorized as cognitive strategies. We define cognitive strategy-enhanced persuasive dia- logue agent as CogAgent. CogAgent aims to integrate a variety of cogni- tive strategies to ensure that the generated dialogue contents can effectively influence the persuadee in terms of their perceptions, opinions, or attitudes [11, 30]. CogAgent has great potential in many scenarios, such as counseling depressed children [31], persuasion for social good [11], winning de- bates [32], and recommending items to users [33, 34]. Fig. 1 depicts a persuasive dialogue example. The dialogue agent persuades the user to reduce anxiety from job crises using various persuasion strategies. The social and communicative dynam- ics behind persuasive dialogue contexts are com- plex. E ffective and successful persuasive dialogue does not mechanically convey target viewpoints to persuadees but rather empathetically addresses per- suadees through social and emotional communi- cations [35]. Thus, persuasive dialogues are not strictly task-oriented but are carried around tasks with additional cognitive strategies to build trust and empathy with persuadees, leading to a smooth persuasive process. As an emerging research area, an in-depth sur- vey of the existing academic e fforts is necessary. Duerr et al.[36] broadly reviews the works that use natural language generation to automatically de- tect and generate persuasive texts. Zhan et al.[37] concentrates on the negotiation dialogue system, a typical type of persuasive dialogue system, and comprehensively summarizes benchmarks, evalu- ations, and methodologies of negotiation dialogue systems. Deng et al.[38] provide an overview of the prominent problems and advanced designs in proactive dialogue systems, which treats persua- sive dialogue as the subset of the proactive dia- logue. Compared with these surveys, we provide a comprehensive review of concepts, challenges, methodologies, and applications in the field of cog- nitive strategy-enhanced persuasive dialogue. We formalize the definition of cognitive strategies ex- tended from cognitive psychology theory. Based on the formalized concept model and generic sys- tem architecture, we summarize representative re- search in the field of CogAgent from a systematic perspective. Furthermore, benchmarks, evaluation metrics, and thoughts on promising research trends are analyzed to promote the research progress. To sum up, our contributions are summarized as fol- lows.4 Front. Comput. Sci., 2024, 0(0): 1–36 • Drawing from cognitive psychology theories, we formalize the definition of cognitive strat- egies, and present the concept model and gen- eric system architecture of CogAgent, to pro- vide an overall picture for the summary of research works. • We make a profound investigation of the de- velopment in CogAgent by presenting the co- re contributions of each work, according to the addressed challenges. Besides, we also comprehensively summarize available datase- ts and evaluation metrics. • We further discuss some open issues and pro- mising research trends in CogAgent, includ- ing model adaptivity/generality of CogAgent, multi-party CogAgent, multimodal CogAge- nt, etc., to promote the development of the research community. The rest of the paper is organized as follows. In Section 2, we first summarize the typical cog- nitive psychology theories and present the defini- tion of cognitive strategies. Then we formalize the concept model of CogAgent and design a generic system architecture, followed by typical applica- tion scenarios of CogAgent. In Section 3, we first introduce the challenges faced by CogAgent and then summarize the key techniques to achieve Co- gAgent based on the user cognitive strategies. In Section 4, we summarize the available datasets and evaluation metrics, followed by open issues and promising research trends in Section 5. 2 Formalized Concept Model and Sys- tem Architecture for CogAgent In this section, we first summarise the typical cog- nitive psychology theories involved in human con- versations, as the theoretical foundation for the de- sign of CogAgent. Then we formalize the concept model for CogAgent and present the generic sys- tem architecture to visualize the overall picture in CogAgent. 2.1 The Cognitive Psychology Theory As a typical cognitive-psychological activity, the persuasion process requires the support of cogni- tive psychology theories to e ffectively model the mental changes that people experience during con- versations, thus promoting the design of CogAgent. This section summarises typical cognitive psychol- ogy theories to inspire subsequent CogAgent re- searchers. 2.1.1 Pre-suasion The concept of Pre-suasion [39], proposed by the renowned authority on persuasion, Robert Cialdini, is a prominent theory in the persuasion field. Pre- suasion means that the success rate of persuasion can be significantly enhanced by attracting the at- tention of the persuadee through appropriate choic- es of words and actions before communication or requests are conducted. Pre-suasion emphasizes that the timing of persuasion is as important as per- suasive content. When we intend to persuade oth- ers to accept our points, we need to consider others’ perspectives and organize our conversational argu- ments at the appropriate time to e ffectively com- plete the persuasion process. 2.1.2 Principle of Consistency The principle of consistency suggests that people usually try to maintain consistency based on what they have expressed and the commitments they have made in the past [40]. By planning topic paths, one can think about and define one’s opinions and arguments in advance to maintain consistency andMengqi Chen et al. 5 increase persuasiveness when communicating with others. The principle of consistency plays an im- portant role in persuading others. Through consis- tency of statements, the persuadees will recognize that the points raised are consistent with their be- liefs or opinions and will e ffectively increase the effectiveness of persuasion. 2.1.3 Theory of Mind The theory of mind (ToM) [41] suggests that effec- tive questions and answers in communications are based on a shared world of experiences and refer- ents between interlocutors. To communicate effec- tively, people model both the mental states of their listeners and the e ffects of their behavior on the world, and then react to and predict the behavior of others. This ability to understand and infer human intentions is defined as a ToM. One way to imitate ToM is to observe others’ perspectives in various situations and to derive a set of rules that a ffect their perspectives and emotions. When the same or highly similar scenarios reoccur, we can make reasonable behavioral or emotional predictions ac- cordingly. Many researchers explicitly model ToM as a concrete cognitive process to ensure that dia- logue agents can access potential human psycho- logical states and cognitive processes [42–44]. 2.1.4 Rhetoric Aristotle, one of the earliest masters of the art of persuasion, proposes three basic elements of per- suasion: ethos (credibility), pathos (emotions), and logos (logic) in his work, The Philosophy of Rheto- ric [45]. These principles serve as a guide to ef- fective persuasive communication. By establish- ing credibility, appealing to emotions, and applying logical reasoning, one can effectively persuade oth- ers to accept his propositions. Aristotle’s insights in Rhetoric remain highly influential not only in the field of persuasive dialogue but also in shaping our understanding of aesthetics and related concepts. Credibility represents the identification of per- suaders, including their identity and moral char- acter, which influences the persuasiveness of the speaker. Aristotle in his Rhetoric explains in detail the three elements that a ffect credibility, namely wisdom, virtue, and goodwill. Wisdom includes elements such as breadth of knowledge, expertise, and authority. Virtue includes elements such as fairness, honesty, and dignity. By demonstrating wisdom, virtue, and goodwill, persuaders can en- hance their persuasiveness and foster the trust and reliability of persuadees. Combining essential ele- ments of credibility can greatly enhance the e ffec- tiveness of CogAgent. Emotion refers to the ex- pression of sentiments during the persuasion pro- cess, thus lowering people’s psychological defenses in accepting persuasive content. Aristotle stated that we cannot persuade others through rationality, but can achieve it with emotion. Emotional expres- sions play an important role in changing the cog- nitive decisions of others. The use of emotionally charged content and expressions can be more ef- fective in eliciting agreement and empathy from the persuadee. Logic refers to the use of inher- ent factual logic, causality, or other rational fac- tors in expressions to gain persons’ trust and per- suade them to change their perceptions. By pre- senting coherent logical arguments, supported by factual data and authoritative sources, persuaders can establish credibility, gain persuadees’ percep- tions, and change their opinions. The cognitive psychology theories, which can be used to model the dynamics of human cogni- tive psychological status, provide a solid founda- tion for CogAgent. Under the guidance of cogni-6 Front. Comput. Sci., 2024, 0(0): 1–36 Table 1 Part of definitions and examples of persuasion strategies. Strategy Definition Example Present of Facts Using factual evidence (e.g., official news reports, statistics) and a credible reasoning process to persuade others In recent months, the demand for residential properties has become extremely high. The price of residential property has risen almost twenty percent. Challenges and Inquiries Expressing disbelief or opposition to the other side’s viewpoints and providing strong rebuttal evidence to enhance persuasiveness Really? I don’t agree. This Star Wars episode was incredible! Emotional Resonance Eliciting specific emotions to influence others’ attitudes State-of-the-art special effects are the main reason for the success of previous episodes, so audiences have high expectations for this one, and I don’t think they will be disappointed Eliciting Anger If that’s the case, there’s not much point in further discussion. We might as well call the whole deal off. Eliciting Guilt Come on, you can at least try a little, besides your cigarette. Self- modeling Indicating one’s intention to act and choosing to act as a role model for the persuadee to follow That still leaves a gap of 20 dollars to be covered. Let’s meet each other halfway once more, then the gap will be closed and our business completed. Building Trust Building rapport and psychological trust through a harmonious conversation I’m glad we’ve agreed on price. We’ll go on to the other terms and conditions at our next meeting. Courtesy Tips Expressing gratitude, approval, praise, etc. to lower the other party’s psychological defensesI know exactly what you mean. Hearing that song gives me a nostalgic feeling. Compromise Expressing concessions on time to avoid being too intense in the guidance process and causing the other party to end the conversation I think it unwise for either of us to insist on his price. How about meeting each other halfway so that business can be concluded? Attachment of Views Expressing kindness and concern through active listening and to some extent seconding the other person’s point of view Better late than never. tive psychology, we can comprehensively investi- gate and model explicit cognitive factors and strate- gies that can change users’ cognitive psychological states, such as logical expressions and emotional appeals. These cognitive strategies can facilitate CogAgent to understand the psychological state of the persuadee and enhance the persuasiveness of responses from multiple perspectives to achieve mo- re efficient persuasion processes. 2.2 Cognitive Strategy To achieve efficient persuasion, it is necessary to integrate various cognitive strategies for precisely responding to the psychological changes of the per- suadee. Evolved from cognitive psychological the- ories, we categorize cognitive strategies into three aspects, persuasion strategy, topic path planning strategy, and argument structure prediction stra- tegy, detailed as follows. 2.2.1 Persuasion Strategy The persuasion strategy aims to influence or change the perceptions, opinions, attitudes, or behaviors of persuadees from a psychological standpoint, thro- ugh the use of linguistic techniques of expression, such as logical appeal, foot-in-the-door, and self- disclosure [4, 11, 46]. Based on existing research, we construct a comprehensive and e ffective set of persuasion strategies that can achieve persuasive goals, inspired by the theory of mind, the rhetoric, and other psychology theories. We formalize the definitions and examples of expressions of persua- sion strategies, as shown in Table 1 and Table 2. Numerous studies [11,47–49] have demonstrated that persuasion strategies can e ffectively enhance the persuasiveness of the dialogue content. How to reasonably select the appropriate strategies accord- ing to the dialogue context and the perusadee’s psy- chological state to generate a persuasive dialogue response is crucial to achieving high-quality Co-Mengqi Chen et al. 7 Table 2 Part of definitions and examples of persuasion strategies. Strategy Definition Example Problem De- composition Decomposing the ultimate persuasion goal into sub-issues and stepping through the persuasion process Let me get down some information about your apartment first. what is your property’s address? Social Identity Gaining psychological support from the other person by emphasizing group and identity belonging I know. I have been a subscriber for the past two years. Herd Mentality Presenting a viewpoint that is recognized or accepted by the majority of people and persuading the other side to accept it There was always a good round of applause every time she sang. Expression of Disgust Expressing a particular point of view or emotion to emphasize the persuasive content Oh, my god! I look so old. I look as if I were 40. I think it’s time for some plastic surgeries. Expression of Empathy I know, dear. I am too. But we’ve just been too busy to look for a house. Expression of Views That means the apartment has furniture in it. Logical Appeal Enhancing the credibility of persuasive content through the logical and reasoning process It certainly is. But to tell you the truth, the room is so large that I can share it with someone else, and that will decrease the total amount of the rent. Task Inquiry Asking questions related to persuasive goals That might be going overboard a bit. How about just that scarf with a bracelet? Personal Story Using narrative examples to illustrate the positive outcomes of your actions to inspire others to follow suit Yes, I’m sure I’ve done a lot of house painting in my life. If I got even a tiny drop of paint on her furniture, she would get furious. So I learned to be very picky. Refutation of Objections Directly refuting the other side’s point of view Not necessary. If we use a realtor to find a house, it will be more expensive. Greeting Greeting at the beginning of a dialogue Hi there! How are you doing today? gAgent. 2.2.2 Topic Path Planning Strategy The topic path planning strategy aims to plan the topic transition sequence during the persuasive di- alogue process, to ensure the dialogue coherence and the progress of the dialogue towards the per- suasive target. The persuasive dialogue agent shou- ld smoothly navigate between topics to reduce ir- relevant associations of the persuadee and the dif- ficulty of the persuasion process [50,51]. The topic path planning strategy is widely employed in target- guided persuasive dialogue systems [52,53]. Start- ing from the topic of interest to the persuadee, the persuasive dialogue agent needs to gradually and smoothly transfer the conversation topic to the per- suasive target to improve the persuadee’s psycho- logical acceptability and ensure the persuasive ef- fect. How to plan the reasonable topic path and generate an in-depth multi-turn persuasive conver- sation according to the corresponding topics is to be explored. 2.2.3 Argument Structure Prediction Strategy Argument structure prediction strategy is designe- d to predict persuasive and authoritative argument surrounding the discussed topic, thereby enhancing the credibility of persuasive dialogue contents and convincing the persuadee of the plausibility of the proposed claims [54–56]. Persuasive dialogue age- nts need to be equipped with a large-scale library of arguments and counter-arguments. By predicting reasonable argument structures based on specific persuasive topics, dialogue agents can incorporate coherent argumentation skills, such as citing au- thorities and providing convincing arguments and evidence, to effectively enhance the plausibility of dialogue contents and the credibility of the persua- sion process. The argument structure prediction strategy has been extensively explored in the field of debate dialogue, where debaters often consider argument structures to express viewpoints with clar-8 Front. Comput. Sci., 2024, 0(0): 1–36 ity, logical coherence, and compelling evidence [32, 57,58]. With the argumentative structure, the whole persuasive process can be progressed incrementally, and the overall organization, logical coherence, and credibility of the persuasive process can be signifi- cantly increased. How to mine the supporting argu- ment structures based on the dialogue context and reasonably integrate the argument structures into dialogue contents to enhance the credibility of per- suasive dialogue is to be investigated. 2.3 Formalized Concept Model for CogAgent Based on the definitions of cognitive strategies, we define the dialogue system that is incorporated with cognitive strategies to accomplish persuasive tasks through smooth and accessible conversations as C- ognitive Strategy-enhanced Persuasive Dialogue (- CogAgent). We introduce the formalized concept model of CogAgent as follows. Typically, given the dialogue context sequence H = {(Q1, A1), ...,(QS −1, AS −1)}with S -1 turns, wh- ere Qi and Ai are the dialogue query and response at the i-th dialogue turn, and the current dialogue query QS = (q1, ...,qm) with m words, the objec- tive of general dialogue system is to generate the dialogue response AS = (a1, ...,an) with n words. The modern dialogue systems usually follow the encoder-decoder architecture [2,59] or decoder-on- ly architecture [16,60]. For the encoder-decoder ar- chitecture, the encoder aims to transform input text sequence into vector representations using LSTM [61], Transformer [62] or other advanced neural models, as shown in Eq. 1. QS, H = Encoder(QS , H) (1) Based on the semantic vectors of dialogue con- text and input query, the decoder generates the dia- logue response word by word in an auto-regressive manner, as shown in Eq. 2, where at is the t-th words in the response. P(A|QS, H) = nY t=1 p(at|QS, H, a<t) (2) For the decoder-only architecture, all input text sequences will concatenated into a uniform sequen- ce with special tokens, and then the decoder also generates the response in a word-by-word manner. The general dialogue system can generate smooth and fluent responses based on the dialogue context. To generate persuasive dialogue content, it is es- sential to combine three kinds of cognitive strate- gies. Based on the definitions of three cognitive strate- gies, we give the formalized definition of CogA- gent. Given the dialogue context and the current query, CogAgent needs to first predict the persua- sion strategy Per, conversation topic Top , and the argument content Arg based on the current dialogue content, as follows. Per, Top , Arg = S trPre(QS, H) (3) where S trPre refers to the cognitive strategy pre- dictor. Then the dialogue decoder generates the di- alogue response word by word conditioned on ad- ditional cognitive strategies, as shown in Eq. 4. P(A|QS, H, Per, Top , Arg) = nY t=1 p(at|QS, H, Per, Top , Arg, a<t) (4) 2.4 Generic System Architecture After the concept model of CogAgent, we present the generic system architecture of CogAgent, asMengqi Chen et al. 9 Fig. 2 The generic system architecture of CogAgent. shown in Fig 2. The overall process of CogAgent starts from the semantic understanding of dialogue context and the persuasive target, powered by LLMs (e.g., ChatGPT, LLaMa, Claude, ChatGLM). The input text will be encoded into semantic embed- dings for subsequent processes. TheCognitive Stra- tegy Mining part is responsible for mining cogni- tive strategies, including persuasion strategies, topic paths over knowledge graph, and argument struc- ture of topics. The Cognitive Strategy Prediction for Dialogue Modelling part predicts appropriate cognitive strategies based on dialogue context and enhances the linguistic expression, logical struc- ture, and other persuasive aspects of responses. The persuasion strategy mining process first min- es various kinds of persuasion strategies through crowd strategy emergence based on cognitive psy- chology theories. According to the dialogue con- text, the persuasion strategies to be used in subse- quent rounds of dialogue will be predicted. The topic graph construction process constructs topic graphs or topic paths and then plans the wander- ing paths of topics for persuasion according to the dialogue context and persuasion strategies. The ar- gument mining process first constructs a complete argument structure from credible data sources and then predicts the arguments needed for persuasion based on the above cognitive strategies. Finally, the cognitive strategies-enhanced dialogue context will be fed into LLMs to generate persuasive dialogue responses, for numerous applications, such as psy- chological counseling, bargaining, and persuasion for social good.10 Front. Comput. Sci., 2024, 0(0): 1–36 2.5 Application Scenarios Persuasive dialogue system has widespread appli- cations in daily life. It is an ongoing e ffort of the academic/industry researchers to conduct persua- sive dialogue with users to achieve persuasive tar- gets, summarized as follows. Persuasion for social good. Persuasion for so- cial good is a typical persuasive dialogue scenario where people are persuaded to donate money or goods to charities for social good purposes, such as children’s aid and natural disaster relief. Many researchers have explored how to combine persua- sion strategies to promote users’ donation behavior. For example, Wanget al.[11] provide an insightful analysis of what persuasion strategies are effective for what types of personal characteristics of users. Mishra et al.[63] propose a Reinforcement Learn- ing (RL) based persuasive dialogue system with an efficient reward function consisting of five di ffer- ent sub rewards, Persuasion, Emotion, Politeness- Strategy Consistency, Dialogue-Coherence, and N- on-repetitiveness. Chen et al.[35] produce a mod- ular persuasive dialogue system that seamlessly in- tegrates factual information and persuasive content into generated dialogue response using the condi- tional language model. Persuasion for psychological counseling. The frequent occurrence of mental diseases, such as de- pression, makes mental health gradually receive ex- tensive attention from society [64–66]. Psycho- logical counseling aims at reducing people’s emo- tional distress and helping them understand and wo- rk through the challenges that they face. Relieving the psychological pressure of the persuaded through conversation holds profound significance for the pe- rsuasive dialogue system. Extensive studies have explored the possibility of using persuasive dialogue systems to provide psychological counseling. For example, Liu et al. [46] collect the Emotion Sup- port Conversation dataset (ESConv) with well-desi- gned persuasion strategy annotation to train dia- logue system to provide emotional support through dialogue interactions. Zhou et al.[67] build a com- monsense cognition graph and an emotional con- cept graph based on commonsense knowledge from COMET [68] and concept knowledge from Con- ceptNet [69]. The two kinds of knowledge are alig- ned to generate dialogue responses for emotional support. Persuasion for negotiation. Negotiation is a common real-life persuasion scenario in which two parties negotiate through ongoing conversations to persuade the other party to accept the terms or de- mands they make to maximize their interests. Ne- gotiation is a necessary means of facilitating agree- ments among people and improving the e fficiency of society. There have been several studies using persuasive dialogue systems to achieve negotiation. For instance, Joshi et al. propose DIALOGRAPH [13], a negotiation dialogue system that explicitly incorporates dependencies between sequences of strategies into graph neural networks. Nortio et al. [70] embark on an exploration of persuasive tech- niques in international negotiations, emphasizing the significance of persuasive strategies during the negotiation process. Persuasion for debate. Debate is a professional persuasive scenario in which debaters persuade the opponent and the audience to accept their view- points by planning their arguments wisely and ar- guing their points from multiple perspectives. Many researchers have explored the automatic generation of persuasive arguments from online discussions or debate competitions [57, 71, 72]. Slonimet al.[32] introduce Project Debater, an autonomous debat-Mengqi Chen et al. 11 ing system that can engage in a competitive debate with humans. Persuasion for recommendation. Engaging in dialogue-based recommendations for movies, prod- ucts, and other such aspects proves to be a highly practical application of a persuasive dialogue sys- tem. To achieve successful recommendations, it is crucial to employ a persuasion strategy to fa- cilitate rapid user comprehension and acceptance of the recommendations. For example, Gupta et al. [73] propose to decompose the recommenda- tion response generation process into first generat- ing explicit commonsense paths between the source and persuasive target followed by generating re- sponses conditioned on the generated paths. 3 Research Challenges and Key Tech- niques Due to the complexity of modeling the psycholog- ical changes in the persuasive conversation, many critical challenges in CogAgent need to be address- ed. In this section, we first detail these challenges faced by CogAgent, and then conduct a compre- hensive investigation of representative works of Co- gAgent according to the adopted cognitive strate- gies, i.e., the persuasion strategy, the topic path planning strategy, and the argument structure pre- diction strategy. 3.1 Research Challenges in CogAgent Exhaustive mining of cognitive strategies . Ps- ychology defines human cognition as the process by which a person encounters, perceives, and un- derstands things [74, 75]. The formation and evo- lution of human cognition is an extremely complex process involving knowledge, personality, emotion, and many other aspects. E ffective persuasive di- alogue changes people’s feelings and perceptions about things through persuasive strategies that con- vince people to change their opinions and behav- iors [11, 76]. Therefore, it is a great challenge to build a complete set of cognitive strategies from the perspective of cognitive psychology by mining cognitive strategies that can effectively change the way human beings perceive and understand things. Several researchers have defined some persuasion strategies based on cognitive psychology theories (e.g., logical appealand emotion appealfrom [11], self-disclosure from [46]). However, most of these strategies are task-specific and not exhaustive enough to cope with generalized persuasion scenarios. How to construct well-defined cognitive strategies from multiple perspectives needs to be explored in depth. Modeling and selecting of cognitive strategies. In persuasive dialogues, people usually dynamicall- y choose different persuasive strategies depending on different persuasive goals and the evolving con- versational contexts. Persuasive strategies contain complex semantic patterns, rather than mere names or descriptions [77, 78]. How to model the implicit associations between strategy definitions and lin- guistic expressions, and precisely select cognitive strategies according to the dialog context to facili- tate the smooth flow of the persuasive dialog pro- cess is a serious challenge. Some researches have explored how to select appropriate cognitive strate- gies based on the dialogue context [13,49]. The ap- propriate selection of cognitive strategies is a crit- ical step for CogAgent to simulate humans in per- suasive conversations and is essential for achieving high-quality persuasive conversations. Integrating cognitive strategies into models. As defined at the cognitive psychology level, cog- nitive strategies are more abstract semantic con- cepts. Data-driven neural network models (DNNs), even LLMs, remain superficial in the understand-12 Front. Comput. Sci., 2024, 0(0): 1–36 ing of cognitive strategies. How to facilitate DNNs to learn the profound semantics of cognitive strate- gies, to rationally integrate cognitive strategies into the generation of persuasive dialogues, and to im- prove the persuasiveness of CogAgent, is quite cha- llenging. Graph-based, [13], reinforcement lear- ning-based [79] and other advanced methods are investigated to integrate cognitive strategies into per- suasive dialog generation. It is promising to inte- grate cognitive strategies with the outstanding lan- guage comprehension ability of LLMs. Absence of evaluation metrics. To improve the quality of persuasive dialogue, the performance of CogAgent needs to be evaluated accurately and co- mprehensively. However, existing evaluation met- rics for dialog systems (e.g., BLEU [80], METEOR [81], ROUGE-L [82]) are usually evaluated at the level of word similarity or semantic similarity be- tween generated responses and ground truth, with- out taking into account the effectiveness of persua- sive strategies, the rationality of persuasive path planning, and the richness of argument structure. It is a challenge to develop comprehensive and rea- sonable evaluation metrics to accurately evaluate the quality of CogAgent, incorporating the charac- teristics of persuasive dialog systems. 3.2 Persuasion Strategy-based CogAgent Incorporating persuasion strategies to enhance the persuasiveness of dialog responses is an important research direction in CogAgent. By using specific persuasive strategies, CogAgent can express the pe- rsuasive content in a way that is more acceptable to the persuadees, thus accomplishing the persua- sive goals more smoothly. As abstract psychologi- cal concepts, how to select appropriate persuasion strategies according to the dialogue context and gui- de the generation of responses is an important re- search question. In this section, we make an inves- tigation of the employment of persuasion strategies in CogAgent, summarized in Table 3. 3.2.1 Strategy Classification based on Dialogue Context A straightforward approach to fusing persuasion strategies in persuasive conversations is to predict a strategy label (e.g., Present of Facts) based on the dialogue context and feed the strategy into the decoder with the dialogue context to generate the dialogue response. For example, Wang et al. [11] propose a per- suasion strategy classifier to predict 10 persuasion strategies based on the dialogue context informa- tion and sentence-level features. The authors also analyze the impact that different people’s backgrou- nds on strategy prediction, laying the groundwork for research on personalized persuasive dialogue agents. He et al. [47] decouple strategy selection and response generation in CogAgent. The dia- logue manager predicts a persuasion strategy ba- sed on the persuasion strategies in dialogue history by a sequence-to-sequence model and the response generator produces a response conditioned on the strategy and dialogue history. 3.2.2 Persuasion Strategy Planning Persuasive dialogue is usually a process that lasts multiple turns, supported by successive strategies [87, 88]. Consequently, strategy planning within a long planning horizon in CogAgent is quite im- portant, rather than predicting a specific strategy based on the dialogue history. Several studies fo- cus on long-term planning of persuasion strategies, making CogAgent more e fficient in reaching per- suasion goals.Mengqi Chen et al. 13 Table 3 Representative works of persuasion strategy-based CogAgent. Solution Work Description Strategy classifying based on dialogue context Wang et al. [11] Proposing a classifier to predict persuasion strategies in dialogue using context and sentence features. He et al. [47] Decoupling strategy selection and response generation in CogAgent for predicting strategy and generating responses based on dialogue history. Persuasion strategy planning Cheng et al. [49] Proposing lookahead heuristics to estimate future user feedback after using the specific strategy. Yu et al. [83] Using Monte Carlo Tree Search for persuasion strategy planning without model training. Graph-based strategy incorporation Joshi et al. [48] Using GNNs to model strategies, dialogue acts, and dependencies in graph structures for response generation. Zhou et al. [84] Modeling both dialogue context semantic and persuasion strategy history with finite state transducers. Knowledge-enhanced strategy modeling Jia et al. [85] Introducing a knowledge-enriched encoder and memory-enhanced strategy module for dynamic emotion and semantic pattern modeling. Chen et al. [35] Designing RAP for dynamic factual and persuasive responses based on knowledge and individual persuasion strategies. Novel integration mechanism Mishra et al. [63] Creating an RL reward function to enhance consistency in politeness strategy, persuasiveness, and emotion acknowledgment in persuasive dialogue. Tu et al. [86] Proposing a novel model MISC, which firstly infers the user’s fine-grained emotional status, and then responds skillfully using a mixture of strategies. For instance, Cheng et al. [49] firstly adopt an A* search algorithm for persuasion strategy plan- ning. When predicting the appropriate strategy in each dialogue turn, look-ahead heuristics are pro- posed to estimate future user feedback after using the specific strategy, thus considering the long-term effect of persuasion strategies. The proposed looka- head method requires abundant annotated data, af- fecting the application to broader persuasive dia- logue scenarios. To overcome this bottleneck, Yu et al. [83] prompts LLMs to perform persuasion strategy planning by simulating future dialogue in- teractions using the Monte Carlo Tree Search (MC- TS) algorithm. This method requires no model trai- ning and can therefore be adapted to any persua- sion scenario. 3.2.3 Graph-based Strategy Incorporation Graph Neural Networks (GNNs) [89–91] can com- bine the benefits of interpretability and expressiv- ity, benefiting from encoding graph-structured data through message propagation. Due to the human brain’s reasoning process to capture semantic as- sociations, graph-based methods have been widely used in various tasks [92,93]. Numerous researche- rs have embarked on exploring the potential of gra- ph-based methods for incorporating persuasion str- ategies in CogAgent. For example, Joshi et al. [48] introduce DIAL- OGRAPH, as shown in Fig 3, a persuasive dialogue system that incorporates persuasion strategies and dialogue acts using GNNs. DIALOGRAPH mod- els persuasion strategies in multi-turn dialogue con- text and their dependencies as graph structures and incorporating strategies into response generation us- ing hierarchical graph pooling-based approaches. Zhou et al. [84] propose to model both dialogue context semantic and persuasion strategy history fi- nite state transducers (FSTs). To model the per- suasion factors affecting the persuasive content of dialogues, Liu et al.[94] present persuasion-factor graph convolutional layers to encode and learn rep- resentations of the persuasion-aware interaction data.14 Front. Comput. Sci., 2024, 0(0): 1–36 Fig. 3 Overview architecture of DIALOGRAPH which models persuasion strategies as graph structure. 3.2.4 Knowledge-enhanced Strategy Modeling As concepts in cognitive psychology, persuasion strategies encompass complex semantic informa- tion and various intricate linguistic features [88, 95]. To comprehensively represent the complex semantics embedded within persuasion strategies, researches investigate combining external knowl- edge to model and mimic the the intricate patterns in strategies. For example, Jiaet al.[85] propose a knowledge- enriched dialogue context encoder to model the dy- namic emotion state and a memory-enhanced strat- egy modeling module to model the semantic pat- terns of persuasion strategies. The same-strategy responses are stored in the memory bank to pro- vide more specific guidance for the strategy- con- strained response generation. Chen et al.[35] de- sign the Response-Agenda Pushing Framework (R- AP) to dynamically produce factual responses bas- ed on knowledge facts and persuasive responses conditioned on individual persuasion strategies. 3.2.5 Novel Integration Mechanisms In addition to the above studies to model and in- tegrate persuasion strategies, researchers propose some novel integration mechanisms to improve the performance of CogAgent, summarized as follows. Combined with RL, Yanget al.[96] propose two variants of ToM-based persuasive dialog agent, wh- ere the explicit version that outputs the opponent type as an intermediate prediction, and an implicit version that models the opponent type as a latent variable. Both models are optimized using rein- forcement learning. Similarly, Mishra et al. [63] design an e fficient reward function in RL to im- prove the politeness-strategy consistency, persua- siveness, and emotional acknowledgement in per- suasive dialogue. To increase the expressed empathy and learn the gradual transition in the long response, Tu [86] in- troduce a MIxed Srategy-aware model (MISC) in- tegrating COMET, a pre-trained generative com- monsense reasoning model, for emotional persua- sive dialogue. The COMET knowledge tuples are adopted to enhance the fine-grained emotional un- derstanding of users. Then MISC formulates per-Mengqi Chen et al. 15 Table 4 Representative works of topic path planning strategy-based CogAgent. Solution Work Description Reinforcement learning based planning Xu et al. [98] Presenting KnowHRL, a three-layer Knowledge-aware hierarchical RL-based model for coherent topic path planning and multi-turn persuasive dialogue responses. Liu et al. [99] Hierarchical RL for conversation topic path planning, using high-level strategies and low-level responses. Lei et al. [100] Introducing four persuasion-related factors in the reward function to achieve persuasive goals efficiently. Graph-based planning Zhong et al. [101] Using commonsense knowledge graphs and GNN to enhance semantic relations between topic keywords, improving keyword-augmented response retrieval. Zou et al. [102] Employing a concept graph for topic planning, utilizing an Insertion Transformer for persuasive response generation based on multi-concept paths. Wang et al. [103] Introducing a Transformer-based network for target-driven topic path planning with knowledge-target mutual attention and set-search decoding. Novel planning mechanism Tang et al. [53] Combining various planning algorithms for robust and smooth topic path planning, incorporating a sampling strategy, flow generator, and global planner. Wang et al. [104] Introducing a consistency-driven dialogue planning approach that utilizes stochastic processes to model the temporal evolution of the conversation path dynamically. suasion strategy as a probability distribution over a strategy codebook to use a mixture of strategies for persuasive response generation. To investigate the potential of LLMs in persua- sive conversations, Zheng et al.[97] first construc- t a large-scale persuasive dialogue dataset in the emotional support domain, leveraging the genera- tive capabilities of LLMs. Then several advanced tuning techniques (fine-tuning, adapter-tuning, Lo- RA-tuning) are employed to to showcase the supe- riority of LLMs in persuasive dialogue generation. 3.3 Topic Path Planning Strategy-based CogAgent In persuasive dialogues, generating engaging re- sponses through effective topic path planning is crit- ical to achieving persuasive targets. Topic path plan- ning strategy is a navigation tool that enhances the coherence of the persuasion process by continu- ously leading users to discuss di fferent points and topics until reaching persuasive targets. This sec- tion deepens into the intricate details of the topic path planning strategy, summarized in Table 4. 3.3.1 Reinforcement Learning-based Planning In the context of topic paths planning strategy, Re- inforcement Learning serves as a dynamic framew- ork for guiding persuasive dialogue systems in a goal-oriented manner. The core of RL is to learn the optimal sequence of actions according to the reward function and is therefore ideally suited for planning coherent topic paths in CogAgent. For example, to achieve coherent topic path plan- ning, Xu et al.[98] introduce a three-layer Knowl- edge aware hierarchical RL-based model (KnowH- RL). The upper layer of KnowHRL plans a high- level topic sequence to track user interests toward persuasive targets. The lower layers are responsi- ble for generating multi-turn persuasive dialogue responses. similarly, Liu et al.[99] propose a hier- archical RL method, GoChat, for topic path plan- ning, as shown in Fig 4. The high-level strategies in GoChat determine sub-goals that guide the conver- sation towards the ultimate target and the low-level strategy generates the corresponding responses to achieve those sub-goals. To plan topic paths from a global perspective, Yanget al.[105] introduce the global planning met- hod integrated with a commonsense knowledge gr-16 Front. Comput. Sci., 2024, 0(0): 1–36 Fig. 4 The overall framework of GoChat with hierarchical reinforcement learning. aph (KG). The key advancement is the introduction of a global RL framework that utilizes topic path planning on KG to guide the local response gener- ation model toward persuasive targets, resulting in more coherent conversations. To achieve persua- sive goals more e ffectively, Lei et al. [100] con- sider four factors (dialogue turn, goal completion difficulty, user satisfaction estimation, and cooper- ative degree) in the reward function. The targets of achieving persuasive targets quickly and maintain- ing the engagingness of users. 3.3.2 Graph-based Planning Knowledge is essential to the cognitive reasoning processes of human beings. We humans usually perform common reason during persuasive conver- sation to enhance the logic and persuasiveness of dialog contents. Therefore, relying on common- sense knowledge graphs for topic path planning can produce more persuasive target-related topic paths for CogAgent, thus reaching persuasive targets mo- re efficiently. Initially, the semantic knowledge relations amo- ng topic keywords are captured to perform next- turn topic prediction during conversation [28,106]. Then the predicted topic keywords are used to re- trieve appropriate candidate responses for persua- sive targets. Furthermore, Zhong et al.[101] intro- duce commonsense knowledge graphs and Graph Neural Networks (GNN) to model the semantic re- lations between topic keywords and enhance the keyword-augmented response retrieval, To plan topic paths more reasonably, Zou et al. [102] introduces a concept graph based on the dia- logue data, where the vertices represent concepts and edges are concept transitions between utter- ances. The topic sequence containing multiple con- cepts is obtained by the multi-concept planning mo- dule and an Insertion Transformer generates a per- suasive response according to the planned topic pat- hs. Wang et al.[107] propose a target-driven plan- ning network (TPNet), which models the topic path planning as a sequence generation task using Trans- former, as shown in Fig 5. A knowledge-target mu- tual attention mechanism and a set-search decod- ing (SSD) strategy are developed to generate topic paths based on the dialogue context. 3.3.3 Novel Planning Mechanism In addition to the above research to plan topic paths in CogAgent, there are some novel planning mech- anisms to be explored, summarized as follows. Combining the strengths of multiple topic plan- ning algorithms, the Tang et al. [53] propose anMengqi Chen et al. 17 Fig. 5 The overview framework of TPNet. EAGLE model for topic path planning. Compris- ing a topic path sampling strategy, topic flow gener- ator, and global planner, EAGLE achieves robust- ness to unseen target topics and smooth transitions. The model demonstrates enhanced global planning ability through its integrated approach, addressing limitations in existing topic-planning conversation models. To ensure the smooth and coherent progression toward persuasive goals across different turns, Wan- g et al. [108] introduce a consistency-driven di- alogue planning approach that utilizes stochastic processes to model the temporal evolution of the conversation path dynamically. Firstly, a latent spa- ce is defined, and Brownian bridge processes are employed to capture the continuity of goal-oriented behavior, allowing for more flexible integration of user feedback into dialogue planning, and explic- itly generating conversation paths. Ultimately, these paths are employed as natural language prompts to guide the generation of persuasive dialogue. 3.4 Argument Structure Prediction Strategy-based CogAgent CogAgent entails an ongoing conversation between a dialogue agent and a user at the cognitive level, where the dialogue agent proactively steers the con- versation. As the conversation progresses, the con- tents presented by the dialogue agent to support its perspectives undergo dynamic transformations. Consequently, the reasonable selection and appli- cation of arguments and evidence play a pivotal role in the persuasiveness of the dialogue. The uti- lization of arguments and evidence is imperative in the process of persuasion. Firstly, employing argu- ments and evidence allows for the gradual decom- position and progressive reasoning of persuasive targets, thereby facilitating a logical and sequential flow of the conversation that enhances the users’ acceptance of viewpoints [109]. Secondly, the pro- vision of factual support elevates the credibility of persuasive discourse, thereby augmenting the per- suasiveness of the conversation. In this section, we provide an investigation of the crucial techniques for argument mining and argument structure pre- diction in CogAgent, as summarized in Table 5.18 Front. Comput. Sci., 2024, 0(0): 1–36 Table 5 Representative works of argument structure prediction strategy-based cogAgent. Solution Work Description Argument mining Khatib et al. [110] Classifying and structurally modeling arguments from online debate portals based on diverse vocabulary, grammar, and metric features. Hua et al. [111] Proposing an argument generation framework with retrieval modules and a sentence-level LSTM for generating viewpoints. Srivastava et al. [112] Using attention-based link prediction and Transformer encoder to model hierarchical causal relationships and discover associations in online argument structures. Niculae et al. [113] Introducing factor graph model for argument mining, concurrently learning fundamental unit types classification and argument relationship prediction. Argument structure prediction Rach et al. [57] Proposing argument search technique using supervised learning-based relation classification to retrieve arguments for debate dialogue system Sakai et al. [71] Introducing an approach to consider the human agreement and disagreement, resulting in a persuasive argument with a hierarchical argumentation structure. Prakken et al. [29] Enhancing argument modeling with a five-layer graph, serving as a knowledge base for a chatbot to identify user focal points and select rebuttal points. Li et al. [114] Using factor graphs to extract online debate features, incorporating them into an LSTM model to predict persuasive arguments. 3.4.1 Argument Mining To integrate the argument structure into CogAgent, it is first necessary to perform argument mining ac- cording to conversation topics. Researchers em- bark on mining argumentative text from dialogues for CogAgent. Debate involves the explicit use of argumenta- tive content for dialogue expression, making it an important source of argument mining. For exam- ple, Khatib et al.[110] utilize online debate portals to acquire both controversial and non-controversial text snippets related to several contentious topics. These snippets are organized in a semi-structured format. Eventually, by employing a diverse set of vocabulary, grammar, and metric feature types, the arguments are classified and structurally modeled. Hua et al. [111] propose a framework for gener- ating arguments to opposing viewpoints. The re- trieval module of this framework comprises Query Formulation, Keyphrase Extraction, and Passage Ranking and Filtering. Subsequently, a sentence- level LSTM is trained to generate a sequence of sentences. In online discussion platforms, people also use argumentative texts to enhance their expressions. For instance, Tran et al.[115] and others [104,116, 117] employ multi-task learning to unearth argu- ments and evidence at both the micro and macro levels, enhancing persuasive power in online dis- cussions. Srivastava et al.[112] employs an attenti- on-based link prediction embedding model to mod- el the hierarchical causal relationships within com- mon argument structures in online discussions. Th- ey then utilize Transformer encoder layers to dis- cover the associations and boundaries between ar- guments. Furthermore, they employ AMPERSAN- D et al. [56] and SMOTE et al. [118] to address data imbalance issues, thereby improving model accuracy. Furthermore, Niculae et al.[113] intro- duce a factor graph model for argument mining, wherein the model concurrently learns the classifi- cation of fundamental unit types and prediction of argument relationships. Furthermore, the parame- ter structures of structured SVM and RNN can en- force structural constraints (e.g., transitivity), while also representing dependencies between adjacent relationships and propositions.Mengqi Chen et al. 19 Fig. 6 The overall framework of the model for predicting which side makes more convincing arguments [114]. 3.4.2 Argument Structure Prediction Dialogue systems of persuasive tasks commonly rely on structured knowledge concerning arguments and their relationships. Numerous researchers have demonstrated that predicting argument structures and integrating them into CogAgent can signifi- cantly enhance topic consistency, content coher- ence, and persuasiveness of persuasive dialogue co- ntents [32, 71, 72]. For example, Rach et al. [57] propose an argu- ment search technique for a debate dialogue sys- tem, which utilizes supervised learning-based re- lation classification to retrieve arguments mapped to a generic tree structure for the dialogue model. Sakai et al.[71] introduce an approach to consider human agreement and disagreement, resulting in a persuasive argument with a hierarchical argumen- tation structure. The dialogue agent selects the next action based on the user’s agreement or disagree- ment and sends the chosen action to the response generation module to generate logically consistent and persuasive dialogue. For more intensive argument modeling, Prakken et al. [29] equip dialogue agents with a five-layer argument graph, consisting of 1288 nodes, with an average of three counterarguments per node. This graph serves as the knowledge base for the pro- posed chatbot, allowing it to dynamically identify and annotate the user’s focal points on the param- eters, enabling the selection of appropriate rebuttal points. Li et al. [114] utilized factor graph mod- els to extract features of argument structures from online debate platforms. These features were then incorporated into an LSTM model to predict the most persuasive arguments, as shown in Fig 6. This study proves that the consideration of argument str- ucture plays a vital role in producing persuasive di- alogue content. 4 Datasets and Evaluation Metrics for CogAgent 4.1 Datasets for CogAgent Massive data is undeniably indispensable for train- ing high-quality CogAgent. To foster advancement in this field, numerous large-scale and high-quality datasets have been released. In this section, we cat- egorize existing datasets by application scenarios, including psychological counseling, debate, price negotiation, persuasion for donation, and product recommendation, summarized as Table 6.20 Front. Comput. Sci., 2024, 0(0): 1–36 Table 6 A review of available datasets for CogAgent. Scenario Dataset Description Psychological counseling ESConv [46] The first dataset for psychological counseling, annotated with persuasive strategies. AUGESC [119] The enhanced dataset from ESConv using LLMs with a broader range of topics. PsyQA [120] A Chinese mental health support dataset featuring annotated persuasive strategies. Debate IAC [121] Argumentative dialog dataset with curated threads, posts, and annotations. Winning Arguments [7] A metadata-rich subset of r/ChangeMyView subreddit conversations includes data on the success of user utterances in persuading the poster. DebateSum [122] A dataset for the competitive formal debate with corresponding argument and extractive summaries. Price negotiation Craigslist- Bargain [47] A human-human dialogue dataset for price negotiation where the buyer and seller are encouraged to reach an agreement to get a better deal. Negotiation- Coach [123] An additional negotiation coach based on CraigslistBargain, which monitors the exchange between two annotators and provides real-time negotiation strategy. Persuasion for donation Persuasion For Good [11] A collection of online conversations where one participant (the persuader) tries to convince the other (the persuadee) to donate to a charity. EPP4G and ETP4G [63] Datasets extending Persuasion For Good by annotating it with the emotion and politeness-strategy labels. FaceAct [124] A dataset extending Persuasion For Good by adding the utterance-level annotations that change the positive and/or the negative face of the participants in a conversation Product recommen- dation TG-ReDial [33] A dataset consisting of dialogues between a seeker and a recommender. DuRecDial [125] A human-to-human Chinese dialog dataset, which contains multiple sequential dialogues for every pair of a recommendation seeker and a recommender. INSPIRED [126] A movie recommendation dataset, consisting of human-human dialogues with an annotation scheme for persuasive strategies. 4.1.1 Datasets for Psychological Counseling Psychological counseling is a typical field of per- suasive dialogue, where CogAgent reduces users’ psychological anxiety and encourages positive emo- tions through the persuasive dialogue process. Re- searchers have released several datasets for psy- chological counseling. ESConv. ESConv6) [46] is a well-designed and rich, e ffective corpora for psychological counsel- ing, consisting of 1,053 dialogue pairs and a to- tal of 31,410 sentences. Each dialogue pair in- cludes information about the initial emotional state of the seeker, the persuasive strategies employed by the supporter during each interaction, and the con- tent of the conversation. The dataset encompasses seven distinct emotional states and eight supportive strategies, with the labeling of these strategies be- ing inspired by Hill’s Helping Skills Theory [88]. 6)https://github.com/thu-coai/ Emotional-Support-Conversation AUGESC. The limitations imposed by crowd- sourcing platforms on data themes and collection methods, along with the substantial regulatory costs, have hindered the extension of downstream dia- logue models to open-domain topics. In response, Zheng et al. augment ESConv to AUGESC7) [119] using LLMs, which comprises 65,000 dialogue ses- sions and a total of 1,738,000 utterances. It sub- stantially expands the scale of ESConv and encom- passes a broader range of topics. PsyQA. PsyQA8) [120] is a Chinese mental heal- th support dataset collected from a Chinese mental health service platform, including 22,000 questions and 56,000 lengthy, well-structured answers. In line with psychological counseling theory, PsyQA annotates some of the answer texts with persuasive strategies and further conducts in-depth analyses of the lexical features and strategic patterns within 7)https://github.com/thu-coai/AugESC 8)https://github.com/thu-coai/PsyQAMengqi Chen et al. 21 counseling responses. 4.1.2 Datasets for Debate Debates are typically persuasive scenarios in which each party of the debate organizes arguments to persuade the other party to accept his or her side’s viewpoints. Existing datasets for debate are listed as follows. Internet Argument Corpus (IAC).IAC9) [121] is a scriptless argumentative dialog dataset, com- prising 390,704 posts extracted from 11,800 dis- cussions on the online debate platform 4forums. com. Within this corpus, a manually curated subset of 2,866 threads and 130,206 posts is formed, cate- gorized based on discussion topics. Extended from IAC, IAC 210) [127] is a corpus for research in po- litical debate on Internet forums, consists of three data sets: 4forums (414K posts), ConvinceMe (65K posts), and a sample from CreateDebate (3K posts). Winning Arguments. To delve deeper into the mechanisms of changing others’ viewpoints in so- cial interactions, Tan et al. [7] introduce the Win- ing Arguments (ChangeMyView) Corpus. Wining ArgumentsCorpus is a metadata-rich subset of con- versations made in the r/ChangeMyview subreddit between 1 Jan 2013 - 7 May 2015, with informa- tion on the delta (success) of a user’s utterance in convincing the poster. There are 34911 Speakers, 293297 Utterances, and 3051 Conversations. DebateSum. DebateSum11) [122] is a dataset for the competitive formal debate, including 187,386 unique pieces of evidence with corresponding ar- gument and extractive summaries. The argument data is collected from the National Speech and De- bate Association over 7 years. 9)https://nlds.soe.ucsc.edu/iac 10)https://nlds.soe.ucsc.edu/iac2 11)https://debate.cards/ 4.1.3 Datasets for Price Negotiation Price negotiation is an everyday persuasive scenario where buyers and sellers reach their desired price through the persuasive dialog process. Datasets for price negotiation are summarized as follows. CraigslistBargain. CraigslistBargain12) [47] is a human-human dialogue dataset for price negoti- ation, which consists of 6682 dialogues, collected using Amazon Mechanical Turk (AMT) in a ne- gotiation setting where two workers were assigned the roles of buyer and seller, respectively. The buyer is additionally given a target price and both parties are encouraged to reach an agreement while each of the workers tries to get a better deal. Negotiation-Coach. Negotiation-Coach13) [123] introduce an additional negotiation coach based on CraigslistBargain, which monitors the exchange be- tween two annotators and provides real-time nego- tiation strategy recommendations to the seller for achieving better deals. 4.1.4 Datasets for Persuasion for Donation Persuasion for donation is very common in life, where the persuader persuades others to donate pro- perty or labor to charities for a public good pur- pose. Datasets for persuasion for donation are listed as follows. Persuasion for Social Good. Persuasion for So- cial Good14) [11] is a collection of online conversa- tions generated by AMT workers, where one par- ticipant (the persuader) tries to convince the other (the persuadee) to donate to a charity. This dataset 12)https://worksheets.codalab.org/worksheets/ 0x453913e76b65495d8b9730d41c7e0a0c/ 13)https://github.com/zhouyiheng11/ Negotiation-Coach 14)https://gitlab.com/ucdavisnlp/ persuasionforgood22 Front. Comput. Sci., 2024, 0(0): 1–36 contains 1017 conversations, along with demograp- hic data and responses to psychological surveys fro- m users. 300 conversations also have per-sentence human annotations of dialogue acts that pertain to the persuasion setting, and sentiment. EPP4G and ETP4G.EPP4G and ETP4G15) [63] extend Persuasion For Good by annotating it with the emotion and politeness-strategy labels. FaceAct. FaceAct16) [124] further extend Per- suasion For Good by adding the utterance-level an- notations that change the positive and /or the neg- ative face of the participants in a conversation. A face act can either raise or attack the positive face or negative face of either the speaker or the listener in the conversation. 4.1.5 Datasets for Product Recommendation Product recommendation intends to induce the rec- ommended person to accept or buy a particular pro- duct through persuasive dialogues. Datasets for product recommendation are listed as follows. TG-ReDial. TG-ReDial17) [33] consists of 10,0- 00 two-party dialogues between a seeker and a rec- ommender in the movie domain. DuRecDial. DuRecDial18) [125] is a human-to- human Chinese dialog dataset (about 10k dialogs, 156k utterances), which contains multiple sequen- tial dialogues for every pair of a recommendation seeker (user) and a recommender (bot). In each di- alogue, the recommender proactively leads a multi- type dialogue to approach recommendation targets and then makes multiple recommendations with ri- ch interaction behavior. 15)https://github.com/Mishrakshitij/PEPDS 16)https://github.com/ShoRit/face-acts 17)https://github.com/RUCAIBox/TG-ReDial 18)https://github.com/PaddlePaddle/Research/ tree/master/NLP/ACL2020-DuRecDial INSPIRED. INSPIRED19) [126] is a movie rec- ommendation dataset, consisting of 1,001 human- human dialogues with an annotation scheme for persuasive strategies based on social science the- ories. 4.2 Evaluation metrics Toward CogAgent The reasonable evaluation of the quality of CogA- gent is a challenging dilemma. Di fferent from the open-domain dialog system, the evaluation of Co- gAgent needs to be performed under di fferent per- suasion scenarios and multifaceted persuasive goal- s. This requires judging the quality of dialogue response while emphasizing the persuasive e ffects in specific persuasive contexts and assessing the adaptability and persuasiveness of the system’s cog- nitive strategies in di fferent domains. Up to now, there is no unified theory on how to effectively eval- uate CogAgent, and researchers predominantly em- ploy two kinds of evaluation methods: automatic evaluation metrics and human evaluation. We sum- marized commonly used automatic evaluation and human evaluation metrics in table 7. Notably, eval- uating CogAgent based on LLMs has also recently received significant attention. 4.2.1 Automatic Evaluation Metrics Automatic evaluation metrics evaluate the perfor- mance of CogAgent by calculating the similarity between the responses generated by CogAgent and ground truths. There are typical categories of auto- matic evaluation metrics: overlap-based methods, embedding-based methods, and learning-based tec- hniques. Overlap-based metrics. Overlap-based meth- ods measure the degree of text overlap between 19)https://github.com/sweetpeach/InspiredMengqi Chen et al. 23 Table 7 Evaluation metrics for CogAgent. Evaluation Method Category Description Metrics Automaticevaluation Overlap- based Measuring the degree of text overlap between generated responses and golden responses BLEU [80], ROUGE [82], METEOR [128], CIDEr [129] Embedding- based Evaluating the semantic similarity of embedding vectors between generated responses and reference ones Greedy Matching [130], Embedding averaging [131], Vector Extreme [132] Learning- based Employing machine learning models to predict the quality scores of generated responses, relying not only on given references ADEM [133] Human evaluation Scoring by human annotators to evaluate the quality of the generated responses with subjective judgment Fluency, Coherence, Contextualization, Emotional expression, Diversity, Persuasiveness generated responses and golden responses, with pa- rticular emphasis on the number of the same n- grams. These methods quantify the similarity of the text, especially the local structural similarity, to measure the quality of generated responses. Clas- sical Overlap-based methods include BLEU [80], ROUGE [82], METEOR [128] and CIDEr [129]. Among these, BLEU evaluates response quality by comparing the harmonic mean of n-gram overlaps between generated responses and the golden ones. BLEU is a straightforward and intuitive metric, yet it is constrained by surface features and may ex- hibit a weak capture of semantic relevance. ROUG- E calculates the length of the longest common sub- sequences between generated and golden responses and considers the precision and recall to evaluate the quality. METEOR integrates multiple aspects of information, including precision, recall, and syn- tactic structure, providing a more comprehensive evaluation. CIDEr evaluates the semantic similar- ity between generated responses and ground truths using n-gram level cosine similarity. These metrics have been widely applied in the evaluation of open- domain dialog systems, but they focus mainly on surface features of the response and may not cap- ture semantic relevance. In addition, relying solely on n-gram overlap to measure similarity may not always accurately evaluate the quality of long texts. Embedding-based metrics. Embedding-based metrics evaluate the semantic similarity of embed- ding vectors between generated responses and ref- erence ones. These methods utilize pre-trained wo- rd embedding models (e.g., BERT [134]) to map textual responses into embedding vectors, thus cap- turing the semantic relationships between the texts more accurately. Specifically, Greedy Matching [130] computes the cosine similarity of word em- beddings between each word in generated response and golden ones. Embedding averaging [131] av- erages all words in the sentence to calculate the sentence-level similarity. Vector Extrema [132] takes the most extreme value in the embedding vec- tor to represent the response to be evaluated. In essence, embedding-based metrics emphasize the semantic quality of CogAgent more than overlap- based metrics and better capture the semantic cor- relations between generated responses and refer- ences. Learning-based metrics. Learning-based met- rics employ machine learning models to predict the quality scores of generated responses, relying not only on given references but aiming to better cor- relate with human judgment. ADEM [133] is a deep model-based evaluation metric for dialogue systems. A hierarchical RNN model is trained in a semi-supervised manner to capture semantic infor- mation and contextual associations and align with the human preferences for dialogue responses.24 Front. Comput. Sci., 2024, 0(0): 1–36 In summary, automatic evaluation metrics o ffer advantages in terms of e fficiency and consistency. However, they face challenges in terms of semantic understanding, manual annotation costs, and model complexity. When selecting and applying automat- ed evaluation metrics, it is important to balance their advantages and disadvantages according to sp- ecific persuasive tasks and scenarios. 4.2.2 Human Evaluation Human evaluation involves subjective judgment an- d scoring by human annotators to evaluate the qual- ity of the generated responses. The annotators are usually domain experts and crowd workers who sub- jectively evaluate the generated responses based on specified criteria and task requirements. Compared to automatic evaluation metrics, human evaluation captures the subjectivity, emotion, and use of per- suasive strategies expressed by CogAgent. There- fore, the flexibility and highly customizable nature of human evaluation becomes a reliable means to ensure that the quality of CogAgent is robustly eval- uated. The human evaluation mainly evaluates CogA- gent in the following main aspects: fluency, coher- ence, contextualization, emotional expression, di- versity, and persuasiveness. In summary, human evaluation has advantages in terms of insightful and accurate evaluation of the quality of CogAgent. Ho- wever, it also has limitations in terms of cost and efficiency, due to the requirement of human labor and time resources. Therefore, in practical applica- tions, researchers need to strike a balance between human and automatic evaluation and choose the evaluation metrics that best suit the task require- ments. 5 Open Issues and Future Trends Though researchers have made considerable efforts to address the above challenges in CogAgent, there are still open issues to be resolved. In this section, we present some open issues and future develop- ment trends for CogAgent to promote the advance- ment of the research community. 5.1 Comprehensive Modeling of Cognitive Psy- chology Theory for CogAgent Although we have summarized some of the cogni- tive psychology theories, a comprehensive investi- gation of the cognitive mechanisms of persuasive dialogues from a cognitive psychology perspective is essential for understanding users’ cognitive weak- nesses and generating engaging persuasive dialogu- es. Many researchers have demonstrated the in- dispensability of employing specific strategies to achieve persuasive effects based on different cogni- tive psychology theories. Utilizing cognitive strate- gies, CogAgent can avoid cognitive dissonance in users and e fficiently persuade them to accept spe- cific viewpoints [11, 96, 135]. Prakken et al.[136] argue that psychological dissonance occurs when individuals are confronted with multiple conflict- ing cognitions. To alleviate this dissonance, three approaches can be used: changing cognitively rel- evant factors in the environment, introducing new cognitive elements, and changing cognitive elemen- ts in behavior. CogAgent should be aware of cogni- tive dissonance to mitigate the obstacles it creates in the persuasion process. In addition, researchers utilize the dual process theory of persuasion and guide the persuasive process with the Elaboration Likelihood Model (ELM) [27], a theory that fo- cuses on cognitive and affective appeals in persua- sion. Another noteworthy aspect is modeling theMengqi Chen et al. 25 user’s cognition. Proposing agreements or mak- ing concessions promptly facilitates the perception of the user’s cognitive state, enabling CogAgent to adapt to changes in the user’s cognition on time and avoiding the failure of the persuasive process [137]. Besides using data analysis to study the mech- anisms of persuasive dialog, we can also explore this phenomenon from the perspective of the cog- nitive functions of the human brain. Advances in neuroscience have provided valuable methods for studying the cognitive mechanisms of persuasive dialogue. As Poldrack et al.state [138], the use of electroencephalography (EEG), magnetoencephal- ography (MEG), functional magnetic resonance im- aging (fMRI), and other brain-imaging tools can deepen our understanding of how the human brain produces social behavior. Arapakis et al.[139] use brainwave recordings to measure users’ interest in news articles, and the experimental results suggest that frontal asymmetry (FFA) can objectively as- sess users’ receptive preferences for content. Ex- ploring the changes in neural signals in the brain of the persuadee during persuasive conversations to model which persuasive factors are effective in be- ing accepted by users and convincing them to adopt persuasive targets is a promising research direction. 5.2 Model Adaptivity /Generality of CogAgent Equipping CogAgent with cross-domain understan- ding and generation capabilities is a promising re- search direction. Existing CogAgent usually fo- cuses on one specific persuasion scenario, such as persuasion for social good, bargaining, and debat- ing. However, it is crucial to develop the ability of CogAgent to understand and transfer through mul- tiple domains, which enables CogAgent to dynam- ically optimize cognitive strategies based on differ- ent persuasive targets and e fficiently perform per- suasive tasks. For example, Wolfet al.[140] utilize transfer learning to jointly fine-tune multiple unsu- pervised response prediction tasks. They demon- strate the e ffectiveness of language model trans- fer learning on the PERSONA-CHAT dataset, es- pecially on the dialogue response generation task. Qian et al.[141] propose a meta-learning-based ap- proach to domain adaptive dialogue generation that learns from multiple resource-rich tasks. They uti- lize multiple resource-rich single-domain dialog dat- asets to train the dialogue system so that it can adapt to new domains with minimal training sam- ples. Therefore, improving the transferability of CogAgent across di fferent domains using transfer learning and other advanced approaches is an im- portant step towards the universal CogAgent. 5.3 Multi-party CogAgent Existing research of CogAgent has demonstrated remarkable performance in two-party conversation- al scenarios. However, in real world, multi-party conversations (MPCs) are more prevalent and re- quire CogAgent to persuade multiple participants simultaneously. Unlike existing persuasive dialog systems, multi-party dialogue scenarios require the collaboration of multiple CogAgent to e fficiently achieve persuasion targets [142–144]. Specifically, a single CogAgent is prone to be overly purpose- ful when interacting with users, which can cause the users’ resentment and resistance and hinder the realization of persuasion targets. In contrast, mul- tiple CogAgents can assume di fferent persuasive roles, cooperate, and persuade from di fferent per- spectives, thus winning users’ trust and realizing persuasion targets more effectively. Existing stud- ies have explored MPCs in open-domain dialogue systems. For instance, Ito et al. [145] construct a multi-modal and multi-party model based on GRU26 Front. Comput. Sci., 2024, 0(0): 1–36 to predict the persuasiveness of multiple members within a group during multi-party conversations, thereby providing a model paradigm for the study of multi-party dialogues. Gu et al.[146] propose a Speaker-Aware BERT (SABERT) model to select appropriate speaking targets from multiple users based on dialogue contexts. Gu et al. [147] ex- plore the problem of ”who says what to whom” in MPCs and propose a plug-and-play graphically- induced fine-tuning (GIFT) module for tuning a va- riety of PLMs for generalized multi-party conver- sation understanding. Inspired by multi-party dia- logue research, it is promising to utilize multiple CogAgents to collaborate on persuasive tasks to enhance the credibility and efficiency of the persua- sion process. Multiple CogAgents utilize persua- sive roles with complementary capabilities, strate- gies, and trust-building to enhance persuasion and effectiveness, thereby facilitating more persuasive and successful persuasion results. 5.4 Interpretability of Persuasive Process Interpretability of models can improve their credi- bility. Improving the interpretability of the persua- sion process is essential to ensure that persuasive dialogue contents produced by CogAgent are ac- cepted and adopted. In recent years, the field of Natural Language Processing (NLP) has increas- ingly focused on improving the interpretability of deep models [148, 149]. For example, Gaur et al. [150] argue that domain-specific knowledge helps to understand how deep models work. They demon- strate the utility of incorporating knowledge-infuse- d learning in knowledge graph format into complex neural networks to achieve model interpretability. Similarly, Yasunagaet al.[151] demonstrate model interpretability and structure inference by combin- ing a pre-trained language model a knowledge grap- h, and a quality assurance context into a unified graph. Currently, research on the interpretability of the persuasion process still lacks an overall frame- work. For the interpretability of the persuasion pro- cess, the e ffectiveness of the persuasion strategy can be verified from the cognitive theory, combined with the knowledge graph reasoning, and the per- suasion behavior can be analyzed interactively. 5.5 Multimodal CogAgent Multimodal perception and comprehension capa- bilities are essential for human beings in daily con- versations. By understanding the multimodal sur- roundings around them, including visual, textual, auditory, and other modal information, we humans can produce engaging dialogues to communicate messages, emotions, and attitudes with others [152, 153]. Despite the outstanding natural language un- derstanding and generation capabilities, perceiving and understanding multimodal context information is essential for natural and harmonious human-mac- hine conversation systems [154, 155]. To persuade people to change their thoughts, opinions, or at- titudes, it is crucial to understand the multimodal surroundings of users. Different environments may lead users to develop different attitudes towards thi- ngs. Combining multimodal contextual informa- tion, persuasive dialogue systems can comprehen- sively understand users’ mental states to generate more specific persuasive dialogue content. There has been extensive research on multimodal dialogue systems that enable the understanding of image or video content through dialogue [156–158]. For ex- ample, Murahari et al. adapt ViLBERT [159] to achieve multi-turn image-based dialogue, which un- derstands the image information through image-text pre-trained on multimodal datasets. Visual Chat- GPT [158] integrates ChatGPT with visual foun-Mengqi Chen et al. 27 dation models to achieve visual dialogue. Different kinds of visual information, such as images, depth images, and mask matrices, are converted into lan- guage formats based on visual foundation models and the prompt manager. Then ChatGPT takes the information from visual and textual modalities to generate dialogue responses. These e fforts have laid a solid foundation for multimodal persuasive dialogue systems. The integration of multimodal information to generate more persuasive conversa- tional content is a highly promising research direc- tion. 5.6 Data and Model Co-Optimization for CogA- gent The huge impact of LLMs (e.g., ChatGPT) in the field of dialog systems has sparked the enthusiasm of researchers and has been widely used in many domains [160–162]. For example, Lianget al.[161] rewrite the policy code for controlling a robot us- ing LLMs. The policy code can receive and un- derstand commands and then outputs the execution code to the API to achieve coherent control of the robot’s actions through the classical chain logic. Similarly, Wen et al.[162] combine the common- sense knowledge implicit in LLMs with the domai- n-specific knowledge of mobile applications to re- alize hands-free speech-based interaction between users and smartphones. LLM can be surprisingly useful in a variety of domains. To develop a high- quality CogAgent, we can utilize LLMs to gener- ate large-scale persuasive dialogue data to quickly validate the algorithm at an early stage. Since the capability of LLMs stems from massive amounts of data, retraining this data is hugely expensive. Therefore, the persuasion process also needs to be modeled to e fficiently and accurately perform the persuasion task. The combination of data-driven LLMs and model-driven persuasion process is the most efficient way to develop intelligent CogAgent. Future research directions for combining LLMs and model-driven persuasion processes include issues such as when to employ the generation abilities of LLMs, when model constraints are needed, and the rules and timing of collaboration between LLMs and the persuasion process. 5.7 Construction of standardized datasets and be- nchmarks Despite the significant progress researchers have made in CogAgent, datasets, and benchmarks for the study of CogAgent are still scarce. The rel- atively small size of many existing datasets (e.g., Persuasion for good [11]) limits the performance of the model in a wider range of applications. The limited amount of data hinders the ability to cap- ture the full complexity and diversity of persuasive dialogue. Moreover, the lack of detailed annota- tions about cognitive strategies in existing datasets creates challenges for training persuasive dialogue agents. Building large-scale, high-quality datasets of persuasive dialogues with rich cognitive strategy annotations is indispensable for the development of CogAgent. Combining the superior text genera- tion capabilities of LLMs [163, 164] is a potential way to build large-scale and high-quality datasets for CogAgent. 6 Conclusion Persuasion is an essential ability in human social communication, and people often skillfully persua- de others to accept their standpoints, views, or per- spectives for various purposes. Consequently, per- suasive dialogue systems have become an engaging research direction. In this paper, we have made a systematic survey of CogAgent. We first present28 Front. Comput. Sci., 2024, 0(0): 1–36 some representative cognitive psychology theories to guide the design of CogAgent at the principle level and formalize the necessary cognitive strate- gies for generating highly persuasive dialogue con- tents, including the persuasion strategy, the topic path planning strategy, and the argument structure prediction strategy. Based on the formalized def- inition and generic architecture of CogAgent, we comprehensively investigate representative works by categorizing cognitive strategies. The available datasets and evaluation metrics for CogAgent are also summarized. Despite significant progress, the research of CogAgent is still in the early stage and massive open issues and prospective future trends to be explored, such as model adaptivity/generality of CogAgent, multi-party CogAgent, and multimo- dal CogAgent. Acknowledgements This work was partially supported by the National Science Fund for Distinguished Young Schol- ars(62025205), and the National Natural Science Foundation of China (No. 62032020). References 1. Guo B, Wang H, Ding Y , Wu W, Hao S, Sun Y , Yu Z. Conditional text generation for harmonious human- machine interaction. ACM Transactions on Intelligent Systems and Technology (TIST), 2021, 12(2): 1–50 2. Huang M, Zhu X, Gao J. Challenges in building in- telligent open-domain dialog systems. ACM Trans- actions on Information Systems (TOIS), 2020, 38(3): 1–32 3. Petty R E, Cacioppo J T, Petty R E, Cacioppo J T. The elaboration likelihood model of persuasion. Springer, 1986 4. Fogg B J. Persuasive technology: using computers to change what we think and do. Ubiquity, 2002, 2002(December): 2 5. IJsselsteijn W, De Kort Y , Midden C, Eggen B, Van Den Hoven E. Persuasive technology for human well- being: setting the scene. In: Persuasive Technology: First International Conference on Persuasive Tech- nology for Human Well-Being, PERSUASIVE 2006, Eindhoven, The Netherlands, May 18-19, 2006. Pro- ceedings 1. 2006, 1–5 6. Fogg B J. Mass interpersonal persuasion: An early view of a new phenomenon. In: Persuasive Technol- ogy: Third International Conference, PERSUASIVE 2008, Oulu, Finland, June 4-6, 2008. Proceedings 3. 2008, 23–34 7. Tan C, Niculae V , Danescu-Niculescu-Mizil C, Lee L. Winning arguments: Interaction dynamics and persua- sion strategies in good-faith online discussions. In: Proceedings of the 25th international conference on world wide web. 2016, 613–624 8. Hidey C, Musi E, Hwang A, Muresan S, McKeown K. Analyzing the semantic types of claims and premises in an online persuasive forum. In: Proceedings of the 4th Workshop on Argument Mining. 2017, 11–21 9. Torning K, Oinas-Kukkonen H. Persuasive system de- sign: state of the art and future directions. In: Proceed- ings of the 4th international conference on persuasive technology. 2009, 1–8 10. Eagly A H, Chaiken S. Cognitive theories of persua- sion. In: Advances in experimental social psychology, volume 17, 267–359. Elsevier, 1984 11. Wang X, Shi W, Kim R, Oh Y , Yang S, Zhang J, Yu Z. Persuasion for good: Towards a personalized persua- sive dialogue system for social good. In: Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019, 5635–5649 12. Shi W, Wang X, Oh Y J, Zhang J, Sahay S, Yu Z. Ef- fects of persuasive dialogues: testing bot identities and inquiry strategies. In: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 2020, 1–13 13. Joshi R, Balachandran V , Vashishth S, Black A, Tsvetkov Y . Dialograph: Incorporating interpretable strategy-graph networks into negotiation dialogues. In: International Conference on Learning Represen- tations (ICLR). 2021 14. Min B, Ross H, Sulem E, Veyseh A P B, Nguyen T H, Sainz O, Agirre E, Heintz I, Roth D. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Sur- veys, 2021 15. Zhao W X, Zhou K, Li J, Tang T, Wang X, Hou Y , Min Y , Zhang B, Zhang J, Dong Z, others . A survey of large language models. arXiv preprint arXiv:2303.18223, 2023 16. Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y , Bashlykov N, Batra S, Bhargava P, Bhosale S, others . Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023 17. Bai Y , Kadavath S, Kundu S, Askell A, Kernion J, Jones A, Chen A, Goldie A, Mirhoseini A, McKin-Mengqi Chen et al. 29 non C, others . Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022 18. Zhou C, Li Q, Li C, Yu J, Liu Y , Wang G, Zhang K, Ji C, Yan Q, He L, others . A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. arXiv preprint arXiv:2302.09419, 2023 19. Ray P P. Chatgpt: A comprehensive review on back- ground, applications, key challenges, bias, ethics, lim- itations and future scope. Internet of Things and Cyber-Physical Systems, 2023 20. Li J, Han D, Guo Z, Qiao B, Wu G. Generating empathetic responses through emotion tracking and constraint guidance. Frontiers of Computer Science, 2024, 18(2) 21. Wang W, Feng S, Song K, Wang D, Li S. Infor- mative and diverse emotional conversation generation with variational recurrent pointer-generator. Frontiers of Computer Science, 2022, 16: 1–3 22. Vaithilingam P, Zhang T, Glassman E L. Expectation vs. experience: Evaluating the usability of code gen- eration tools powered by large language models. In: Chi conference on human factors in computing sys- tems extended abstracts. 2022, 1–7 23. Ni A, Iyer S, Radev D, Stoyanov V , Yih W t, Wang S, Lin X V . Lever: Learning to verify language-to-code generation with execution. In: International Confer- ence on Machine Learning. 2023, 26106–26128 24. Yuan A, Coenen A, Reif E, Ippolito D. Wordcraft: story writing with large language models. In: 27th In- ternational Conference on Intelligent User Interfaces. 2022, 841–852 25. Dergaa I, Chamari K, Zmijewski P, Saad H B. From human writing to artificial intelligence generated text: examining the prospects and potential threats of chat- gpt in academic writing. Biology of Sport, 2023, 40(2): 615–622 26. Bless H, Bohner G, Schwarz N, Strack F. Mood and persuasion: A cognitive response analysis. Personality and social psychology bulletin, 1990, 16(2): 331–345 27. Petty R E, Bri ˜nol P. Emotion and persuasion: Cog- nitive and meta-cognitive processes impact attitudes. Cognition and Emotion, 2015, 29(1): 1–26 28. Qin J, Ye Z, Tang J, Liang X. Dynamic knowledge routing network for target-guided open-domain con- versation. In: Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 8657–8664 29. Prakken H, others . A persuasive chatbot using a crowd-sourced argument graph and concerns. Com- putational Models of Argument, 2020, 326: 9 30. Dijkstra A. The psychology of tailoring-ingredients in computer-tailored persuasion. Social and personality psychology compass, 2008, 2(2): 765–784 31. Kolenik T, Gams M. Intelligent cognitive assistants for attitude and behavior change support in mental health: state-of-the-art technical review. Electronics, 2021, 10(11): 1250 32. Slonim N, Bilu Y , Alzate C, Bar-Haim R, Bogin B, Bonin F, Choshen L, Cohen-Karlik E, Dankin L, Edel- stein L, others . An autonomous debating system. Na- ture, 2021, 591(7850): 379–384 33. Zhou K, Zhou Y , Zhao W X, Wang X, Wen J R. To- wards topic-guided conversational recommender sys- tem. In: Proceedings of the 28th International Confer- ence on Computational Linguistics. 2020, 4128–4139 34. Kang D, Balakrishnan A, Shah P, Crook P, Boureau Y L, Weston J. Recommendation as a communica- tion game: Self-supervised bot-play for goal-oriented dialogue. In: 2019 Conference on Empirical Meth- ods in Natural Language Processing and 9th Interna- tional Joint Conference on Natural Language Process- ing, EMNLP-IJCNLP 2019. 2020, 1951–1961 35. Chen M, Shi W, Yan F, Hou R, Zhang J, Sahay S, Yu Z. Seamlessly integrating factual information and social content with persuasive dialogue. In: Proceed- ings of the 2nd Conference of the Asia-Pacific Chap- ter of the Association for Computational Linguistics and the 12th International Joint Conference on Natu- ral Language Processing. 2022, 399–413 36. Duerr S, Gloor P A. Persuasive natural lan- guage generation–a literature review. arXiv preprint arXiv:2101.05786, 2021 37. Zhan H, Wang Y , Feng T, Hua Y , Sharma S, Li Z, Qu L, Ha ffari G. Let’s negotiate! a sur- vey of negotiation dialogue systems. arXiv preprint arXiv:2212.09072, 2022 38. Deng Y , Lei W, Lam W, Chua T S. A survey on proactive dialogue systems: Problems, methods, and prospects. arXiv preprint arXiv:2305.02750, 2023 39. Cialdini R. Pre-suasion: A revolutionary way to influ- ence and persuade. Simon and Schuster, 2016 40. Bilu Y , Gera A, Hershcovich D, Sznajder B, Lahav D, Moshkowich G, Malet A, Gavron A, Slonim N. Ar- gument invention from first principles. In: Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019, 1013–1026 41. Premack D, Woodru ff G. Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1978, 1(4): 515–526 42. Wu J, Chen Z, Deng J, Sabour S, Huang M. Coke: A cognitive knowledge graph for machine theory of mind. arXiv preprint arXiv:2305.05390, 2023 43. Sap M, Le Bras R, Fried D, Choi Y . Neural theory-of-30 Front. Comput. Sci., 2024, 0(0): 1–36 mind? on the limits of social intelligence in large lms. In: Proceedings of the 2022 Conference on Empiri- cal Methods in Natural Language Processing. 2022, 3762–3780 44. Roman H R, Bisk Y , Thomason J, Celikyilmaz A, Gao J. Rmm: A recursive mental model for dialogue navi- gation. In: Findings of the Association for Computa- tional Linguistics: EMNLP 2020. 2020, 1732–1745 45. Campbell G. The philosophy of rhetoric. SIU Press, 1988 46. Liu S, Zheng C, Demasi O, Sabour S, Li Y , Yu Z, Jiang Y , Huang M. Towards emotional support dialog sys- tems. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan- guage Processing (V olume 1: Long Papers). 2021, 3469–3483 47. He H, Chen D, Balakrishnan A, Liang P. Decou- pling strategy and generation in negotiation dialogues. In: 2018 Conference on Empirical Methods in Natu- ral Language Processing, EMNLP 2018. 2018, 2333– 2343 48. Joshi R, Balachandran V , Vashishth S, Black A, Tsvetkov Y . Dialograph: Incorporating interpretable strategy-graph networks into negotiation dialogues. In: International Conference on Learning Represen- tations. 2020 49. Cheng Y , Liu W, Li W, Wang J, Zhao R, Liu B, Liang X, Zheng Y . Improving multi-turn emotional support dialogue generation with lookahead strategy planning. In: Proceedings of the 2022 Conference on Empiri- cal Methods in Natural Language Processing. 2022, 3014–3026 50. Cacioppo J T, Petty R E. E ffects of message repetition and position on cognitive response, recall, and persua- sion. Journal of personality and Social Psychology, 1979, 37(1): 97 51. Cialdini R B, Cialdini R B. Influence: The psychology of persuasion. volume 55. Collins New York, 2007 52. Ni J, Pandelea V , Young T, Zhou H, Cambria E. Hitkg: Towards goal-oriented conversations via multi- hierarchy learning. In: Proceedings of the AAAI con- ference on artificial intelligence. 2022, 11112–11120 53. Tang Z H, Yeh M Y . Eagle: Enhance target-oriented dialogs by global planning and topic flow integration. In: Proceedings of the 32nd ACM International Con- ference on Information and Knowledge Management. 2023, 2402–2411 54. Petty R E, Cacioppo J T. Communication and persua- sion: Central and peripheral routes to attitude change. Springer Science & Business Media, 2012 55. Swanson R, Ecker B, Walker M. Argument mining: Extracting arguments from online dialogue. In: Pro- ceedings of the 16th annual meeting of the special in- terest group on discourse and dialogue. 2015, 217–226 56. Chakrabarty T, Hidey C, Muresan S, Mckeown K, Hwang A. Ampersand: Argument mining for persua- sive online discussions. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Lan- guage Processing and the 9th International Joint Con- ference on Natural Language Processing (EMNLP- IJCNLP). 2019, 2933–2943 57. Rach N, Schindler C, Feustel I, Daxenberger J, Minker W, Ultes S. From argument search to argumentative dialogue: A topic-independent approach to argument acquisition for dialogue systems. In: Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue. 2021, 368–379 58. Wambsganss T, Kueng T, Soellner M, Leimeister J M. Arguetutor: An adaptive dialog-based learning system for argumentation skills. In: Proceedings of the 2021 CHI conference on human factors in computing sys- tems. 2021, 1–13 59. Ni J, Young T, Pandelea V , Xue F, Cambria E. Recent advances in deep learning based dialogue systems: A systematic survey. Artificial intelligence review, 2023, 56(4): 3055–3155 60. Bubeck S, Chandrasekaran V , Eldan R, Gehrke J, Horvitz E, Kamar E, Lee P, Lee Y T, Li Y , Lund- berg S, others . Sparks of artificial general intelli- gence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023 61. Hochreiter S, Schmidhuber J. Long short-term mem- ory. Neural computation, 1997, 9(8): 1735–1780 62. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez A N, Kaiser Ł, Polosukhin I. Attention is all you need. Advances in neural information processing systems, 2017, 30 63. Mishra K, Samad A M, Totala P, Ekbal A. Pepds: A polite and empathetic persuasive dialogue system for charity donation. In: Proceedings of the 29th In- ternational Conference on Computational Linguistics. 2022, 424–440 64. Walker E R, McGee R E, Druss B G. Mortality in mental disorders and global disease burden implica- tions: a systematic review and meta-analysis. JAMA psychiatry, 2015, 72(4): 334–341 65. Xu B, Zhuang Z. Survey on psychotherapy chatbots. Concurrency and Computation: Practice and Experi- ence, 2022, 34(7): e6170 66. Liang Y , Liu L, Ji Y , Huangfu L, Zeng D D. Identify- ing emotional causes of mental disorders from socialMengqi Chen et al. 31 media for effective intervention. Information Process- ing & Management, 2023, 60(4): 103407 67. Zhou J, Zheng C, Wang B, Zhang Z, Huang M. Case: Aligning coarse-to-fine cognition and a ffection for empathetic response generation. arXiv preprint arXiv:2208.08845, 2022 68. Bosselut A, Rashkin H, Sap M, Malaviya C, Celikyil- maz A, Choi Y . Comet: Commonsense transformers for automatic knowledge graph construction. In: Pro- ceedings of the 57th Annual Meeting of the Associa- tion for Computational Linguistics. 2019 69. Speer R, Chin J, Havasi C. Conceptnet 5.5: An open multilingual graph of general knowledge. In: Proceed- ings of the AAAI conference on artificial intelligence. 2017 70. Nortio E, Jasinskaja-Lahti I, H ¨am¨al¨ainen M, Pakkasvirta J. Fear of the russian bear? negoti- ating finnish national identity online. Nations and Nationalism, 2022, 28(3): 861–876 71. Sakai K, Higashinaka R, Yoshikawa Y , Ishiguro H, Tomita J. Hierarchical argumentation structure for persuasive argumentative dialogue generation. IE- ICE TRANSACTIONS on Information and Systems, 2020, 103(2): 424–434 72. Rach N, Minker W, Ultes S. Increasing the natural- ness of an argumentative dialogue system through ar- gument chains. In: Computational Models of Argu- ment, 331–338. IOS Press, 2020 73. Gupta P, Jhamtani H, Bigham J P. Target-guided di- alogue response generation using commonsense and data augmentation. In: Findings of the Association for Computational Linguistics: NAACL 2022. 2022, 1301–1317 74. Mondal P. A unifying perspective on perception and cognition through linguistic representations of emo- tion. Frontiers in Psychology, 2022, 13: 768170 75. Shettleworth S J. Cognition, evolution, and behavior. Oxford university press, 2009 76. Nguyen H, Mastho ff J. Designing persuasive dialogue systems: Using argumentation with care. In: Per- suasive Technology: Third International Conference, PERSUASIVE 2008, Oulu, Finland, June 4-6, 2008. Proceedings 3. 2008, 201–212 77. Orji R. Why are persuasive strategies e ffective? exploring the strengths and weaknesses of socially- oriented persuasive strategies. In: Persuasive Technol- ogy: Development and Implementation of Personal- ized Technologies to Change Attitudes and Behaviors: 12th International Conference, PERSUASIVE 2017, Amsterdam, The Netherlands, April 4–6, 2017, Pro- ceedings 12. 2017, 253–266 78. Ham J, Bokhorst R, Cuijpers R, Van Der Pol D, Cabibihan J J. Making robots persuasive: the influence of combining persuasive strategies (gazing and ges- tures) by a storytelling robot on its persuasive power. In: Social Robotics: Third International Conference, ICSR 2011, Amsterdam, The Netherlands, November 24-25, 2011. Proceedings 3. 2011, 71–83 79. Samad A M, Mishra K, Firdaus M, Ekbal A. Em- pathetic persuasion: reinforcing empathy and persua- siveness in dialogue systems. In: Findings of the Association for Computational Linguistics: NAACL 2022. 2022, 844–856 80. Papineni K, Roukos S, Ward T, Zhu W J. Bleu: a method for automatic evaluation of machine transla- tion. In: Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002, 311–318 81. Banerjee S, Lavie A. Meteor: An automatic metric for mt evaluation with improved correlation with hu- man judgments. In: Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for ma- chine translation and/or summarization. 2005, 65–72 82. Lin C Y , Hovy E. Automatic evaluation of summaries using n-gram co-occurrence statistics. In: Proceedings of the 2003 human language technology conference of the North American chapter of the association for computational linguistics. 2003, 150–157 83. Yu X, Chen M, Yu Z. Prompt-based monte-carlo tree search for goal-oriented dialogue policy planning. arXiv preprint arXiv:2305.13660, 2023 84. Zhou Y , Tsvetkov Y , Black A W, Yu Z. Augmenting non-collaborative dialog systems with explicit seman- tic and strategic dialog history. In: International Con- ference on Learning Representations. 2019 85. Jia M, Chen Q, Jing L, Fu D, Li R. Knowledge- enhanced memory model for emotional support con- versation. arXiv preprint arXiv:2310.07700, 2023 86. Tu Q, Li Y , Cui J, Wang B, Wen J R, Yan R. Misc: A mixed strategy-aware model integrating comet for emotional support conversation. In: Proceedings of the 60th Annual Meeting of the Association for Com- putational Linguistics (V olume 1: Long Papers). 2022, 308–319 87. Greene J O, Burleson B R. Handbook of communi- cation and social interaction skills. Psychology Press, 2003 88. Hill C E. Helping skills: Facilitating, exploration, in- sight, and action. American Psychological Associa- tion, 2009 89. Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks. In: International32 Front. Comput. Sci., 2024, 0(0): 1–36 Conference on Learning Representations. 2016 90. Veli ˇckovi´c P, Cucurull G, Casanova A, Romero A, Li`o P, Bengio Y . Graph attention networks. In: Interna- tional Conference on Learning Representations. 2018 91. Wu Z, Pan S, Chen F, Long G, Zhang C, Philip S Y . A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 2020, 32(1): 4–24 92. Wu L, Chen Y , Shen K, Guo X, Gao H, Li S, Pei J, Long B, others . Graph neural networks for natu- ral language processing: A survey. Foundations and Trends® in Machine Learning, 2023, 16(2): 119–328 93. Wang H, Guo B, Liu J, Ding Y , Yu Z. Towards infor- mative and diverse dialogue systems over hierarchi- cal crowd intelligence knowledge graph. ACM Trans- actions on Knowledge Discovery from Data, 2023, 17(7): 1–25 94. Liu C, Gao C, Yuan Y , Bai C, Luo L, Du X, Shi X, Luo H, Jin D, Li Y . Modeling persuasion factor of user decision for recommendation. In: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022, 3366–3376 95. Zheng C, Liu Y , Chen W, Leng Y , Huang M. Comae: A multi-factor hierarchical framework for empathetic response generation. In: Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021, 813–824 96. Yang R, Chen J, Narasimhan K. Improving dialog sys- tems for negotiation with personality modeling. In: Proceedings of the 59th Annual Meeting of the As- sociation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (V olume 1: Long Papers). 2021, 681–693 97. Zheng Z, Liao L, Deng Y , Nie L. Building emotional support chatbots in the era of llms. arXiv preprint arXiv:2308.11584, 2023 98. Xu J, Wang H, Niu Z, Wu H, Che W. Knowledge graph grounded goal planning for open-domain conversation generation. In: Proceedings of the AAAI conference on artificial intelligence. 2020, 9338–9345 99. Liu J, Pan F, Luo L. Gochat: Goal-oriented chat- bots with hierarchical reinforcement learning. In: Pro- ceedings of the 43rd International ACM SIGIR Con- ference on Research and Development in Information Retrieval. 2020, 1793–1796 100. Lei W, Zhang Y , Song F, Liang H, Mao J, Lv J, Yang Z, Chua T S. Interacting with non-cooperative user: A new paradigm for proactive dialogue policy. In: Pro- ceedings of the 45th International ACM SIGIR Con- ference on Research and Development in Information Retrieval. 2022, 212–222 101. Zhong P, Liu Y , Wang H, Miao C. Keyword-guided neural conversational model. In: Proceedings of the AAAI Conference on Artificial Intelligence. 2021, 14568–14576 102. Zou Y , Liu Z, Hu X, Zhang Q. Thinking clearly, talk- ing fast: Concept-guided non-autoregressive genera- tion for open-domain dialogue systems. In: Proceed- ings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021, 2215–2226 103. Wang J, Lin D, Li W. Dialogue planning via brownian bridge stochastic process for goal-directed proactive dialogue. arXiv preprint arXiv:2305.05290, 2023 104. Wang S, Yin Z, Zhang W, Zheng D, Li X. Two stage learning for argument pairs extraction. In: Natural Language Processing and Chinese Computing: 10th CCF International Conference, NLPCC 2021, Qing- dao, China, October 13–17, 2021, Proceedings, Part II 10. 2021, 538–547 105. Yang Z, Wang B, Zhou J, Tan Y , Zhao D, Huang K, He R, Hou Y . Topkg: Target-oriented dialog via global planning on knowledge graph. In: Proceedings of the 29th International Conference on Computational Lin- guistics. 2022, 745–755 106. Tang J, Zhao T, Xiong C, Liang X, Xing E, Hu Z. Target-guided open-domain conversation. In: Pro- ceedings of the 57th Annual Meeting of the Associa- tion for Computational Linguistics. 2019, 5624–5634 107. Wang J, Lin D, Li W. A target-driven planning ap- proach for goal-directed dialog systems. IEEE Trans- actions on Neural Networks and Learning Systems, 2023 108. Wang J, Lin D, Li W. Dialogue planning via brownian bridge stochastic process for goal-directed proactive dialogue. 2023 109. Vecchi E M, Falk N, Jundi I, Lapesa G. Towards ar- gument mining for social good: A survey. In: Pro- ceedings of the 59th Annual Meeting of the Associa- tion for Computational Linguistics and the 11th Inter- national Joint Conference on Natural Language Pro- cessing (V olume 1: Long Papers). 2021, 1338–1352 110. Al Khatib K, Wachsmuth H, Hagen M, K ¨ohler J, Stein B. Cross-domain mining of argumentative text through distant supervision. In: Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human lan- guage technologies. 2016, 1395–1404 111. Hua X, Hu Z, Wang L. Argument generation with re- trieval, planning, and realization. In: Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics. 2019, 2661–2672 112. Srivastava P, Bhatnagar P, Goel A. Argument min-Mengqi Chen et al. 33 ing using bert and self-attention based embeddings. In: 2022 4th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N). 2022, 1536–1540 113. Niculae V , Park J, Cardie C. Argument mining with structured svms and rnns. In: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers). 2017, 985–995 114. Li J, Durmus E, Cardie C. Exploring the role of argu- ment structure in online debate persuasion. In: Pro- ceedings of the 2020 Conference on Empirical Meth- ods in Natural Language Processing (EMNLP). 2020, 8905–8912 115. Tran N, Litman D. Multi-task learning in argument mining for persuasive online discussions. In: Proceed- ings of the 8th Workshop on Argument Mining. 2021, 148–153 116. Cheng L, Bing L, He R, Yu Q, Zhang Y , Si L. Iam: A comprehensive and large-scale dataset for integrated argument mining tasks. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers). 2022, 2277– 2287 117. Sun J, Zhu Q, Bao J, Wu J, Yang C, Wang R, Xu R. A hierarchical sequence labeling model for argument pair extraction. In: Natural Language Processing and Chinese Computing: 10th CCF International Confer- ence, NLPCC 2021, Qingdao, China, October 13–17, 2021, Proceedings, Part II 10. 2021, 472–483 118. Chawla N V , Bowyer K W, Hall L O, Kegelmeyer W P. Smote: synthetic minority over-sampling tech- nique. Journal of artificial intelligence research, 2002, 16: 321–357 119. Zheng C, Sabour S, Wen J, Zhang Z, Huang M. Augesc: Dialogue augmentation with large language models for emotional support conversation. In: Find- ings of the Association for Computational Linguistics: ACL 2023. 2023, 1552–1568 120. Sun H, Lin Z, Zheng C, Liu S, Huang M. Psyqa: A chinese dataset for generating long counseling text for mental health support. In: Findings of the Asso- ciation for Computational Linguistics: ACL-IJCNLP 2021. 2021, 1489–1503 121. Walker M, Tree J E F, Anand P, Abbott R, King J. A corpus for research on deliberation and debate. In: Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12). 2012, 812–817 122. Roush A, Balaji A. Debatesum: A large-scale argu- ment mining and summarization dataset. In: Proceed- ings of the 7th Workshop on Argument Mining. 2020, 1–7 123. Zhou Y , He H, Black A W, Tsvetkov Y . A dynamic strategy coach for effective negotiation. In: 20th An- nual Meeting of the Special Interest Group on Dis- course and Dialogue. 2019, 367 124. Dutt R, Joshi R, Rose C. Keeping up appearances: Computational modeling of face acts in persuasion oriented discussions. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP). 2020, 7473–7485 125. Liu Z, Wang H, Niu Z Y , Wu H, Che W, Liu T. To- wards conversational recommendation over multi-type dialogs. In: Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics. 2020, 1036–1049 126. Hayati S A, Kang D, Zhu Q, Shi W, Yu Z. In- spired: Toward sociable recommendation dialog sys- tems. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020, 8142–8152 127. Abbott R, Ecker B, Anand P, Walker M. Internet ar- gument corpus 2.0: An sql schema for dialogic social media and the corpora to go with it. In: Proceedings of the Tenth International Conference on Language Re- sources and Evaluation (LREC’16). 2016, 4445–4452 128. Lavie A, Agarwal A. Meteor: An automatic metric for mt evaluation with high levels of correlation with hu- man judgments. In: Proceedings of the second work- shop on statistical machine translation. 2007, 228–231 129. Vedantam R, Lawrence Zitnick C, Parikh D. Cider: Consensus-based image description evaluation. In: Proceedings of the IEEE conference on computer vi- sion and pattern recognition. 2015, 4566–4575 130. Rus V , Lintean M. An optimal assessment of natu- ral language student input using word-to-word simi- larity metrics. In: Intelligent Tutoring Systems: 11th International Conference, ITS 2012, Chania, Crete, Greece, June 14-18, 2012. Proceedings 11. 2012, 675– 676 131. Wieting J, Bansal M, Gimpel K, Livescu K. Towards universal paraphrastic sentence embeddings. arXiv preprint arXiv:1511.08198, 2015 132. Forgues G, Pineau J, Larchev ˆeque J M, Tremblay R. Bootstrapping dialog systems with word embeddings. In: Nips, modern machine learning and natural lan- guage processing workshop. 2014, 168 133. Lowe R, Noseworthy M, Serban I V , Angelard-Gontier N, Bengio Y , Pineau J. Towards an automatic turing test: Learning to evaluate dialogue responses. In: Pro- ceedings of the 55th Annual Meeting of the Associa- tion for Computational Linguistics (V olume 1: Long34 Front. Comput. Sci., 2024, 0(0): 1–36 Papers). 2017 134. Devlin J, Chang M W, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the As- sociation for Computational Linguistics: Human Lan- guage Technologies, V olume 1 (Long and Short Pa- pers). 2019, 4171–4186 135. Chen Y , Deng S, Kwak D H, Elnoshokaty A, Wu J. A multi-appeal model of persuasion for online petition success: A linguistic cue-based approach. Journal of the Association for Information Systems, 2019, 20(2): 105–131 136. Jing Wen T, Kim E, Wu L, Dodoo N A. Activating persuasion knowledge in native advertising: the influ- ence of cognitive load and disclosure language. Inter- national Journal of Advertising, 2020, 39(1): 74–93 137. Thimm M. Strategic argumentation in multi-agent sys- tems. KI-K ¨unstliche Intelligenz, 2014, 28: 159–168 138. Poldrack R A, Farah M J. Progress and challenges in probing the human brain. Nature, 2015, 526(7573): 371–379 139. Arapakis I, Barreda-Angeles M, Pereda-Ba ˜nos A. In- terest as a proxy of engagement in news reading: Spectral and entropy analyses of eeg activity patterns. IEEE Transactions on A ffective Computing, 2017, 10(1): 100–114 140. Wolf T, Sanh V , Chaumond J, Delangue C. Trans- fertransfo: A transfer learning approach for neural network based conversational agents. arXiv preprint arXiv:1901.08149, 2019 141. Qian K, Yu Z. Domain adaptive dialog generation via meta learning. In: Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics. 2019, 2639–2649 142. Shi Z, Huang M. A deep sequential model for dis- course parsing on multi-party dialogues. In: Pro- ceedings of the AAAI Conference on Artificial Intel- ligence. 2019, 7007–7014 143. Ju D, Feng S, Lv P, Wang D, Zhang Y . Learning to improve persona consistency in multi-party dia- logue generation via text knowledge enhancement. In: Proceedings of the 29th International Conference on Computational Linguistics. 2022, 298–309 144. Yuan L, Chen F, Zhang Z, Yu Y . Communication- robust multi-agent learning by adaptable auxiliary multi-agent adversary generation. Frontiers of Com- puter Science, 2024, 18(6): 186331 145. Ito A, Nakano Y I, Nihei F, Sakato T, Ishii R, Fukayama A, Nakamura T. Predicting persuasive- ness of participants in multiparty conversations. In: 27th International Conference on Intelligent User In- terfaces. 2022, 85–88 146. Gu J C, Li T, Liu Q, Ling Z H, Su Z, Wei S, Zhu X. Speaker-aware bert for multi-turn response selec- tion in retrieval-based chatbots. In: Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020, 2041–2044 147. Gu J C, Ling Z H, Liu Q, Liu C, Hu G. Gift: Graph- induced fine-tuning for multi-party conversation un- derstanding. arXiv preprint arXiv:2305.09360, 2023 148. Belinkov Y , Gehrmann S, Pavlick E. Interpretability and analysis in neural nlp. In: Proceedings of the 58th annual meeting of the association for computational linguistics: tutorial abstracts. 2020, 1–5 149. Jacovi A, Goldberg Y . Towards faithfully interpretable nlp systems: How should we define and evaluate faith- fulness? In: Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics. 2020, 4198–4205 150. Gaur M, Faldu K, Sheth A. Semantics of the black- box: Can knowledge graphs help make deep learning systems more interpretable and explainable? IEEE Internet Computing, 2021, 25(1): 51–59 151. Yasunaga M, Ren H, Bosselut A, Liang P, Leskovec J. Qa-gnn: Reasoning with language models and knowl- edge graphs for question answering. In: Proceedings of the 2021 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies. 2021, 535–546 152. Quek F, McNeill D, Bryll R, Duncan S, Ma X F, Kir- bas C, McCullough K E, Ansari R. Multimodal human discourse: gesture and speech. ACM Transactions on Computer-Human Interaction (TOCHI), 2002, 9(3): 171–193 153. Turk M. Multimodal interaction: A review. Pattern recognition letters, 2014, 36: 189–195 154. Jaimes A, Sebe N. Multimodal human–computer in- teraction: A survey. Computer vision and image un- derstanding, 2007, 108(1-2): 116–134 155. Baltru ˇsaitis T, Ahuja C, Morency L P. Multimodal ma- chine learning: A survey and taxonomy. IEEE trans- actions on pattern analysis and machine intelligence, 2018, 41(2): 423–443 156. Qi J, Niu Y , Huang J, Zhang H. Two causal principles for improving visual dialog. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition. 2020, 10860–10869 157. Alamri H, Cartillier V , Das A, Wang J, Cherian A, Essa I, Batra D, Marks T K, Hori C, Anderson P, oth- ers . Audio visual scene-aware dialog. In: Proceedings of the IEEE/CVF Conference on Computer Vision andMengqi Chen et al. 35 Pattern Recognition. 2019, 7558–7567 158. Wu C, Yin S, Qi W, Wang X, Tang Z, Duan N. Vi- sual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023 159. Lu J, Batra D, Parikh D, Lee S. Vilbert: Pretrain- ing task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural infor- mation processing systems, 2019, 32 160. Pal S, Bhattacharya M, Lee S S, Chakraborty C. A domain-specific next-generation large language model (llm) or chatgpt is required for biomedical engineer- ing and research. Annals of Biomedical Engineering, 2023, 1–4 161. Liang J, Huang W, Xia F, Xu P, Hausman K, Ichter B, Florence P, Zeng A. Code as policies: Language model programs for embodied control. In: 2023 IEEE International Conference on Robotics and Automation (ICRA). 2023, 9493–9500 162. Wen H, Li Y , Liu G, Zhao S, Yu T, Li T J J, Jiang S, Liu Y , Zhang Y , Liu Y . Empowering llm to use smart- phone for intelligent task automation. arXiv preprint arXiv:2308.15272, 2023 163. Kim H, Hessel J, Jiang L, Lu X, Yu Y , Zhou P, Bras R L, Alikhani M, Kim G, Sap M, others . Soda: Million-scale dialogue distillation with so- cial commonsense contextualization. arXiv preprint arXiv:2212.10465, 2022 164. Zheng C, Sabour S, Wen J, Huang M. Augesc: Large- scale data augmentation for emotional support con- versation with pre-trained language models. arXiv preprint arXiv:2202.13047, 2022 Mengqi Chen was born in 1997. She received her master’s de- gree in digital textiles from Xi’an Polytechnical University (XPU) in 2022. She is currently working toward a Ph.D. degree at North- western Polytechnical University (NWPU). Her current research in- terests include natural language processing, dialog sys- tems, and large language models. Bin Guo was born in 1980. He is a Ph.D. professor and Ph.D. super- visor at Northwestern Polytechni- cal University (NWPU). He is a senior member of the China Com- puter Federation. His main re- search interests include ubiquitous computing, social and community intelligence, urban big data mining, mobile crowdsensing, and human- computer interaction. Hao Wang was born in 1996. He received his B.E. degree in com- puter science and technology from Northwestern Polytechnical Uni- versity (NWPU) in 2019. He is currently working toward a Ph.D. degree at NWPU. His current re- search interests include natural language processing, di- alog systems, and large language models. Haoyu Li was born in 2002. He received his B.E. degree in com- puter science and technology from Northwestern Polytechnical Uni- versity (NWPU) in 2023. He is currently working toward a mas- ter’s degree at NWPU. His cur- rent research interests include natural language process- ing, large language models, and robot dynamic obstacle avoidance.36 Front. Comput. Sci., 2024, 0(0): 1–36 Qian Zhao was born in 2001. She received her B.E. degree in In- ternet of Things engineering from Tianjin University of Technology (TUT) in 2023. She is currently working toward a master’s de- gree at Northwestern Polytechni- cal University (NWPU). Her cur- rent research interests include multimodal dialogue, large language models, and visual human-computer in- teraction. Jingqi Liu was born in 2002. She entered Northwestern Polytechni- cal University(NWPU) to study for a bachelor’s degree in infor- mation and computing science in 2020. Her current research inter- ests include natural language pro- cessing, dialogue systems, and large language models. Yasan Ding was born in 1995. He received his B.E. degree in com- puter science and technology from Northwestern Polytechnical Uni- versity (NWPU) in 2018. He is currently working toward a Ph.D. degree at NWPU. His current research interests include fake news detection and natural language processing. Yan Pan was born in 1991. He is a lecturer at the Science and Technology on Information Sys- tems Engineering Laboratory. He respectively received the B.S. de- gree in 2013 and the Ph.D. degree in 2020 from Northwestern Poly- technical University (NWPU). His research interests in- clude Big Data, Machine Learning, and Crowd Intelli- gence. Zhiwen Yu was born in 1977. He is a Ph.D. professor and Ph.D. su- pervisor. He is a senior member of the China Computer Federation. His main research interests in- clude mobile internet, ubiquitous computing, social and community intelligence, urban big data min- ing, mobile crowdsensing, and human-computer inter- action.",
      "references": [
        "Conditional text generation for harmonious human-machine interaction",
        "Challenges in building intelligent open-domain dialog systems",
        "The elaboration likelihood model of persuasion",
        "Persuasive technology: using computers to change what we think and do",
        "Persuasive technology for human well-being: setting the scene",
        "Mass interpersonal persuasion: An early view of a new phenomenon",
        "Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions",
        "Analyzing the semantic types of claims and premises in an online persuasive forum",
        "Persuasive system design: state of the art and future directions",
        "Cognitive theories of persuasion",
        "Persuasion for good: Towards a personalized persuasive dialogue system for social good",
        "Effects of persuasive dialogues: testing bot identities and inquiry strategies",
        "Dialograph: Incorporating interpretable strategy-graph networks into negotiation dialogues",
        "Recent advances in natural language processing via large pre-trained language models: A survey",
        "A survey of large language models",
        "Llama 2: Open foundation and fine-tuned chat models",
        "Constitutional ai: Harmlessness from ai feedback",
        "A comprehensive survey on pretrained foundation models: A history from bert to chatgpt",
        "Chatgpt: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope",
        "Generating empathetic responses through emotion tracking and constraint guidance",
        "Informative and diverse emotional conversation generation with variational recurrent pointer-generator",
        "Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models",
        "Lever: Learning to verify language-to-code generation with execution",
        "Wordcraft: story writing with large language models",
        "From human writing to artificial intelligence generated text: examining the prospects and potential threats of chat-gpt in academic writing",
        "Mood and persuasion: A cognitive response analysis",
        "Emotion and persuasion: Cognitive and meta-cognitive processes impact attitudes",
        "Dynamic knowledge routing network for target-guided open-domain conversation",
        "A persuasive chatbot using a crowd-sourced argument graph and concerns",
        "The psychology of tailoring-ingredients in computer-tailored persuasion",
        "Intelligent cognitive assistants for attitude and behavior change support in mental health: state-of-the-art technical review",
        "An autonomous debating system",
        "Towards topic-guided conversational recommender system",
        "Recommendation as a communication game: Self-supervised bot-play for goal-oriented dialogue",
        "Seamlessly integrating factual information and social content with persuasive dialogue",
        "Persuasive natural language generation–a literature review",
        "Let’s negotiate! a survey of negotiation dialogue systems",
        "A survey on proactive dialogue systems: Problems, methods, and prospects",
        "Pre-suasion: A revolutionary way to influence and persuade",
        "Argument invention from first principles",
        "Does the chimpanzee have a theory of mind?",
        "Coke: A cognitive knowledge graph for machine theory of mind",
        "Neural theory-of-mind? on the limits of social intelligence in large lms",
        "Rmm: A recursive mental model for dialogue navigation",
        "The philosophy of rhetoric",
        "Towards emotional support dialog systems",
        "Decoupling strategy and generation in negotiation dialogues",
        "Improving multi-turn emotional support dialogue generation with lookahead strategy planning",
        "Effects of message repetition and position on cognitive response, recall, and persuasion",
        "Influence: The psychology of persuasion",
        "Hitkg: Towards goal-oriented conversations via multi-hierarchy learning",
        "Eagle: Enhance target-oriented dialogs by global planning and topic flow integration",
        "Communication and persuasion: Central and peripheral routes to attitude change",
        "Argument mining: Extracting arguments from online dialogue",
        "Ampersand: Argument mining for persuasive online discussions",
        "From argument search to argumentative dialogue: A topic-independent approach to argument acquisition for dialogue systems",
        "Arguetutor: An adaptive dialog-based learning system for argumentation skills",
        "Recent advances in deep learning based dialogue systems: A systematic survey",
        "Sparks of artificial general intelligence: Early experiments with gpt-4",
        "Long short-term memory",
        "Attention is all you need",
        "Pepds: A polite and empathetic persuasive dialogue system for charity donation",
        "Mortality in mental disorders and global disease burden implications: a systematic review and meta-analysis",
        "Survey on psychotherapy chatbots",
        "Identifying emotional causes of mental disorders from social media for effective intervention",
        "Case: Aligning coarse-to-fine cognition and affection for empathetic response generation",
        "Comet: Commonsense transformers for automatic knowledge graph construction",
        "Conceptnet 5.5: An open multilingual graph of general knowledge",
        "Fear of the russian bear? negotiating finnish national identity online",
        "Hierarchical argumentation structure for persuasive argumentative dialogue generation",
        "Increasing the naturalness of an argumentative dialogue system through argument chains",
        "Target-guided dialogue response generation using commonsense and data augmentation",
        "A unifying perspective on perception and cognition through linguistic representations of emotion",
        "Cognition, evolution, and behavior",
        "Designing persuasive dialogue systems: Using argumentation with care",
        "Why are persuasive strategies effective? exploring the strengths and weaknesses of socially-oriented persuasive strategies",
        "Making robots persuasive: the influence of combining persuasive strategies (gazing and gestures) by a storytelling robot on its persuasive power",
        "Empathetic persuasion: reinforcing empathy and persuasiveness in dialogue systems",
        "Bleu: a method for automatic evaluation of machine translation",
        "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
        "Automatic evaluation of summaries using n-gram co-occurrence statistics",
        "Prompt-based monte-carlo tree search for goal-oriented dialogue policy planning",
        "Augmenting non-collaborative dialog systems with explicit semantic and strategic dialog history",
        "Knowledge-enhanced memory model for emotional support conversation",
        "Misc: A mixed strategy-aware model integrating comet for emotional support conversation",
        "Handbook of communication and social interaction skills",
        "Helping skills: Facilitating, exploration, insight, and action",
        "Semi-supervised classification with graph convolutional networks",
        "Graph attention networks",
        "A comprehensive survey on graph neural networks",
        "Graph neural networks for natural language processing: A survey",
        "Towards informative and diverse dialogue systems over hierarchical crowd intelligence knowledge graph",
        "Modeling persuasion factor of user decision for recommendation",
        "Comae: A multi-factor hierarchical framework for empathetic response generation",
        "Improving dialog systems for negotiation with personality modeling",
        "Building emotional support chatbots in the era of llms",
        "Knowledge graph grounded goal planning for open-domain conversation generation",
        "Gochat: Goal-oriented chatbots with hierarchical reinforcement learning",
        "Interacting with non-cooperative user: A new paradigm for proactive dialogue policy",
        "Keyword-guided neural conversational model",
        "Thinking clearly, talking fast: Concept-guided non-autoregressive generation for open-domain dialogue systems",
        "Dialogue planning via brownian bridge stochastic process for goal-directed proactive dialogue",
        "Two stage learning for argument pairs extraction",
        "Topkg: Target-oriented dialog via global planning on knowledge graph",
        "Target-guided open-domain conversation",
        "A target-driven planning approach for goal-directed dialog systems",
        "Towards argument mining for social good: A survey",
        "Cross-domain mining of argumentative text through distant supervision",
        "Argument generation with retrieval, planning, and realization",
        "Argument mining using bert and self-attention based embeddings",
        "Argument mining with structured svms and rnns",
        "Exploring the role of argument structure in online debate persuasion",
        "Multi-task learning in argument mining for persuasive online discussions",
        "Iam: A comprehensive and large-scale dataset for integrated argument mining tasks",
        "A hierarchical sequence labeling model for argument pair extraction",
        "Smote: synthetic minority over-sampling technique",
        "Augesc: Dialogue augmentation with large language models for emotional support conversation",
        "Psyqa: A chinese dataset for generating long counseling text for mental health support",
        "A corpus for research on deliberation and debate",
        "Debatesum: A large-scale argument mining and summarization dataset",
        "Internet argument corpus 2.0: An sql schema for dialogic social media and the corpora to go with it",
        "A dynamic strategy coach for effective negotiation",
        "Keeping up appearances: Computational modeling of face acts in persuasion oriented discussions",
        "Towards conversational recommendation over multi-type dialogs",
        "Inspired: Toward sociable recommendation dialog systems",
        "Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments",
        "Cider: Consensus-based image description evaluation",
        "An optimal assessment of natural language student input using word-to-word similarity metrics",
        "Towards universal paraphrastic sentence embeddings",
        "Bootstrapping dialog systems with word embeddings",
        "Towards an automatic turing test: Learning to evaluate dialogue responses",
        "Bert: Pre-training of deep bidirectional transformers for language understanding",
        "A multi-appeal model of persuasion for online petition success: A linguistic cue-based approach",
        "Activating persuasion knowledge in native advertising: the influence of cognitive load and disclosure language",
        "Strategic argumentation in multi-agent systems",
        "Progress and challenges in probing the human brain",
        "Interest as a proxy of engagement in news reading: Spectral and entropy analyses of eeg activity patterns",
        "Transfertransfo: A transfer learning approach for neural network based conversational agents",
        "Domain adaptive dialog generation via meta learning",
        "A deep sequential model for discourse parsing on multi-party dialogues",
        "Learning to improve persona consistency in multi-party dialogue generation via text knowledge enhancement",
        "Communication-robust multi-agent learning by adaptable auxiliary multi-agent adversary generation",
        "Predicting persuasiveness of participants in multiparty conversations",
        "Speaker-aware bert for multi-turn response selection in retrieval-based chatbots",
        "Gift: Graph-induced fine-tuning for multi-party conversation understanding",
        "Interpretability and analysis in neural nlp",
        "Towards faithfully interpretable nlp systems: How should we define and evaluate faithfulness?",
        "Semantics of the black-box: Can knowledge graphs help make deep learning systems more interpretable and explainable?",
        "Qa-gnn: Reasoning with language models and knowledge graphs for question answering",
        "Multimodal human discourse: gesture and speech",
        "Multimodal interaction: A review",
        "Multimodal human–computer interaction: A survey",
        "Multimodal machine learning: A survey and taxonomy",
        "Two causal principles for improving visual dialog",
        "Audio visual scene-aware dialog",
        "Visual chatgpt: Talking, drawing and editing with visual foundation models",
        "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
        "A domain-specific next-generation large language model (llm) or chatgpt is required for biomedical engineering and research",
        "Code as policies: Language model programs for embodied control",
        "Empowering llm to use smartphone for intelligent task automation",
        "Soda: Million-scale dialogue distillation with social commonsense contextualization",
        "Augesc: Large-scale data augmentation for emotional support conversation with pre-trained language models"
      ],
      "meta_data": {
        "arxiv_id": "2402.04631v1",
        "authors": [
          "Mengqi Chen",
          "Bin Guo",
          "Hao Wang",
          "Haoyu Li",
          "Qian Zhao",
          "Jingqi Liu",
          "Yasan Ding",
          "Yan Pan",
          "Zhiwen Yu"
        ],
        "published_date": "2024-02-07T07:28:34Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper provides a systematic survey of Cognitive Strategy-enhanced Persuasive Dialogue Agents (CogAgent). It formalizes the definition of three typical cognitive strategies (persuasion, topic path planning, and argument structure prediction) based on cognitive psychology theories. It proposes a new system architecture for CogAgent, comprehensively investigates representative works categorized by cognitive strategies, summarizes authoritative benchmarks and evaluation metrics, and outlines open issues and future research directions to advance the field.",
        "methodology": "The CogAgent framework integrates cognitive strategies to achieve persuasive targets. It is built upon fundamental cognitive psychology theories such as Pre-suasion, Principle of Consistency, Theory of Mind, and Rhetoric (ethos, pathos, logos). The core methodology involves: 1) predicting persuasion strategy (Per), conversation topic (Top), and argument content (Arg) based on dialogue context using a cognitive strategy predictor; and 2) generating dialogue responses conditioned on these predicted cognitive strategies. The generic system architecture involves semantic understanding (powered by LLMs), cognitive strategy mining (for persuasion strategies, topic paths over knowledge graphs, and argument structures), cognitive strategy prediction for dialogue modeling, and final dialogue response generation using LLMs. Specific techniques discussed in surveyed works include Reinforcement Learning-based planning, Graph Neural Networks (GNNs) for strategy incorporation and planning, Knowledge-enhanced strategy modeling, and novel integration mechanisms like Monte Carlo Tree Search and Transformer-based networks for topic path planning.",
        "experimental_setup": "The paper surveys datasets and evaluation metrics used in CogAgent research. Datasets are categorized by application scenarios: psychological counseling (ESConv, AUGESC, PsyQA), debate (Internet Argument Corpus, Winning Arguments, DebateSum), price negotiation (CraigslistBargain, Negotiation-Coach), persuasion for donation (Persuasion for Social Good, EPP4G and ETP4G, FaceAct), and product recommendation (TG-ReDial, DuRecDial, INSPIRED). Evaluation metrics are divided into: 1) Automatic Evaluation Metrics, including overlap-based (BLEU, ROUGE, METEOR, CIDEr), embedding-based (Greedy Matching, Embedding averaging, Vector Extrema), and learning-based (ADEM); and 2) Human Evaluation, assessing aspects like fluency, coherence, contextualization, emotional expression, diversity, and persuasiveness.",
        "limitations": "Current challenges in CogAgent research include: 1) Exhaustive mining of cognitive strategies, as existing strategies are often task-specific and not comprehensive enough for generalized persuasion scenarios. 2) Difficulties in modeling and precisely selecting appropriate cognitive strategies dynamically based on complex dialogue contexts. 3) The challenge of effectively integrating abstract cognitive strategies into data-driven neural network models, including Large Language Models (LLMs), to genuinely enhance persuasiveness. 4) The absence of comprehensive evaluation metrics that can accurately assess the effectiveness of persuasive strategies, rationality of persuasive path planning, and richness of argument structure, beyond mere text similarity.",
        "future_research_directions": "Future research directions for CogAgent include: 1) More comprehensive modeling of cognitive psychology theories, exploring cognitive mechanisms, user cognitive weaknesses, cognitive dissonance, dual process theory, and leveraging neuroscience tools. 2) Enhancing the model adaptivity and generality of CogAgent for cross-domain understanding and transfer capabilities. 3) Developing Multi-party CogAgent systems to handle complex real-world conversations involving multiple participants. 4) Improving the interpretability of the persuasive process to ensure model credibility and acceptance. 5) Integrating multimodal perception and comprehension capabilities to create Multimodal CogAgent systems. 6) Co-optimizing data-driven LLMs with model-driven persuasion processes for more efficient and accurate task performance. 7) Constructing standardized, large-scale, and high-quality datasets with rich cognitive strategy annotations to further advance the field.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ],
  "research_hypothesis": {
    "open_problems": "Even when we prune context and carry only a compact checkpoint, two failure modes persist that limit real-world reliability of Chain-of-Thought (CoT) protocols on long multi-step problems:\n\n1) **Silent state corruption**: the checkpoint can faithfully compress an *incorrect* intermediate claim; once committed, later rounds are forced to be consistent with the wrong state.\n2) **Unreliable state serialization**: small models often emit ill-formed or underspecified checkpoints (missing variables, inconsistent arithmetic), making multi-round protocols brittle.\n\nFocused, high-impact gap: Can we build an **inference-only, single-model** CoT protocol that (a) keeps the context-pruning benefits of state-only recurrence, but (b) makes checkpoint states **self-verifying and repairable** using only lightweight deterministic checks—no training, no extra models, and no domain-specific tools beyond a Python script—thereby reducing both drift *and* silent state corruption?",
    "method": "## Error-Correcting State Recurrence (ECSR-CoT)\n\nHuman analogy: people don’t just “rewrite clean notes”; they also **add redundancy** (write the same fact in two forms) and **checksum their work** (quick arithmetic/equation checks) before continuing. ECSR-CoT turns checkpoints into *error-correcting codes* for reasoning state.\n\n### Core idea\nMake each checkpoint an **information bottleneck + consistency contract**:\n- the model must output a **structured STATE** (compact key–value store), and\n- a small set of **invariants** (equations) that must hold *with respect to that state*.\nA Python script validates invariants deterministically. If validation fails, the script triggers a constrained **REPAIR** call that asks the same model to fix the state (still without exposing previous long rationale), then continues with a context reset.\n\n### Protocol (single model, inference-only)\nFor each problem, run R rounds (complexity-calibrated):\n1) **State update round** (context contains only: question + current state):\n   - Model outputs either `FINAL: <number>` or:\n     - `STATE: { ... }` (≤10 keys; numeric values where possible)\n     - `INVS:` followed by 2–4 lines like `key == expression_using_keys` (e.g., `total == price*qty`)\n2) **Deterministic validation** (Python):\n   - parse STATE (via `ast.literal_eval` for robustness)\n   - evaluate each invariant safely (AST-restricted arithmetic)\n   - pass if all invariants hold within tolerance and required keys are present\n3) **Repair loop** (at most M tries, e.g., 2):\n   - If parsing or any invariant fails, call the same model with a *repair prompt* containing:\n     - question\n     - previous STATE\n     - the list of failed invariants / missing keys\n     - instruction: output corrected `STATE` + `INVS` only\n4) **Context reset**: next round sees only question + last validated STATE.\n5) Finalization: ask for `FINAL:` using only the last validated STATE.\n\n### Novelty vs. prior checkpoint/reset ideas\n- Introduces a **self-verifying checkpoint contract** (state + invariants) and a **deterministic repair loop**, turning state recurrence into an *error-correcting* process rather than mere summarization.\n- Uses invariants that explicitly reference state keys, enabling **model-agnostic validation** without external verifiers/models.\n- Improves feasibility and robustness for small models by handling formatting failures and preventing silent state corruption from propagating across resets.",
    "experimental_setup": "### Tasks / Datasets (HF, lightweight, multi-benchmark)\nEvaluate on three arithmetic reasoning sets to show robustness:\n- **GSM8K**: `gsm8k`, config `main`, split `test[:200]`\n- **SVAMP**: `svamp`, split `test[:200]`\n- **MultiArith**: `multi_arith`, split `test[:200]` (or the closest available HF variant)\n\n### Model / Decoding\n- `google/flan-t5-base`\n- Greedy decoding (`do_sample=False`) to isolate protocol effects.\n\n### Methods compared\n1) **Direct**: answer-only (`FINAL:`)\n2) **Standard CoT**: single-pass “Let’s think step by step.”\n3) **SR-C3oT**: checkpoint + context reset, but *no deterministic validation/repair* (the current hypothesis baseline)\n4) **ECSR-CoT (ours)**: state + invariants + deterministic validation + repair + context reset\n\n### Metrics\n- Primary: **accuracy** (exact match of final numeric answer)\n- Secondary:\n  - `state_valid_rate`: fraction of rounds producing a valid state without repair\n  - `repair_rate`: fraction of problems requiring ≥1 repair\n  - `avg_model_calls`: calls per problem (latency proxy)\n  - `avg_out_tokens`\n  - accuracy by complexity bin (#numbers)\n\n### Key ablations (still scriptable)\n- Remove invariants (state-only repair disabled) → isolates effect of checksum\n- Disable repair (validate-only, fall back to last state) → isolates repair benefit\n- Vary invariant count (2 vs 4) and repair budget M (0/1/2)\n\n### Evaluation\nParse numeric answer from `FINAL:`; compare to gold numeric (GSM8K `####`, others numeric field).",
    "primary_metric": "accuracy",
    "experimental_code": "import re, ast, math\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nNUM_RE = re.compile(r\"[-+]?\\d*\\.?\\d+\")\n\n# -----------------\n# Complexity heuristic\n# -----------------\n\ndef count_numbers(text: str) -> int:\n    return len(NUM_RE.findall(text))\n\n\ndef plan(question: str):\n    n = count_numbers(question)\n    if n <= 2:\n        return 1, 6\n    elif n <= 4:\n        return 2, 5\n    else:\n        return 3, 4\n\n# -----------------\n# Prompting\n# -----------------\n\ndef p_direct(q):\n    return f\"Solve the problem. Reply with one line: FINAL: <number>\\nQuestion: {q}\\nFINAL:\"\n\n\ndef p_cot(q):\n    return (\n        \"Solve the problem. Show reasoning then answer.\\n\"\n        f\"Question: {q}\\n\"\n        \"Let's think step by step.\\nFINAL:\"\n    )\n\n\ndef p_sr_round(q, state, r, R, k):\n    return (\n        \"You solve in rounds and carry only STATE forward.\\n\"\n        f\"Round {r}/{R}.\\n\"\n        f\"Question: {q}\\n\"\n        f\"STATE: {state}\\n\\n\"\n        f\"Do up to {k} short steps to update the state. Then output:\\n\"\n        \"STATE: {key: value, ...} (compact)\\n\"\n        \"If you are sure, output FINAL: <number> instead.\\n\"\n    )\n\n\ndef p_ecsr_round(q, state, r, R, k):\n    return (\n        \"You solve in rounds and carry only a compact STATE forward.\\n\"\n        \"Before continuing, you must make the STATE self-checking.\\n\"\n        f\"Round {r}/{R}.\\n\"\n        f\"Question: {q}\\n\"\n        f\"STATE: {state}\\n\\n\"\n        f\"Do up to {k} short steps to update the state (steps will be discarded).\\n\"\n        \"Then output either:\\n\"\n        \"FINAL: <number>\\n\"\n        \"OR exactly this format:\\n\"\n        \"STATE: {key: value, ...}   (<=10 keys; use numbers when possible)\\n\"\n        \"INVS:\\n\"\n        \"k1 == <expr using keys and + - * / ()>\\n\"\n        \"k2 == <expr using keys and + - * / ()>\\n\"\n        \"(2-4 invariants; each must reference at least one key)\\n\"\n    )\n\n\ndef p_repair(q, bad_state_text, errors):\n    return (\n        \"Repair the STATE so all invariants are true. Output only STATE and INVS.\\n\"\n        f\"Question: {q}\\n\\n\"\n        f\"BAD_OUTPUT:\\n{bad_state_text}\\n\\n\"\n        f\"ERRORS:\\n{errors}\\n\\n\"\n        \"STATE:\"\n    )\n\n\ndef p_finalize(q, state):\n    return (\n        \"Use only the STATE to answer. Output exactly one line: FINAL: <number>\\n\"\n        f\"Question: {q}\\nSTATE: {state}\\nFINAL:\"\n    )\n\n# -----------------\n# Parsing helpers\n# -----------------\n\ndef extract_final(text: str):\n    m = re.search(r\"FINAL:\\s*([-+]?\\d*\\.?\\d+)\", text)\n    if m:\n        return m.group(1)\n    nums = NUM_RE.findall(text)\n    return nums[-1] if nums else None\n\n\ndef extract_state_block(text: str):\n    m = re.search(r\"STATE:\\s*(\\{.*\\})\", text, flags=re.S)\n    return m.group(1).strip() if m else None\n\n\ndef extract_invs(text: str):\n    if \"INVS\" not in text:\n        return []\n    # take lines after INVS:\n    part = text.split(\"INVS:\", 1)[1]\n    invs = []\n    for line in part.splitlines():\n        line = line.strip()\n        if not line:\n            continue\n        if \"==\" in line:\n            invs.append(line)\n        # stop if model starts a new section\n        if line.startswith(\"FINAL\") or line.startswith(\"STATE\"):\n            break\n    return invs[:4]\n\n\ndef parse_state(state_str: str):\n    # accept python-dict-ish output; safe parse\n    obj = ast.literal_eval(state_str)\n    if not isinstance(obj, dict):\n        raise ValueError(\"STATE is not a dict\")\n    return obj\n\n# -----------------\n# Safe expression eval for invariants\n# -----------------\n\n_ALLOWED_NODES = {\n    ast.Expression, ast.BinOp, ast.UnaryOp,\n    ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Pow,\n    ast.USub, ast.UAdd,\n    ast.Load, ast.Name, ast.Constant,\n    ast.Call,  # disallow via check below\n    ast.Mod,   # disallow via check below\n    ast.FloorDiv,  # disallow via check below\n}\n\n\ndef safe_eval(expr: str, env: dict):\n    tree = ast.parse(expr, mode=\"eval\")\n    for node in ast.walk(tree):\n        if type(node) not in _ALLOWED_NODES:\n            raise ValueError(f\"Disallowed node: {type(node).__name__}\")\n        if isinstance(node, ast.Call):\n            raise ValueError(\"Calls not allowed\")\n        if isinstance(node, (ast.Mod, ast.FloorDiv)):\n            raise ValueError(\"mod/floordiv not allowed\")\n        if isinstance(node, ast.Name) and node.id not in env:\n            raise ValueError(f\"Unknown key: {node.id}\")\n    return eval(compile(tree, \"<expr>\", \"eval\"), {\"__builtins__\": {}}, env)\n\n\ndef validate(state: dict, invs: list[str], tol=1e-6):\n    if not invs:\n        return False, [\"missing INVS\"]\n    errors = []\n    # numeric environment only\n    env = {}\n    for k, v in state.items():\n        if isinstance(v, (int, float)):\n            env[k] = float(v)\n        else:\n            # allow numeric strings\n            try:\n                env[k] = float(str(v))\n            except Exception:\n                pass\n\n    for inv in invs:\n        try:\n            lhs, rhs = [x.strip() for x in inv.split(\"==\", 1)]\n            if lhs not in env:\n                errors.append(f\"lhs not numeric or missing: {lhs}\")\n                continue\n            rhs_val = safe_eval(rhs, env)\n            if not (math.isfinite(rhs_val) and math.isfinite(env[lhs])):\n                errors.append(f\"non-finite in {inv}\")\n                continue\n            if abs(env[lhs] - float(rhs_val)) > tol:\n                errors.append(f\"fails: {inv} (lhs={env[lhs]}, rhs={rhs_val})\")\n        except Exception as e:\n            errors.append(f\"bad inv '{inv}': {e}\")\n    return (len(errors) == 0), errors\n\n# -----------------\n# Gold extractors\n# -----------------\n\ndef gold_gsm8k(ans: str):\n    m = re.search(r\"####\\s*([-+]?\\d*\\.?\\d+)\", ans)\n    return m.group(1) if m else None\n\n\ndef gold_last_num(x):\n    nums = NUM_RE.findall(str(x))\n    return nums[-1] if nums else None\n\n# -----------------\n# Model generate\n# -----------------\n\ndef gen(model, tok, prompt, device, max_new_tokens=192):\n    inp = tok(prompt, return_tensors=\"pt\", truncation=True).to(device)\n    with torch.no_grad():\n        out = model.generate(**inp, max_new_tokens=max_new_tokens, do_sample=False, num_beams=1)\n    return tok.decode(out[0], skip_special_tokens=True), out.shape[-1]\n\n# -----------------\n# Runners\n# -----------------\n\ndef run_singlepass(ds, get_q, get_gold, prompter, model, tok, device):\n    c = t = 0\n    toks = 0\n    for ex in ds:\n        q = get_q(ex).strip()\n        g = get_gold(ex)\n        if g is None:\n            continue\n        txt, ot = gen(model, tok, prompter(q), device)\n        p = extract_final(txt)\n        t += 1\n        toks += ot\n        if p == g:\n            c += 1\n    return {\"accuracy\": c/max(t,1), \"avg_out_tokens\": toks/max(t,1), \"n\": t}\n\n\ndef run_sr(ds, get_q, get_gold, model, tok, device):\n    c = t = 0\n    toks = 0\n    for ex in ds:\n        q = get_q(ex).strip()\n        g = get_gold(ex)\n        if g is None:\n            continue\n        R, k = plan(q)\n        state = \"{}\"\n        pred = None\n        for r in range(1, R+1):\n            txt, ot = gen(model, tok, p_sr_round(q, state, r, R, k), device, max_new_tokens=160)\n            toks += ot\n            pred = extract_final(txt)\n            if \"FINAL:\" in txt and pred is not None:\n                break\n            sb = extract_state_block(txt)\n            if sb:\n                state = sb\n        if pred is None or \"FINAL:\" not in txt:\n            txt2, ot2 = gen(model, tok, p_finalize(q, state), device, max_new_tokens=48)\n            toks += ot2\n            pred = extract_final(txt2)\n        t += 1\n        if pred == g:\n            c += 1\n    return {\"accuracy\": c/max(t,1), \"avg_out_tokens\": toks/max(t,1), \"n\": t}\n\n\ndef run_ecsr(ds, get_q, get_gold, model, tok, device, max_repairs=2):\n    c = t = 0\n    toks = 0\n    valid_rounds = 0\n    total_rounds = 0\n    repairs_used = 0\n    total_calls = 0\n\n    for ex in ds:\n        q = get_q(ex).strip()\n        g = get_gold(ex)\n        if g is None:\n            continue\n        R, k = plan(q)\n        state_str = \"{}\"\n        pred = None\n\n        for r in range(1, R+1):\n            total_rounds += 1\n            txt, ot = gen(model, tok, p_ecsr_round(q, state_str, r, R, k), device, max_new_tokens=180)\n            toks += ot\n            total_calls += 1\n\n            pred = extract_final(txt)\n            if \"FINAL:\" in txt and pred is not None:\n                break\n\n            # parse + validate\n            ok = False\n            err_list = []\n            sb = extract_state_block(txt)\n            invs = extract_invs(txt)\n            cur_txt = txt\n\n            for attempt in range(max_repairs + 1):\n                try:\n                    if not sb:\n                        raise ValueError(\"missing STATE\")\n                    st = parse_state(sb)\n                    ok, err_list = validate(st, invs)\n                    if ok:\n                        valid_rounds += 1\n                        state_str = sb\n                        break\n                    raise ValueError(\"; \".join(err_list))\n                except Exception as e:\n                    if attempt >= max_repairs:\n                        break\n                    repairs_used += 1\n                    rep_prompt = p_repair(q, cur_txt, str(e))\n                    rep_txt, rep_ot = gen(model, tok, rep_prompt, device, max_new_tokens=140)\n                    toks += rep_ot\n                    total_calls += 1\n                    cur_txt = rep_txt\n                    sb = extract_state_block(rep_txt)\n                    invs = extract_invs(rep_txt)\n\n        if pred is None or (\"FINAL:\" not in (txt if 'txt' in locals() else \"\")):\n            txt2, ot2 = gen(model, tok, p_finalize(q, state_str), device, max_new_tokens=48)\n            toks += ot2\n            total_calls += 1\n            pred = extract_final(txt2)\n\n        t += 1\n        if pred == g:\n            c += 1\n\n    return {\n        \"accuracy\": c/max(t,1),\n        \"avg_out_tokens\": toks/max(t,1),\n        \"state_valid_rate\": valid_rounds/max(total_rounds,1),\n        \"repair_rate\": repairs_used/max(t,1),\n        \"avg_model_calls\": total_calls/max(t,1),\n        \"n\": t,\n    }\n\n\ndef main():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    name = \"google/flan-t5-base\"\n    tok = AutoTokenizer.from_pretrained(name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(name).to(device)\n\n    gsm = load_dataset(\"gsm8k\", \"main\", split=\"test[:200]\")\n    sv = load_dataset(\"svamp\", split=\"test[:200]\")\n\n    # MultiArith can have slightly different dataset names; try common HF one.\n    try:\n        ma = load_dataset(\"multi_arith\", split=\"test[:200]\")\n    except Exception:\n        ma = None\n\n    suites = [\n        (\"GSM8K\", gsm, lambda ex: ex[\"question\"], lambda ex: gold_gsm8k(ex[\"answer\"])),\n        (\"SVAMP\", sv, lambda ex: (ex[\"Body\"] + \" \" + ex[\"Question\"]), lambda ex: gold_last_num(ex[\"Answer\"])),\n    ]\n    if ma is not None:\n        # common fields: \"question\"/\"final_ans\" depending on version\n        get_q = lambda ex: ex.get(\"question\", ex.get(\"sQuestion\", \"\"))\n        get_g = lambda ex: gold_last_num(ex.get(\"final_ans\", ex.get(\"answer\", ex.get(\"lSolutions\", \"\"))))\n        suites.append((\"MultiArith\", ma, get_q, get_g))\n\n    for dn, ds, get_q, get_g in suites:\n        print(\"\\n===\", dn, \"===\")\n        print(\"direct\", run_singlepass(ds, get_q, get_g, p_direct, model, tok, device))\n        print(\"cot\", run_singlepass(ds, get_q, get_g, p_cot, model, tok, device))\n        print(\"sr_reset\", run_sr(ds, get_q, get_g, model, tok, device))\n        print(\"ecsr\", run_ecsr(ds, get_q, get_g, model, tok, device))\n\nif __name__ == \"__main__\":\n    main()\n",
    "expected_result": "With `flan-t5-base` (greedy) on 200-example subsets:\n\n**GSM8K**\n- Direct: 0.15–0.22\n- Standard CoT: 0.22–0.30\n- SR-C3oT (reset-only): 0.30–0.38\n- **ECSR-CoT (ours): 0.35–0.44** (expected **+0.05 to +0.08** over SR-C3oT; **+0.10 to +0.14** over Standard CoT)\n\n**SVAMP**\n- Standard CoT: 0.30–0.40\n- SR-C3oT: 0.38–0.48\n- **ECSR-CoT: 0.44–0.54**\n\n**MultiArith**\n- Standard CoT: 0.35–0.50\n- SR-C3oT: 0.45–0.58\n- **ECSR-CoT: 0.50–0.64**\n\nBehavioral/efficiency expectations:\n- `state_valid_rate`: ~0.65–0.80 (small models often need repair)\n- `repair_rate`: ~0.30–0.60 repairs/problem (bounded by max_repairs)\n- `avg_model_calls`: increases vs SR-C3oT by ~0.3–0.8 calls/problem, but should remain within a small constant.\n\nHigher is better for **accuracy**. Secondary metrics expose robustness vs. checkpoint-format brittleness.",
    "expected_conclusion": "ECSR-CoT advances “human-like” Chain-of-Thought by adding a missing ingredient to state-reset approaches: **error-correcting memory**. Rather than merely pruning context (which can still propagate wrong compressed states), ECSR-CoT forces checkpoints to come with **explicit invariants** that are deterministically checkable, and uses a lightweight **repair loop** when the state is inconsistent or ill-formed.\n\nAcademic value:\n- Turns the vague notion of “self-checking” into a *testable protocol* with measurable intermediate variables (state validity, repair rate).\n- Separates two mechanisms—(i) context contamination and (ii) silent state corruption—showing how validation/repair specifically targets the latter.\n- Provides a general, model-agnostic framework for studying reasoning as **state transition under constraints**, closer to cognitive working-memory refresh plus proofreading.\n\nPractical/social value:\n- Improves reliability on small, low-cost models without fine-tuning or extra verifier models.\n- Reduces the need to retain verbose rationales (privacy/IP leakage), while making intermediate state auditable and less error-prone—important for educational, budgeting, and other arithmetic-critical assistance."
  }
}